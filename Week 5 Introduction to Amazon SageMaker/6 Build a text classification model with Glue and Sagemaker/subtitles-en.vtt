WEBVTT

1
00:00:04.220 --> 00:00:07.005
Hello, my name is Tasio Guevara.

2
00:00:07.005 --> 00:00:09.120
I'm a solutions
architect with AWS.

3
00:00:09.120 --> 00:00:10.860
In this session, I'm going

4
00:00:10.860 --> 00:00:12.450
to talk and explain how you can

5
00:00:12.450 --> 00:00:14.610
build a text classification model

6
00:00:14.610 --> 00:00:16.995
by using AWS Glue and
Amazon SageMaker.

7
00:00:16.995 --> 00:00:18.300
Not only that, I want to

8
00:00:18.300 --> 00:00:19.725
make sure that you
don't need to know

9
00:00:19.725 --> 00:00:21.030
that much about
machine learning in

10
00:00:21.030 --> 00:00:22.620
order to fulfill this task.

11
00:00:22.620 --> 00:00:25.140
So you may have
been using already

12
00:00:25.140 --> 00:00:28.140
SageMaker and using
this sample notebooks.

13
00:00:28.140 --> 00:00:30.780
Indeed they are a great
tool to get started

14
00:00:30.780 --> 00:00:32.070
and actually start using

15
00:00:32.070 --> 00:00:34.590
the built-in algorithms
or bringing your own,

16
00:00:34.590 --> 00:00:37.620
but you may have noticed
a pattern in them.

17
00:00:37.620 --> 00:00:40.995
Usually, you start by
downloading some dataset,

18
00:00:40.995 --> 00:00:44.110
a public dataset, and
then working with it in

19
00:00:44.110 --> 00:00:47.470
the notebook instance and
eventually uploading it to S3.

20
00:00:47.470 --> 00:00:49.660
Now, that is really great

21
00:00:49.660 --> 00:00:52.570
for learning and for
some small datasets.

22
00:00:52.570 --> 00:00:57.825
When you start working with
real case and real data,

23
00:00:57.825 --> 00:01:00.450
you will see that the size

24
00:01:00.450 --> 00:01:02.200
of these datasets
they are much bigger.

25
00:01:02.200 --> 00:01:04.300
So you may encounter that

26
00:01:04.300 --> 00:01:07.150
these instances
cannot scale up to

27
00:01:07.150 --> 00:01:09.760
the challenge even though
you can actually get even a

28
00:01:09.760 --> 00:01:13.045
P316 extra large in a
single notebook instance.

29
00:01:13.045 --> 00:01:16.810
But this is where AWS glue
becomes really relevant.

30
00:01:16.810 --> 00:01:18.175
Because you can really work with

31
00:01:18.175 --> 00:01:20.950
enormous amounts of data and
you can scale and process

32
00:01:20.950 --> 00:01:23.230
that data in order
to be consumed by

33
00:01:23.230 --> 00:01:26.140
SageMaker or any algorithm
that you want to bring.

34
00:01:26.140 --> 00:01:27.790
First, I would like, actually,

35
00:01:27.790 --> 00:01:29.780
to talk about text
classification.

36
00:01:29.780 --> 00:01:31.920
Why is it important to you?

37
00:01:31.920 --> 00:01:35.390
So you may notice that most

38
00:01:35.390 --> 00:01:38.660
of the data that we humans
produce is nowadays pictures,

39
00:01:38.660 --> 00:01:43.295
videos, audio but we also
produce a ton of texts.

40
00:01:43.295 --> 00:01:44.460
If you go to Twitter,

41
00:01:44.460 --> 00:01:45.720
when it's social network,

42
00:01:45.720 --> 00:01:49.290
what we do is actually post
some pictures but write.

43
00:01:49.290 --> 00:01:51.510
Not only that, we read news.

44
00:01:51.510 --> 00:01:54.170
So you have news, you guys
repost from companies.

45
00:01:54.170 --> 00:01:56.900
You have tons of text that

46
00:01:56.900 --> 00:01:58.475
you may want to leverage

47
00:01:58.475 --> 00:02:00.185
in order to improve
your business.

48
00:02:00.185 --> 00:02:02.300
So text classification is

49
00:02:02.300 --> 00:02:06.965
one specific use case for
natural language processing.

50
00:02:06.965 --> 00:02:09.695
For example, you could think
about sentiment analysis.

51
00:02:09.695 --> 00:02:12.740
What is people talking
about your company?

52
00:02:12.740 --> 00:02:15.290
Are they've talking good
things, bad things?

53
00:02:15.290 --> 00:02:17.915
That's just a specific case

54
00:02:17.915 --> 00:02:20.750
of a broader text
classification problem.

55
00:02:20.750 --> 00:02:22.520
But then you could also use that

56
00:02:22.520 --> 00:02:24.800
for triaging trouble tickets.

57
00:02:24.800 --> 00:02:26.240
You may have your customers

58
00:02:26.240 --> 00:02:28.865
introduce a description
of a problem they have.

59
00:02:28.865 --> 00:02:30.110
But in many cases,

60
00:02:30.110 --> 00:02:31.310
they need to actually introduce

61
00:02:31.310 --> 00:02:36.395
some class to help you
fulfill their need.

62
00:02:36.395 --> 00:02:39.095
What if they could just
introduce a problem,

63
00:02:39.095 --> 00:02:41.689
write it down and
you automatically

64
00:02:41.689 --> 00:02:44.525
can try us out to the right
team to solve the issue.

65
00:02:44.525 --> 00:02:47.350
Well, they would benefit
from, first thing,

66
00:02:47.350 --> 00:02:49.520
not having to introduce

67
00:02:49.520 --> 00:02:50.660
those metadata that are

68
00:02:50.660 --> 00:02:52.880
necessary for you
but not for them,

69
00:02:52.880 --> 00:02:54.890
but also get a quicker response.

70
00:02:54.890 --> 00:02:56.210
First, I would like to talk

71
00:02:56.210 --> 00:02:58.870
about the machine
learning process.

72
00:02:58.870 --> 00:03:00.770
In this session,
we're going to go

73
00:03:00.770 --> 00:03:02.705
through this process
with a real case.

74
00:03:02.705 --> 00:03:04.535
We're going to take
a real dataset,

75
00:03:04.535 --> 00:03:06.380
it's a publicly
available dataset,

76
00:03:06.380 --> 00:03:08.840
and we're going to
end with a model

77
00:03:08.840 --> 00:03:12.055
that we can use to do
some text classification.

78
00:03:12.055 --> 00:03:14.320
Now, there is a
process in between

79
00:03:14.320 --> 00:03:15.710
and there is some pinpoints in

80
00:03:15.710 --> 00:03:19.595
this process that AWS is
helping you overcome.

81
00:03:19.595 --> 00:03:20.780
Thanks to the services and

82
00:03:20.780 --> 00:03:22.880
the data platform
that we provide.

83
00:03:22.880 --> 00:03:25.430
We'll see how you can leverage

84
00:03:25.430 --> 00:03:27.830
that to really kill
those pinpoints.

85
00:03:27.830 --> 00:03:29.960
So this is how the machine
learning process looks like.

86
00:03:29.960 --> 00:03:32.060
It has four different
stages and it

87
00:03:32.060 --> 00:03:34.835
starts with the business problem.

88
00:03:34.835 --> 00:03:36.890
You need to have a business
problem that you want to

89
00:03:36.890 --> 00:03:38.270
solve in order to get

90
00:03:38.270 --> 00:03:40.525
started because if you don't
know where you're going,

91
00:03:40.525 --> 00:03:42.825
you never know that you're there.

92
00:03:42.825 --> 00:03:45.290
Once you have defined the problem

93
00:03:45.290 --> 00:03:47.855
plus some requirements
about accuracy,

94
00:03:47.855 --> 00:03:49.385
then you need to have data.

95
00:03:49.385 --> 00:03:51.200
You need to work with
data and do you have to

96
00:03:51.200 --> 00:03:52.980
collect it, you have
to integrate it,

97
00:03:52.980 --> 00:03:54.155
which means that you may have

98
00:03:54.155 --> 00:03:56.560
different sources and you
want to put it together,

99
00:03:56.560 --> 00:03:57.710
and then you need to prepare and

100
00:03:57.710 --> 00:04:00.380
clear in order to train a model.

101
00:04:00.380 --> 00:04:03.354
So for training you
actually need an algorithm,

102
00:04:03.354 --> 00:04:05.210
then you need to also visualize

103
00:04:05.210 --> 00:04:06.530
the data that you're
going to use,

104
00:04:06.530 --> 00:04:08.540
maybe do some
feature engineering,

105
00:04:08.540 --> 00:04:10.400
and then train the model.

106
00:04:10.400 --> 00:04:13.775
But, usually, it doesn't
really work on the first go.

107
00:04:13.775 --> 00:04:16.040
You probably going to need
to tweak how the algorithm

108
00:04:16.040 --> 00:04:18.590
works in order to
get a better model.

109
00:04:18.590 --> 00:04:20.240
Then you need to evaluate it.

110
00:04:20.240 --> 00:04:22.250
Once it's evaluated, you need to

111
00:04:22.250 --> 00:04:24.665
decide if it's good
for production or not.

112
00:04:24.665 --> 00:04:28.600
But once you decide that it's
suitable for production,

113
00:04:28.600 --> 00:04:31.370
then you need an endpoint
where you can run

114
00:04:31.370 --> 00:04:32.960
inferences against and you

115
00:04:32.960 --> 00:04:34.790
need to monitor how
that is performing.

116
00:04:34.790 --> 00:04:37.880
Eventually, you're going
to go into retrain.

117
00:04:37.880 --> 00:04:40.010
Mastering this cycle is

118
00:04:40.010 --> 00:04:43.355
crucial for achieving
good results.

119
00:04:43.355 --> 00:04:45.470
No single great model out

120
00:04:45.470 --> 00:04:47.960
there is just working
on the first go.

121
00:04:47.960 --> 00:04:50.210
You always circulate in

122
00:04:50.210 --> 00:04:53.290
this cycle over it
and iterate quickly,

123
00:04:53.290 --> 00:04:55.655
as quick as you can,
to get better results.

124
00:04:55.655 --> 00:04:57.500
Let's see this in practice.

125
00:04:57.500 --> 00:05:00.590
Let's see these four phases
and let's go one by one.

126
00:05:00.590 --> 00:05:02.240
First, we have the discovery.

127
00:05:02.240 --> 00:05:04.025
Let's frame our business problem.

128
00:05:04.025 --> 00:05:07.100
At the beginning, I talked
about trouble ticket

129
00:05:07.100 --> 00:05:10.130
and our business goal will

130
00:05:10.130 --> 00:05:13.130
be to do a triaging
of trouble tickets.

131
00:05:13.130 --> 00:05:16.310
We'll see that I cheated a bit on

132
00:05:16.310 --> 00:05:18.320
the business problem but it's

133
00:05:18.320 --> 00:05:20.770
very similar problem
that we'll solve.

134
00:05:20.770 --> 00:05:23.510
The goals that you can
achieve with this is that you

135
00:05:23.510 --> 00:05:26.060
can streamline the
ticket creation process.

136
00:05:26.060 --> 00:05:28.100
As I said, usually when you

137
00:05:28.100 --> 00:05:30.920
are having an issue
with any service,

138
00:05:30.920 --> 00:05:33.140
you have to introduce some
metadata that is helping

139
00:05:33.140 --> 00:05:35.740
the service provider to
try use that ticket.

140
00:05:35.740 --> 00:05:37.590
Now, what if we can remove that?

141
00:05:37.590 --> 00:05:39.770
It would be a much
better experience.

142
00:05:39.770 --> 00:05:41.900
Then you may reduce

143
00:05:41.900 --> 00:05:44.330
this ticket solving
time by getting to

144
00:05:44.330 --> 00:05:45.830
the right queue at

145
00:05:45.830 --> 00:05:47.630
the beginning and then

146
00:05:47.630 --> 00:05:49.975
you would minimize
balancing tickets.

147
00:05:49.975 --> 00:05:53.385
So what dataset can we use?

148
00:05:53.385 --> 00:05:56.330
I'm going to talk about the
Amazon reviews dataset.

149
00:05:56.330 --> 00:05:58.955
As you see, amazon.com

150
00:05:58.955 --> 00:06:01.430
sells millions of
different products.

151
00:06:01.430 --> 00:06:05.145
Each product has
customer reviews.

152
00:06:05.145 --> 00:06:08.000
Now, this is how a customer
review looks like.

153
00:06:08.000 --> 00:06:11.749
It has some metadata
like the author,

154
00:06:11.749 --> 00:06:13.760
the rating, a header,

155
00:06:13.760 --> 00:06:16.685
a text, which is at the
body of the review,

156
00:06:16.685 --> 00:06:20.330
how useful has it been,
plus other metadata.

157
00:06:20.330 --> 00:06:23.810
Amazon.com has made
these available on AWS.

158
00:06:23.810 --> 00:06:26.540
You have this dataset
available from

159
00:06:26.540 --> 00:06:29.540
a public bucket in S3.

160
00:06:29.540 --> 00:06:32.650
It contains 20 years
of product reviews

161
00:06:32.650 --> 00:06:36.260
from Amazon customers and
accompanying metadata.

162
00:06:36.260 --> 00:06:39.395
There are more than
160 million reviews.

163
00:06:39.395 --> 00:06:43.370
There's close to 80
gigabytes of raw data,

164
00:06:43.370 --> 00:06:47.790
51 gigabytes barely if it's
compressed with parquet,

165
00:06:47.790 --> 00:06:50.610
and it's partitioned
by product category.

166
00:06:50.610 --> 00:06:52.830
This is a key thing
for our use case

167
00:06:52.830 --> 00:06:55.280
because what we're going to do is

168
00:06:55.280 --> 00:06:58.325
try to predict a product category

169
00:06:58.325 --> 00:07:01.080
out of the text of that review.

170
00:07:01.080 --> 00:07:04.535
So as I said, it's a
very similar problem

171
00:07:04.535 --> 00:07:08.105
because we're going to predict
a label out of a text.

172
00:07:08.105 --> 00:07:09.560
But in this case,
it's going to be

173
00:07:09.560 --> 00:07:11.990
a product category from
a review instead of

174
00:07:11.990 --> 00:07:17.830
a trouble ticket category
from the problem description.

175
00:07:17.940 --> 00:07:21.175
Now, these are some
examples I picked up.

176
00:07:21.175 --> 00:07:23.170
I picked them
specifically because

177
00:07:23.170 --> 00:07:25.345
they are hard to
predict for a human,

178
00:07:25.345 --> 00:07:28.030
so disappointed, stopped working

179
00:07:28.030 --> 00:07:30.280
due to water after
only two weeks,

180
00:07:30.280 --> 00:07:32.170
what kind of product
could that be?

181
00:07:32.170 --> 00:07:33.925
Ten-year-old loved it.

182
00:07:33.925 --> 00:07:37.090
That sounds like a toy but it
may be something different.

183
00:07:37.090 --> 00:07:40.165
The metal is very cheap and
bendable, but it works fine.

184
00:07:40.165 --> 00:07:43.825
Well, this would be a
hard task for a human.

185
00:07:43.825 --> 00:07:46.180
But let's see how
a machinery model

186
00:07:46.180 --> 00:07:48.430
can deal with it. We'll
see that at the end.

187
00:07:48.430 --> 00:07:50.680
Yeah, we have already the
business problem defined,

188
00:07:50.680 --> 00:07:52.720
now we need to frame it
with some requirements.

189
00:07:52.720 --> 00:07:54.460
So how good should the model be?

190
00:07:54.460 --> 00:07:56.065
Well, in our case,

191
00:07:56.065 --> 00:07:57.520
it doesn't really matter, right?

192
00:07:57.520 --> 00:07:58.945
It's just an example.

193
00:07:58.945 --> 00:08:03.070
So I am going to show you the
perfect confusion matrix.

194
00:08:03.070 --> 00:08:05.455
A confusion matrix is a measure

195
00:08:05.455 --> 00:08:07.990
of how good a
classification model works.

196
00:08:07.990 --> 00:08:09.655
So in the y-axis,

197
00:08:09.655 --> 00:08:12.265
we have the true
labels of the data,

198
00:08:12.265 --> 00:08:14.140
and in the x-axis,

199
00:08:14.140 --> 00:08:15.940
we have the predicted label.

200
00:08:15.940 --> 00:08:17.725
In this case, we
see a perfect one.

201
00:08:17.725 --> 00:08:21.370
So everything is white
except for the diagonal.

202
00:08:21.370 --> 00:08:24.490
So that means that we have
a 100 percent accuracy.

203
00:08:24.490 --> 00:08:26.470
We have 100 percent recall.

204
00:08:26.470 --> 00:08:28.450
So we have a really
perfect model.

205
00:08:28.450 --> 00:08:30.760
This is something you'll
see only theoretically here

206
00:08:30.760 --> 00:08:33.415
but it will never
happen in real life.

207
00:08:33.415 --> 00:08:36.040
So now, we have
finished our discovery

208
00:08:36.040 --> 00:08:38.890
and we can move on
to the integration.

209
00:08:38.890 --> 00:08:41.815
We'll start by visualizing
and analyzing the data.

210
00:08:41.815 --> 00:08:43.555
In order to do that,

211
00:08:43.555 --> 00:08:46.090
we can use AWS Glue crawler.

212
00:08:46.090 --> 00:08:48.865
A crawler is a process
that is going to

213
00:08:48.865 --> 00:08:53.005
traverse our data and try to
extract a schema out of it.

214
00:08:53.005 --> 00:08:54.790
Then, once it's
done, it will have

215
00:08:54.790 --> 00:08:56.770
a database and a table
there that you can

216
00:08:56.770 --> 00:09:00.655
use with other AWS
services like Athena.

217
00:09:00.655 --> 00:09:04.270
So the creation of our Glue
crawler is rather simple.

218
00:09:04.270 --> 00:09:07.570
In our case, we just
specify the path in

219
00:09:07.570 --> 00:09:09.400
an S3 bucket and give it

220
00:09:09.400 --> 00:09:11.680
a role that has access
to that bucket,

221
00:09:11.680 --> 00:09:15.400
and can create tables
and databases on Glue.

222
00:09:15.400 --> 00:09:16.825
So once we have that,

223
00:09:16.825 --> 00:09:19.135
we can start actually
querying that data.

224
00:09:19.135 --> 00:09:22.540
I mentioned Athena, you can
actually take Quicksight

225
00:09:22.540 --> 00:09:24.580
to have visualizations on

226
00:09:24.580 --> 00:09:26.245
data that is being
created by Athena.

227
00:09:26.245 --> 00:09:28.360
This is the schema that was

228
00:09:28.360 --> 00:09:30.865
extracted from the
public dataset.

229
00:09:30.865 --> 00:09:34.120
We see all of the
columns that are there.

230
00:09:34.120 --> 00:09:37.195
We're going to focus on review
body, which is the text,

231
00:09:37.195 --> 00:09:39.010
and product category, which

232
00:09:39.010 --> 00:09:40.945
is what we're going
to try to predict,

233
00:09:40.945 --> 00:09:42.235
also called the target.

234
00:09:42.235 --> 00:09:43.885
Now, let's have a look at

235
00:09:43.885 --> 00:09:46.010
some characteristics
of the dataset.

236
00:09:46.010 --> 00:09:48.660
By using Quicksight,
I could query

237
00:09:48.660 --> 00:09:52.380
how many reviews were there
per product category.

238
00:09:52.380 --> 00:09:54.960
We see this interesting
graph where

239
00:09:54.960 --> 00:09:57.620
we see that this is a very,
very unbalanced dataset.

240
00:09:57.620 --> 00:09:58.810
That means that there are

241
00:09:58.810 --> 00:10:01.585
many more reviews of some
categories in this case,

242
00:10:01.585 --> 00:10:03.415
for example, books than

243
00:10:03.415 --> 00:10:05.650
others like personal
care appliances.

244
00:10:05.650 --> 00:10:07.060
Well, this makes sense.

245
00:10:07.060 --> 00:10:09.280
This dataset covers 20 years of

246
00:10:09.280 --> 00:10:12.340
Amazon.com reviews
starting from 1995.

247
00:10:12.340 --> 00:10:14.725
As you may well remember,

248
00:10:14.725 --> 00:10:16.765
Amazon.com started selling books,

249
00:10:16.765 --> 00:10:17.995
so it makes sense that there are

250
00:10:17.995 --> 00:10:21.265
many more reviews about
books than other categories.

251
00:10:21.265 --> 00:10:23.680
But generating a model out of

252
00:10:23.680 --> 00:10:26.620
this could lead to a
very biased model.

253
00:10:26.620 --> 00:10:28.390
That means that the
predictions that we would

254
00:10:28.390 --> 00:10:30.640
get would lean towards

255
00:10:30.640 --> 00:10:33.640
books just because the algorithm

256
00:10:33.640 --> 00:10:36.460
has been trained
on a lot of books.

257
00:10:36.460 --> 00:10:38.605
So we want to go from

258
00:10:38.605 --> 00:10:41.920
that imbalanced dataset to
something like this where we

259
00:10:41.920 --> 00:10:44.515
have roughly an equivalent amount

260
00:10:44.515 --> 00:10:46.870
of reviews for each category.

261
00:10:46.870 --> 00:10:48.415
So how do we do that?

262
00:10:48.415 --> 00:10:50.214
If we talk about ETL,

263
00:10:50.214 --> 00:10:51.700
T is for transformation,

264
00:10:51.700 --> 00:10:53.020
so we need to transform a bit

265
00:10:53.020 --> 00:10:57.460
this dataset from imbalanced
one to a balanced one.

266
00:10:57.460 --> 00:11:00.955
We're going to do
that with AWS Glue.

267
00:11:00.955 --> 00:11:04.720
Now, you can write
Glue jobs but usually,

268
00:11:04.720 --> 00:11:06.400
it takes some time to spin up

269
00:11:06.400 --> 00:11:08.785
the compute capacity that
is going to run the job.

270
00:11:08.785 --> 00:11:10.390
So it's a very good idea to

271
00:11:10.390 --> 00:11:12.415
start with a
development endpoint.

272
00:11:12.415 --> 00:11:14.560
Recently, it was announced
that you can actually use

273
00:11:14.560 --> 00:11:16.540
Sagemaker Notebooks in order

274
00:11:16.540 --> 00:11:18.610
to connect to this
development endpoints.

275
00:11:18.610 --> 00:11:20.740
Before, and you still can do it,

276
00:11:20.740 --> 00:11:22.180
you needed a settling notebooks.

277
00:11:22.180 --> 00:11:23.500
So you needed to
deploy your own EC2

278
00:11:23.500 --> 00:11:25.345
instance and do
some configuration.

279
00:11:25.345 --> 00:11:27.760
But now, it's a very
seamless experience.

280
00:11:27.760 --> 00:11:29.575
So you go to the Glue console.

281
00:11:29.575 --> 00:11:32.605
You click on the
development endpoints.

282
00:11:32.605 --> 00:11:35.770
There, once you have
deployed your endpoints,

283
00:11:35.770 --> 00:11:37.930
you can just launch
a notebook instance,

284
00:11:37.930 --> 00:11:39.640
and you can access to it from

285
00:11:39.640 --> 00:11:41.815
here or from the
Sagemaker console.

286
00:11:41.815 --> 00:11:45.805
Okay. Let's start with
the balancing work.

287
00:11:45.805 --> 00:11:48.040
When we need to
balance a dataset,

288
00:11:48.040 --> 00:11:49.630
there are different strategies.

289
00:11:49.630 --> 00:11:53.380
One of them could be just
duplicate records of

290
00:11:53.380 --> 00:11:57.070
the categories that have
less reviews in this case.

291
00:11:57.070 --> 00:11:59.260
Another approach could be just to

292
00:11:59.260 --> 00:12:02.485
remove a lot of all
of these records.

293
00:12:02.485 --> 00:12:05.020
There are other
sophisticated stuff

294
00:12:05.020 --> 00:12:06.370
that data scientists do,

295
00:12:06.370 --> 00:12:10.420
but usually, those do not
work so well with text.

296
00:12:10.420 --> 00:12:12.310
So in this session,

297
00:12:12.310 --> 00:12:16.210
we're going to go ahead
and remove some records.

298
00:12:16.210 --> 00:12:20.830
So how many and which
ones do we remove?

299
00:12:20.830 --> 00:12:22.660
So we're going to equalize on

300
00:12:22.660 --> 00:12:24.925
the category with
the lowest counts.

301
00:12:24.925 --> 00:12:26.440
That means that we're
going to remove actually

302
00:12:26.440 --> 00:12:28.525
a lot of these reviews.

303
00:12:28.525 --> 00:12:30.220
It's also going to help us

304
00:12:30.220 --> 00:12:32.290
with this exercise because
we're going to work

305
00:12:32.290 --> 00:12:34.870
with much less data which means

306
00:12:34.870 --> 00:12:38.275
that we are going to be able
to train models much faster.

307
00:12:38.275 --> 00:12:41.200
We're going to remove
just randomly.

308
00:12:41.200 --> 00:12:43.940
So this is what we need to do.

309
00:12:43.940 --> 00:12:47.925
We need to find the category
with the lowest count first.

310
00:12:47.925 --> 00:12:50.460
Then, calculate a
factor that is going

311
00:12:50.460 --> 00:12:53.970
to be assembling factor
for each other category.

312
00:12:53.970 --> 00:12:55.815
Then, we need to take

313
00:12:55.815 --> 00:13:00.100
a sample of N rows
of each category.

314
00:13:00.100 --> 00:13:03.310
Then, we need to write those
into S3 so they can be

315
00:13:03.310 --> 00:13:06.790
consumed by another Glue job.

316
00:13:06.790 --> 00:13:09.460
So the first job is
going to be to find

317
00:13:09.460 --> 00:13:12.460
the lowest count and
calculate a sampling factor.

318
00:13:12.460 --> 00:13:14.440
When we start with Glue,

319
00:13:14.440 --> 00:13:16.615
we need to get a dynamic frame.

320
00:13:16.615 --> 00:13:18.820
This dynamic frame is going to be

321
00:13:18.820 --> 00:13:21.205
used to read data from S3.

322
00:13:21.205 --> 00:13:23.410
But in this case, we're going
to use the Glue catalog,

323
00:13:23.410 --> 00:13:25.720
so we don't need to define
an S3 bucket or anything.

324
00:13:25.720 --> 00:13:27.520
We just point to the database and

325
00:13:27.520 --> 00:13:30.250
the table that we got
from the crawler.

326
00:13:30.250 --> 00:13:32.170
Now, a dynamic frame is

327
00:13:32.170 --> 00:13:34.255
very similar to a
Spark DataFrame.

328
00:13:34.255 --> 00:13:38.140
But AWS Glue provides
some benefits on top of

329
00:13:38.140 --> 00:13:40.570
a regular DataFrame so

330
00:13:40.570 --> 00:13:43.090
it can help you deal
with some daily data.

331
00:13:43.090 --> 00:13:45.595
Once we have this data source,

332
00:13:45.595 --> 00:13:47.110
we're going to convert
it to a regular

333
00:13:47.110 --> 00:13:48.820
Spark DataFrame because
we're going to use

334
00:13:48.820 --> 00:13:51.130
a method in Spark DataFrame

335
00:13:51.130 --> 00:13:53.590
that is not available,
the DynamicFrame.

336
00:13:53.590 --> 00:13:55.810
This case is grouped
by. So we're going to

337
00:13:55.810 --> 00:13:58.270
do the group by counts.

338
00:13:58.270 --> 00:14:00.295
Then, we're going to
collect the result.

339
00:14:00.295 --> 00:14:01.825
That is going to give us

340
00:14:01.825 --> 00:14:04.735
how many reviews we
have per category.

341
00:14:04.735 --> 00:14:07.630
Once we have that, we

342
00:14:07.630 --> 00:14:11.575
find the category that
has the least reviews,

343
00:14:11.575 --> 00:14:15.550
and then we calculate a
factor for each category.

344
00:14:15.550 --> 00:14:18.460
Now, once we have the factors,

345
00:14:18.460 --> 00:14:23.065
we need to take a sample of
N reviews for each category.

346
00:14:23.065 --> 00:14:27.490
So we have an empty list
with the samples and we go

347
00:14:27.490 --> 00:14:32.140
through every category and
every factor for the category.

348
00:14:32.140 --> 00:14:34.210
Again, we read from

349
00:14:34.210 --> 00:14:37.240
the database and the table
that we got from the crawler.

350
00:14:37.240 --> 00:14:40.430
Now, this is a very
important lesson

351
00:14:40.430 --> 00:14:41.825
I learned by doing this.

352
00:14:41.825 --> 00:14:47.270
Push_down_ predicate allows
us to push a query down to

353
00:14:47.270 --> 00:14:50.645
the service in a way that it can

354
00:14:50.645 --> 00:14:54.420
use it to leverage the
partitions in the data.

355
00:14:54.420 --> 00:14:56.305
As we said at the beginning,

356
00:14:56.305 --> 00:15:00.530
the dataset is partitioned
by product category.

357
00:15:00.530 --> 00:15:03.920
By doing this, we're
going to increase

358
00:15:03.920 --> 00:15:07.250
the performance of
these queries by a lot.

359
00:15:07.250 --> 00:15:10.280
By a lot, I mean that I
used to run this with

360
00:15:10.280 --> 00:15:13.745
60 data processing
units and it took me,

361
00:15:13.745 --> 00:15:18.710
without the push_down_predicate,
five hours to run this.

362
00:15:18.710 --> 00:15:21.500
By putting that
simple line of code,

363
00:15:21.500 --> 00:15:23.655
I got it down to four minutes.

364
00:15:23.655 --> 00:15:26.590
So now that we have a reader,

365
00:15:26.590 --> 00:15:28.665
a DynamicFrame used to read,

366
00:15:28.665 --> 00:15:30.905
we have to take a sample of

367
00:15:30.905 --> 00:15:33.875
each category using the factors
that we just calculated.

368
00:15:33.875 --> 00:15:35.990
We do that with the sample method

369
00:15:35.990 --> 00:15:37.580
existing in a DataFrame.

370
00:15:37.580 --> 00:15:40.490
So this is why we first
convert the sample into

371
00:15:40.490 --> 00:15:43.745
a DataFrame and then get
the sample out of it.

372
00:15:43.745 --> 00:15:47.940
We end up by adding the
sample to a list of samples.

373
00:15:47.940 --> 00:15:49.640
Then we can move
to the next step.

374
00:15:49.640 --> 00:15:52.215
We need to write those into S3.

375
00:15:52.215 --> 00:15:55.415
For that, we first are
going to do a union

376
00:15:55.415 --> 00:15:58.970
of every single DataFrame
that we used to calculate it.

377
00:15:58.970 --> 00:16:00.560
Once we have that, we're going to

378
00:16:00.560 --> 00:16:03.080
need a writer with
a DynamicFrame.

379
00:16:03.080 --> 00:16:05.420
So we convert back
from DataFrame to

380
00:16:05.420 --> 00:16:08.115
DynamicFrame and we use

381
00:16:08.115 --> 00:16:10.995
a DynamicFrame writer
to put that into S3.

382
00:16:10.995 --> 00:16:12.720
Now, the format we're
going to use is

383
00:16:12.720 --> 00:16:14.480
parquet because it's
very efficient and

384
00:16:14.480 --> 00:16:18.905
it leverages the partitioning
that the dataset had.

385
00:16:18.905 --> 00:16:23.395
As you see, writing parquet
with Glue is very simple.

386
00:16:23.395 --> 00:16:25.480
We take the DynamicFrame writer

387
00:16:25.480 --> 00:16:28.160
and we just specify the format.

388
00:16:28.160 --> 00:16:30.155
We specify the target,

389
00:16:30.155 --> 00:16:33.050
that means the S3
bucket and prefix,

390
00:16:33.050 --> 00:16:34.985
and the partition key.

391
00:16:34.985 --> 00:16:39.205
That's about it. Now, once
we run this job script,

392
00:16:39.205 --> 00:16:40.960
then we will have

393
00:16:40.960 --> 00:16:44.930
exactly what we're looking
for, a balanced dataset.

394
00:16:45.930 --> 00:16:48.010
With this, we finish

395
00:16:48.010 --> 00:16:49.510
the Integration phase and can

396
00:16:49.510 --> 00:16:52.010
move on into the Training phase.

397
00:16:52.080 --> 00:16:55.810
For Training, we need to choose
an algorithm and prepare

398
00:16:55.810 --> 00:16:59.260
the data for it to be
consumed by the algorithm.

399
00:16:59.260 --> 00:17:02.500
So the algorithm that I
chose and you can use is

400
00:17:02.500 --> 00:17:04.240
BlazingText which is part of

401
00:17:04.240 --> 00:17:06.595
the built-in
SageMaker algorithms.

402
00:17:06.595 --> 00:17:08.860
BlazingText comes in two modes.

403
00:17:08.860 --> 00:17:11.680
One is unsupervised
which allows you to

404
00:17:11.680 --> 00:17:15.145
take text and get word
embeddings out of it.

405
00:17:15.145 --> 00:17:16.630
A word embedding is basically

406
00:17:16.630 --> 00:17:20.125
a representation of a word
in the form of a vector.

407
00:17:20.125 --> 00:17:21.745
As you may know, Machine Learning

408
00:17:21.745 --> 00:17:24.445
requires numbers
in order to work.

409
00:17:24.445 --> 00:17:27.145
Then we have the supervised mode.

410
00:17:27.145 --> 00:17:30.700
This extends a famous
fastText classifier

411
00:17:30.700 --> 00:17:32.290
and is used for multi class and

412
00:17:32.290 --> 00:17:34.135
multi label text classification.

413
00:17:34.135 --> 00:17:37.090
This is the model we will use.

414
00:17:37.090 --> 00:17:40.780
Now, in order for
BlazingText to work,

415
00:17:40.780 --> 00:17:43.675
we need to provide it with
data in a specific way.

416
00:17:43.675 --> 00:17:45.910
This is what BlazingText actually

417
00:17:45.910 --> 00:17:48.280
needs to consume
to train a model.

418
00:17:48.280 --> 00:17:51.385
So it requires a single
preprocessed text file

419
00:17:51.385 --> 00:17:54.190
with space separated tokens.

420
00:17:54.190 --> 00:17:58.285
A token is basically just a
word or a punctuation symbol.

421
00:17:58.285 --> 00:18:00.520
Then we need, in that file,

422
00:18:00.520 --> 00:18:02.590
a single sentence per line

423
00:18:02.590 --> 00:18:05.575
and with the labels
alongside the sentence.

424
00:18:05.575 --> 00:18:07.600
So a label is just
a word which is

425
00:18:07.600 --> 00:18:09.520
prefixed by the
string underscore,

426
00:18:09.520 --> 00:18:11.650
underscore label,
underscore, underscore.

427
00:18:11.650 --> 00:18:14.680
Finally, we need
classification and validation.

428
00:18:14.680 --> 00:18:17.410
It's an option but we'll
definitely make use of it.

429
00:18:17.410 --> 00:18:19.570
So these are the things
that we need to do.

430
00:18:19.570 --> 00:18:20.920
We need to select

431
00:18:20.920 --> 00:18:22.255
only the fields that
we're going to use.

432
00:18:22.255 --> 00:18:25.000
If you remember, we
have this table with

433
00:18:25.000 --> 00:18:28.120
many fields and we
just need two of them.

434
00:18:28.120 --> 00:18:31.330
Then we need to tokenize
the review body

435
00:18:31.330 --> 00:18:33.700
and prepend the label

436
00:18:33.700 --> 00:18:35.560
with the right format
that is required.

437
00:18:35.560 --> 00:18:37.210
Then we need to split

438
00:18:37.210 --> 00:18:40.600
the dataset into
training, validation,

439
00:18:40.600 --> 00:18:42.220
and in this case we're
going to also use

440
00:18:42.220 --> 00:18:48.175
a test subset so we can then
show how good our model is.

441
00:18:48.175 --> 00:18:50.980
Then finally, we need
to write this subset in

442
00:18:50.980 --> 00:18:53.095
a single object because

443
00:18:53.095 --> 00:18:55.930
Bayesian text requires
a single file in S3.

444
00:18:55.930 --> 00:18:57.505
So basically, what we need

445
00:18:57.505 --> 00:18:59.755
is something that
looks like that.

446
00:18:59.755 --> 00:19:01.720
Something with a label,

447
00:19:01.720 --> 00:19:03.985
then a string of tokens,

448
00:19:03.985 --> 00:19:06.685
and one sentence per line.

449
00:19:06.685 --> 00:19:09.490
So as before, in order
to work with this,

450
00:19:09.490 --> 00:19:11.650
we're going to have
a new different job.

451
00:19:11.650 --> 00:19:13.285
So we need a new script.

452
00:19:13.285 --> 00:19:17.395
We start, again, with the
dynamic frame reader.

453
00:19:17.395 --> 00:19:20.500
We point to the
database and table but

454
00:19:20.500 --> 00:19:24.085
in this case we're pointing
to the balanced dataset.

455
00:19:24.085 --> 00:19:27.235
Then we just select the
fields that we want.

456
00:19:27.235 --> 00:19:30.775
We want the product
category under review body.

457
00:19:30.775 --> 00:19:33.865
Then we can move to
the tokenization.

458
00:19:33.865 --> 00:19:37.660
How do we do that?
So the dynamic frame

459
00:19:37.660 --> 00:19:39.580
contains a method called map.

460
00:19:39.580 --> 00:19:43.780
So you can apply map into
the dataset and that means

461
00:19:43.780 --> 00:19:45.400
that every single role in

462
00:19:45.400 --> 00:19:48.160
that data is going to
be applied a function.

463
00:19:48.160 --> 00:19:50.515
The function in this
case is called tokenize.

464
00:19:50.515 --> 00:19:52.525
The tokenized function, it takes

465
00:19:52.525 --> 00:19:54.970
the product category
from the dynamic record,

466
00:19:54.970 --> 00:19:56.530
it puts in lowercase,

467
00:19:56.530 --> 00:19:59.395
then prepends the label,

468
00:19:59.395 --> 00:20:03.625
and transforms review
body in a specific way.

469
00:20:03.625 --> 00:20:05.560
What we're doing is just use

470
00:20:05.560 --> 00:20:07.870
a tokenization library from

471
00:20:07.870 --> 00:20:10.960
a famous NLP library called NLTK.

472
00:20:10.960 --> 00:20:13.150
So we take a tokenizer,

473
00:20:13.150 --> 00:20:16.630
there's a family of
them available in NLTK,

474
00:20:16.630 --> 00:20:20.485
and just apply that
to the string,

475
00:20:20.485 --> 00:20:24.055
and we then just need to
do a join using a space.

476
00:20:24.055 --> 00:20:26.695
Then once we have tokenized

477
00:20:26.695 --> 00:20:30.325
every single sentence and
preparing did with a label,

478
00:20:30.325 --> 00:20:33.325
we need to split the
dataset into the training,

479
00:20:33.325 --> 00:20:35.035
the validation, and the test.

480
00:20:35.035 --> 00:20:36.430
For that, we need to use

481
00:20:36.430 --> 00:20:37.990
a random split function

482
00:20:37.990 --> 00:20:40.855
available in the
DataFrame from Spark.

483
00:20:40.855 --> 00:20:44.290
So we convert the
tokenized dynamic frame

484
00:20:44.290 --> 00:20:46.930
into a DataFrame and
apply the random split.

485
00:20:46.930 --> 00:20:48.790
I just chose 60 percent of

486
00:20:48.790 --> 00:20:51.595
the reviews will be
used for training,

487
00:20:51.595 --> 00:20:55.345
20 percent for validation
and 20 percent for testing.

488
00:20:55.345 --> 00:20:59.515
So now we need to write each
of these subsets into S3.

489
00:20:59.515 --> 00:21:01.450
If you'll remember
the requirements,

490
00:21:01.450 --> 00:21:03.580
we needed just a single file.

491
00:21:03.580 --> 00:21:06.130
The source data is
actually partitioned.

492
00:21:06.130 --> 00:21:08.530
So that means that we
have multiple files with

493
00:21:08.530 --> 00:21:11.680
multiple prefixes.

494
00:21:11.680 --> 00:21:15.775
In this case, we really need
just one. How do we do that?

495
00:21:15.775 --> 00:21:17.080
Well, we can use

496
00:21:17.080 --> 00:21:19.320
the repetition method in

497
00:21:19.320 --> 00:21:22.380
a DataFrame to just
get exactly one file,

498
00:21:22.380 --> 00:21:23.865
one object in S3.

499
00:21:23.865 --> 00:21:26.010
Once we have that,
we just, again,

500
00:21:26.010 --> 00:21:28.380
take a dynamic frame writer.

501
00:21:28.380 --> 00:21:29.820
In this case, we're
going to use this

502
00:21:29.820 --> 00:21:31.740
CSV format because we just have

503
00:21:31.740 --> 00:21:35.205
two columns which is the
label and the review,

504
00:21:35.205 --> 00:21:37.575
and we just need to
separate them by space.

505
00:21:37.575 --> 00:21:39.585
So we have the
separator as a space,

506
00:21:39.585 --> 00:21:40.965
we don't need headers,

507
00:21:40.965 --> 00:21:42.330
and we don't need to quote

508
00:21:42.330 --> 00:21:45.865
our text in there, and that's it.

509
00:21:45.865 --> 00:21:48.415
After this, we will
have the data available

510
00:21:48.415 --> 00:21:51.850
and ready for SageMaker
to use with BlazingText.

511
00:21:51.850 --> 00:21:53.995
Now, we need to train

512
00:21:53.995 --> 00:21:55.510
that model and pass all

513
00:21:55.510 --> 00:21:57.475
of that data through
the algorithm.

514
00:21:57.475 --> 00:22:01.020
For this, we're going to
need a SageMaker Estimator.

515
00:22:01.020 --> 00:22:03.840
We're going to use the generic
Amazon SageMaker Estimator

516
00:22:03.840 --> 00:22:06.585
because right now
in the Python SDK,

517
00:22:06.585 --> 00:22:09.480
there's not a specific one
available for BlazingText.

518
00:22:09.480 --> 00:22:11.910
So we're just going to
configure that estimator with

519
00:22:11.910 --> 00:22:15.610
the container that contains
the BlazingText algorithm,

520
00:22:15.610 --> 00:22:17.710
we're going to provide
the channels that

521
00:22:17.710 --> 00:22:20.125
are going to be used for
training and validating.

522
00:22:20.125 --> 00:22:22.285
We're going to use
a specific role.

523
00:22:22.285 --> 00:22:24.715
We're going to provide
instance configuration,

524
00:22:24.715 --> 00:22:25.945
hyperparameters,

525
00:22:25.945 --> 00:22:27.670
and finally the location where

526
00:22:27.670 --> 00:22:31.630
you're training model artifacts
is going to be stored.

527
00:22:31.630 --> 00:22:33.775
Let's see this in
Jupyter Notebooks.

528
00:22:33.775 --> 00:22:35.800
So let's jump into

529
00:22:35.800 --> 00:22:37.735
the console and then

530
00:22:37.735 --> 00:22:39.820
go to our a notebook
that I prepared.

531
00:22:39.820 --> 00:22:41.560
So here we are in Jupyter.

532
00:22:41.560 --> 00:22:43.630
I have a notebook prepared to do

533
00:22:43.630 --> 00:22:46.525
the training and
let's get started.

534
00:22:46.525 --> 00:22:50.920
So we start by doing
some regular inputs.

535
00:22:50.920 --> 00:22:53.410
We take a SageMakers
session that is going

536
00:22:53.410 --> 00:22:56.215
to allow us to get some defaults.

537
00:22:56.215 --> 00:22:58.615
For example, here we
get the default role.

538
00:22:58.615 --> 00:23:02.365
Then we specify the
bucket where our data is.

539
00:23:02.365 --> 00:23:06.940
We specify a prefix and
then we can proceed.

540
00:23:06.940 --> 00:23:09.700
First, we install NLTK as we

541
00:23:09.700 --> 00:23:12.850
will use it later on
for running some tests,

542
00:23:12.850 --> 00:23:15.535
and we define the data channels.

543
00:23:15.535 --> 00:23:18.835
So data channels
are just S3 inputs

544
00:23:18.835 --> 00:23:22.540
that we acquired by telling
where the training data,

545
00:23:22.540 --> 00:23:24.160
where the validation data are.

546
00:23:24.160 --> 00:23:27.880
Then we define the
upper location.

547
00:23:27.880 --> 00:23:32.065
This is the S3 bucket
and the prefix where

548
00:23:32.065 --> 00:23:34.930
our training artifacts
are going to be

549
00:23:34.930 --> 00:23:39.055
stored and take the region,

550
00:23:39.055 --> 00:23:41.470
take a BlazingText Container.

551
00:23:41.470 --> 00:23:45.340
We take the latest
version and we can get

552
00:23:45.340 --> 00:23:49.405
started with the estimator.

553
00:23:49.405 --> 00:23:51.490
So we define our estimator,

554
00:23:51.490 --> 00:23:55.240
we define Amazon
SageMaker Estimator.

555
00:23:55.240 --> 00:23:58.105
That's the general one.

556
00:23:58.105 --> 00:24:00.805
We specify the
container, the role.

557
00:24:00.805 --> 00:24:03.100
We're going to just
use one instance.

558
00:24:03.100 --> 00:24:05.725
We're going to use a
C5 for extra-large.

559
00:24:05.725 --> 00:24:07.870
The reason for this is
because our dataset

560
00:24:07.870 --> 00:24:10.300
is actually smaller
than one gigabyte.

561
00:24:10.300 --> 00:24:12.850
Recommendation would
be to use a P2 or

562
00:24:12.850 --> 00:24:16.615
P3 instance GPUs if our
dataset was a bit bigger.

563
00:24:16.615 --> 00:24:22.435
Then we specify some
other attributes

564
00:24:22.435 --> 00:24:24.520
and that's about it.

565
00:24:24.520 --> 00:24:26.650
Then we can define
the hyperparameters

566
00:24:26.650 --> 00:24:28.285
for this estimator.

567
00:24:28.285 --> 00:24:30.550
But we're not going to just stick

568
00:24:30.550 --> 00:24:32.965
to the static set
of hyperparameters.

569
00:24:32.965 --> 00:24:34.690
We're going to use
a very nice feature

570
00:24:34.690 --> 00:24:38.050
from SageMaker called
hyperparameter tuning.

571
00:24:38.050 --> 00:24:39.745
All hyperparameter
tuning is going to

572
00:24:39.745 --> 00:24:42.175
do is allow us to define

573
00:24:42.175 --> 00:24:46.660
ranges of hyperparameters and
then run training models,

574
00:24:46.660 --> 00:24:50.035
training jobs in
parallel and explore

575
00:24:50.035 --> 00:24:51.880
the results to try to

576
00:24:51.880 --> 00:24:55.285
find better hyperparameter
configurations.

577
00:24:55.285 --> 00:24:57.580
So what we need to do
is just define ranges.

578
00:24:57.580 --> 00:24:59.410
For example, we say that
learning range rate is going

579
00:24:59.410 --> 00:25:02.710
to be between 0.01 and 0.08

580
00:25:02.710 --> 00:25:05.620
or the number of dimensions
into vectors and the

581
00:25:05.620 --> 00:25:07.000
word embeddings that
are going to be used by

582
00:25:07.000 --> 00:25:09.684
the algorithm between
a 100 and 200,

583
00:25:09.684 --> 00:25:12.490
and then we define
the objective metric.

584
00:25:12.490 --> 00:25:15.910
So this is a metric that
hyperparameter tuning will

585
00:25:15.910 --> 00:25:20.395
use to determine which
model is actually better.

586
00:25:20.395 --> 00:25:22.285
So once we define this,

587
00:25:22.285 --> 00:25:27.235
we create a tuner object
plus the estimator,

588
00:25:27.235 --> 00:25:31.075
plus the hyperparameter ranges.

589
00:25:31.075 --> 00:25:35.050
We'd say how many jobs
are going to be run

590
00:25:35.050 --> 00:25:38.545
and how many in
parallel. So that's it.

591
00:25:38.545 --> 00:25:42.355
We define the tuner and
we call its fit function,

592
00:25:42.355 --> 00:25:45.445
and it will begin with the job.

593
00:25:45.445 --> 00:25:48.100
We're here in the
SageMaker console and

594
00:25:48.100 --> 00:25:51.415
I have already run a few
hyperparameter tuning jobs.

595
00:25:51.415 --> 00:25:53.140
So we pick one of

596
00:25:53.140 --> 00:25:55.315
the hyperparameter
tuning jobs that I have.

597
00:25:55.315 --> 00:25:59.350
We see that it actually

598
00:25:59.350 --> 00:26:02.635
span 10 different jobs

599
00:26:02.635 --> 00:26:04.915
and actually was running
two in parallel.

600
00:26:04.915 --> 00:26:08.725
Then you see there is the
objective metric here.

601
00:26:08.725 --> 00:26:10.810
If we go to the bottom,

602
00:26:10.810 --> 00:26:12.940
we see that it started with 62.9,

603
00:26:12.940 --> 00:26:14.890
then 64, and we started to

604
00:26:14.890 --> 00:26:17.200
see some improvements as it went.

605
00:26:17.200 --> 00:26:20.950
Then we can actually see
the best running job

606
00:26:20.950 --> 00:26:24.940
and the best running job
it tells us that it used,

607
00:26:24.940 --> 00:26:30.505
for example, vector
dimension of 183,

608
00:26:30.505 --> 00:26:34.030
word-ngrams of two, and

609
00:26:34.030 --> 00:26:36.400
you can actually see
everything that was

610
00:26:36.400 --> 00:26:38.620
used for training
this specific one.

611
00:26:38.620 --> 00:26:41.935
So with this, we already
finished the training.

612
00:26:41.935 --> 00:26:43.495
So we've covered

613
00:26:43.495 --> 00:26:45.685
another phase of our
Machine Learning model.

614
00:26:45.685 --> 00:26:48.745
Now, for deploying this model,

615
00:26:48.745 --> 00:26:51.295
we just can create a model,

616
00:26:51.295 --> 00:26:53.730
clicking here, and
then from the console

617
00:26:53.730 --> 00:26:58.060
itself where we see the models.

618
00:26:58.590 --> 00:27:01.915
When we see the model,
we can just click on it

619
00:27:01.915 --> 00:27:05.095
and create an endpoint.

620
00:27:05.095 --> 00:27:07.735
That would deploy our model
into a specific input

621
00:27:07.735 --> 00:27:10.585
that we can use for
running inferences.

622
00:27:10.585 --> 00:27:12.430
Now, that is the way to
do it in the console,

623
00:27:12.430 --> 00:27:15.580
but definitely, we can
also do it using code.

624
00:27:15.580 --> 00:27:18.445
So we go back to the
Jupyter notebook.

625
00:27:18.445 --> 00:27:19.975
So by using code,

626
00:27:19.975 --> 00:27:23.305
we can just call the
tuner deployed method

627
00:27:23.305 --> 00:27:25.510
with a specific counter of

628
00:27:25.510 --> 00:27:27.970
instances and specific
instance type,

629
00:27:27.970 --> 00:27:29.755
endpoint name and that's it.

630
00:27:29.755 --> 00:27:31.450
Then we're ready, once it's

631
00:27:31.450 --> 00:27:33.685
deployed ready to run
inferences against it.

632
00:27:33.685 --> 00:27:36.180
So let's do remember

633
00:27:36.180 --> 00:27:38.850
those three that we said
that will be challenging,

634
00:27:38.850 --> 00:27:41.130
let's see how the modal response.

635
00:27:41.130 --> 00:27:45.255
So we see we can actually run
inferences and we can get

636
00:27:45.255 --> 00:27:49.795
a single prediction
for each sentence.

637
00:27:49.795 --> 00:27:53.200
In the case of our example,

638
00:27:53.200 --> 00:27:54.670
we have the first
sentence that it

639
00:27:54.670 --> 00:27:57.295
says is lawn and garden.

640
00:27:57.295 --> 00:27:59.680
That's actually
not the right one.

641
00:27:59.680 --> 00:28:01.465
Then the second it says toys,

642
00:28:01.465 --> 00:28:05.065
that was the most obvious
one but is not right either.

643
00:28:05.065 --> 00:28:06.970
The third one it

644
00:28:06.970 --> 00:28:09.145
predicted as kitchen,
which is the right one.

645
00:28:09.145 --> 00:28:10.450
So probably we're talking about

646
00:28:10.450 --> 00:28:12.640
a knife or something similar.

647
00:28:12.640 --> 00:28:15.955
But we can also get
more than one label,

648
00:28:15.955 --> 00:28:18.340
for instance, for each sentence.

649
00:28:18.340 --> 00:28:21.070
If we see the first
sentence it says,

650
00:28:21.070 --> 00:28:25.735
it's lawn and garden with
almost 40 percent probability,

651
00:28:25.735 --> 00:28:27.580
but the second is watches

652
00:28:27.580 --> 00:28:29.995
with almost 20 percent and
that actually is right.

653
00:28:29.995 --> 00:28:32.800
That review was about a watch.

654
00:28:32.800 --> 00:28:35.140
Then, if we continue
with the second,

655
00:28:35.140 --> 00:28:37.810
it says it's toys, apparel,

656
00:28:37.810 --> 00:28:39.700
or luggage and none

657
00:28:39.700 --> 00:28:41.260
of them are right because
we're talking about

658
00:28:41.260 --> 00:28:43.345
a gift card and
don't ask me why a

659
00:28:43.345 --> 00:28:45.520
10-year-old gets excited
with a gift card,

660
00:28:45.520 --> 00:28:47.950
but that was the review.

661
00:28:47.950 --> 00:28:52.150
Finally, we also see that
that last sentence could be

662
00:28:52.150 --> 00:28:54.250
also for the home
improvement or majored

663
00:28:54.250 --> 00:28:57.160
appliances which
actually makes sense.

664
00:28:57.160 --> 00:28:59.965
This is just using
some single examples.

665
00:28:59.965 --> 00:29:01.810
What we want is to

666
00:29:01.810 --> 00:29:05.125
build a confusion matrix
out of the training set.

667
00:29:05.125 --> 00:29:10.495
So for that, we can use
some code we can run

668
00:29:10.495 --> 00:29:15.250
all the training set through

669
00:29:15.250 --> 00:29:17.590
the predictions and then we can

670
00:29:17.590 --> 00:29:21.740
build our confusion matrix.

671
00:29:21.960 --> 00:29:25.840
Since I said that I iterated
a few times over it,

672
00:29:25.840 --> 00:29:28.720
I'm going to show you different
metrics is that I got.

673
00:29:28.720 --> 00:29:30.370
With that, we have deployed

674
00:29:30.370 --> 00:29:32.560
our model and fulfilled
the deployment.

675
00:29:32.560 --> 00:29:34.435
Now, are we finished?

676
00:29:34.435 --> 00:29:36.235
Well, as I said at the beginning,

677
00:29:36.235 --> 00:29:38.770
it's crucial to iterate over

678
00:29:38.770 --> 00:29:40.630
this model training in

679
00:29:40.630 --> 00:29:42.325
order to get a more
accurate model.

680
00:29:42.325 --> 00:29:44.500
So I'm going to walk
you over some of

681
00:29:44.500 --> 00:29:46.600
the things that are
learned by doing this.

682
00:29:46.600 --> 00:29:48.520
In one of the first
iterations that I

683
00:29:48.520 --> 00:29:51.760
did build in this text
classification model,

684
00:29:51.760 --> 00:29:54.295
I got this confusion matrix.

685
00:29:54.295 --> 00:29:57.550
Now, can you spot
something weird about it?

686
00:29:57.550 --> 00:30:01.660
Well, I see a column that
is darker than the rest

687
00:30:01.660 --> 00:30:03.130
and that may show

688
00:30:03.130 --> 00:30:05.950
some bias into predicting
a specific label.

689
00:30:05.950 --> 00:30:08.860
In this case, I didn't know
exactly where to look,

690
00:30:08.860 --> 00:30:10.540
but so I started
exploring a bit of

691
00:30:10.540 --> 00:30:13.570
the training dataset and
then I saw that actually

692
00:30:13.570 --> 00:30:15.969
when I repartitioned previously

693
00:30:15.969 --> 00:30:19.195
that got the file sorted
by product category.

694
00:30:19.195 --> 00:30:21.520
Which means that when

695
00:30:21.520 --> 00:30:23.905
I pass that data
through the algorithm,

696
00:30:23.905 --> 00:30:26.770
it saw first one category,

697
00:30:26.770 --> 00:30:28.195
then it saw another one,

698
00:30:28.195 --> 00:30:31.750
it saw another one and
there was a last one.

699
00:30:31.750 --> 00:30:34.090
The last one was
what the algorithm

700
00:30:34.090 --> 00:30:36.700
learned in the last time

701
00:30:36.700 --> 00:30:38.650
and that it was what's

702
00:30:38.650 --> 00:30:41.860
shaped the final results
of that algorithm.

703
00:30:41.860 --> 00:30:45.370
So when I checked the file,

704
00:30:45.370 --> 00:30:47.290
I saw that that category,

705
00:30:47.290 --> 00:30:49.915
the last one, was exactly
this one in the column.

706
00:30:49.915 --> 00:30:52.240
What do you need to
do to avoid that?

707
00:30:52.240 --> 00:30:53.755
Well, we need to shuffle it.

708
00:30:53.755 --> 00:30:55.375
Remember that you repartition,

709
00:30:55.375 --> 00:30:57.610
the only thing that you
need to do is order by

710
00:30:57.610 --> 00:31:00.040
a random value and
then repartition.

711
00:31:00.040 --> 00:31:02.395
Once you have that,
then everything

712
00:31:02.395 --> 00:31:04.900
in the training dataset
and the validation one,

713
00:31:04.900 --> 00:31:06.175
it will be mixed.

714
00:31:06.175 --> 00:31:08.110
So you're going to
train the algorithm now

715
00:31:08.110 --> 00:31:11.530
learning about each category
in every iteration.

716
00:31:11.530 --> 00:31:14.695
The results after
that are much better.

717
00:31:14.695 --> 00:31:17.800
We still see some darker sites,

718
00:31:17.800 --> 00:31:20.815
but they could be for
many different reasons.

719
00:31:20.815 --> 00:31:23.800
Now, one thing that I learned
is that I should have

720
00:31:23.800 --> 00:31:25.480
explored the data a bit better

721
00:31:25.480 --> 00:31:27.625
before running into all of this.

722
00:31:27.625 --> 00:31:30.010
So then I thought, well,

723
00:31:30.010 --> 00:31:33.820
let's see what reviews are we
getting and then I decided

724
00:31:33.820 --> 00:31:38.125
to select the ones that were
shorter than 20 characters.

725
00:31:38.125 --> 00:31:42.369
So if you see them,
well, excellent,

726
00:31:42.369 --> 00:31:45.040
great, good that doesn't

727
00:31:45.040 --> 00:31:47.665
tell anything about
a specific category.

728
00:31:47.665 --> 00:31:49.930
They're actually
very helpless here.

729
00:31:49.930 --> 00:31:51.970
They may be helpful for a review,

730
00:31:51.970 --> 00:31:53.845
but for our purpose,

731
00:31:53.845 --> 00:31:55.510
we can get rid of them because

732
00:31:55.510 --> 00:31:57.880
this can be about anything.

733
00:31:57.880 --> 00:31:59.635
So once we do that,

734
00:31:59.635 --> 00:32:02.680
then we get a slightly
better model.

735
00:32:02.680 --> 00:32:04.780
Now, this model offered us

736
00:32:04.780 --> 00:32:08.140
about 65 percent
accuracy which is

737
00:32:08.140 --> 00:32:09.820
pretty good for a
model that has to

738
00:32:09.820 --> 00:32:11.995
select between 36 categories.

739
00:32:11.995 --> 00:32:15.520
Let's see how much
actually this would cost.

740
00:32:15.520 --> 00:32:18.280
I've taken the main
drivers for cars,

741
00:32:18.280 --> 00:32:21.265
in this case, which are AWS
Glue and Amazon SageMaker.

742
00:32:21.265 --> 00:32:23.950
So first, we start with
the development endpoint.

743
00:32:23.950 --> 00:32:26.785
We actually can run these
in around three hours.

744
00:32:26.785 --> 00:32:28.600
So I selected an endpoint

745
00:32:28.600 --> 00:32:30.580
with just two Data
Processing Unit,

746
00:32:30.580 --> 00:32:33.325
which is what you pay for.

747
00:32:33.325 --> 00:32:39.850
The cost at the moment
is $0.44 per DPU hour.

748
00:32:39.850 --> 00:32:43.750
So that would total for $2.64.

749
00:32:43.750 --> 00:32:45.580
Then we have the sampling job and

750
00:32:45.580 --> 00:32:49.120
the preparation job and I
select a different DPUs because

751
00:32:49.120 --> 00:32:52.210
parallelization and the type
of data that we're working

752
00:32:52.210 --> 00:32:55.345
with and it took six
minutes to just sample.

753
00:32:55.345 --> 00:32:57.595
So think about it. It's taken

754
00:32:57.595 --> 00:32:59.605
50 gigabytes of
data and actually,

755
00:32:59.605 --> 00:33:01.765
they were in the
North Virginia region

756
00:33:01.765 --> 00:33:04.180
and when I moved
it and copied it,

757
00:33:04.180 --> 00:33:07.525
it was actually in Ireland.

758
00:33:07.525 --> 00:33:09.190
So it took about six minutes

759
00:33:09.190 --> 00:33:11.440
to just take those
samples and move them

760
00:33:11.440 --> 00:33:13.960
over to Ireland across

761
00:33:13.960 --> 00:33:17.335
regions and that would
cost about $2.2.

762
00:33:17.335 --> 00:33:20.770
Then the preparation one
only 20 DPUs, in this case,

763
00:33:20.770 --> 00:33:25.630
it took about 25 minutes and
that will amount to $3.67.

764
00:33:25.630 --> 00:33:27.490
For the notebook instance,

765
00:33:27.490 --> 00:33:30.460
I just chose the smallest
one that would run through

766
00:33:30.460 --> 00:33:32.035
the workshop about in three hours

767
00:33:32.035 --> 00:33:34.210
and that would just be $0.15.

768
00:33:34.210 --> 00:33:38.350
Now, for the tuning job which
is 10 of this tuning jobs,

769
00:33:38.350 --> 00:33:39.970
but internally we would average

770
00:33:39.970 --> 00:33:42.430
about almost three hours with 1

771
00:33:42.430 --> 00:33:44.740
mlc.5.4xlarge for each of

772
00:33:44.740 --> 00:33:47.695
the training jobs and
that would be $3.19.

773
00:33:47.695 --> 00:33:49.780
So in total, you
can actually build

774
00:33:49.780 --> 00:33:54.140
a model for a barely
more than $10.

775
00:33:54.810 --> 00:33:58.900
Now, you can optimize on this.

776
00:33:58.900 --> 00:34:01.945
So when you actually
run glue jobs,

777
00:34:01.945 --> 00:34:04.660
you can get metrics and
these metrics can tell

778
00:34:04.660 --> 00:34:08.305
you things about how many
executors do you need.

779
00:34:08.305 --> 00:34:12.279
Executors actually can
be translated into DPUs

780
00:34:12.279 --> 00:34:13.720
and I would encourage
you to go to

781
00:34:13.720 --> 00:34:16.450
the documentation to check
how that calculation is done.

782
00:34:16.450 --> 00:34:17.650
But in this case, for example,

783
00:34:17.650 --> 00:34:19.480
we can see we could have sped up

784
00:34:19.480 --> 00:34:23.830
our sampling job by

785
00:34:23.830 --> 00:34:25.690
increasing the number of DPUs

786
00:34:25.690 --> 00:34:27.415
and that would have
taken a shorter time.

787
00:34:27.415 --> 00:34:29.185
Now, optimizing on costs.

788
00:34:29.185 --> 00:34:30.820
It could mean that, well,

789
00:34:30.820 --> 00:34:34.989
if you can run doing shorter
time by using more DPUs,

790
00:34:34.989 --> 00:34:38.050
well, you need to find
what is the sweet spot for

791
00:34:38.050 --> 00:34:41.740
you because it really
depends on the workload.

792
00:34:41.740 --> 00:34:44.560
Then, in this case,
we saw that actually,

793
00:34:44.560 --> 00:34:46.585
we were very, very close

794
00:34:46.585 --> 00:34:48.925
from the needed executors
to the active ones.

795
00:34:48.925 --> 00:34:51.355
So this cannot be
optimized so much,

796
00:34:51.355 --> 00:34:53.335
but again, this really depends.

797
00:34:53.335 --> 00:34:54.805
I hope this was helpful.

798
00:34:54.805 --> 00:34:57.800
My name is [inaudible] ,
thank you for watching.