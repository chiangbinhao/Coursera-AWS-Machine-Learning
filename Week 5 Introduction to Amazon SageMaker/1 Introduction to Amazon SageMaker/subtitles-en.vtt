WEBVTT

1
00:00:10.610 --> 00:00:13.440
Hello everyone. Welcome to

2
00:00:13.440 --> 00:00:16.260
the introductory video
of Amazon SageMaker.

3
00:00:16.260 --> 00:00:18.690
My name is Fan Li and I'm

4
00:00:18.690 --> 00:00:21.900
the senior product manager
on Amazon SageMaker team.

5
00:00:21.900 --> 00:00:25.590
Amazon SageMaker is a
fully managed service to

6
00:00:25.590 --> 00:00:28.890
help data scientists and
developers to build,

7
00:00:28.890 --> 00:00:30.300
train, and deploying

8
00:00:30.300 --> 00:00:33.600
Machine Learning models
quickly and easily.

9
00:00:33.600 --> 00:00:36.405
It has three major components.

10
00:00:36.405 --> 00:00:38.340
A hosted notebook instance,

11
00:00:38.340 --> 00:00:41.324
a distributed on-demand
training environment,

12
00:00:41.324 --> 00:00:44.704
and a model hosting
environment that is elastic,

13
00:00:44.704 --> 00:00:47.825
scalable, secure, and reliable.

14
00:00:47.825 --> 00:00:50.060
Although using all components,

15
00:00:50.060 --> 00:00:51.455
end-to-end brings you

16
00:00:51.455 --> 00:00:53.945
a seamless experience
on Amazon SageMaker,

17
00:00:53.945 --> 00:00:57.260
you have the flexibility
to use any combination of

18
00:00:57.260 --> 00:01:01.010
those components in order
to fit your own workflow.

19
00:01:01.010 --> 00:01:05.225
Amazon SageMaker provides
hosted Jupiter notebooks

20
00:01:05.225 --> 00:01:07.235
that requires no setup.

21
00:01:07.235 --> 00:01:08.765
With a few clicks on

22
00:01:08.765 --> 00:01:11.840
Amazon SageMaker console
or through APIs,

23
00:01:11.840 --> 00:01:15.200
you can create a 40
managed notebook instance,

24
00:01:15.200 --> 00:01:18.110
which comes with preloaded
data science packages

25
00:01:18.110 --> 00:01:20.614
such as popular Python libraries,

26
00:01:20.614 --> 00:01:25.250
deep learning frameworks,
Apache Spark, and so on.

27
00:01:25.250 --> 00:01:27.155
You can then start processing

28
00:01:27.155 --> 00:01:31.115
your datasets and developing
your algorithms immediately.

29
00:01:31.115 --> 00:01:34.894
If you want to use extra
packages that are not preloaded,

30
00:01:34.894 --> 00:01:37.760
you can simply PIP
install or Conda install

31
00:01:37.760 --> 00:01:41.990
them which will be persisted
in that notebook instance.

32
00:01:41.990 --> 00:01:43.650
Although you can certainly run

33
00:01:43.650 --> 00:01:46.545
training jobs in the
hosted notebook instance,

34
00:01:46.545 --> 00:01:48.440
many times, you want to

35
00:01:48.440 --> 00:01:50.765
get access to more
compute capacity,

36
00:01:50.765 --> 00:01:52.930
especially for large datasets.

37
00:01:52.930 --> 00:01:57.050
In that case, you simply select
the type and quantity of

38
00:01:57.050 --> 00:02:01.550
Amazon EC2 instances you need
and kick-off a train job.

39
00:02:01.550 --> 00:02:05.124
Amazon's SageMaker then sets
up the compute cluster,

40
00:02:05.124 --> 00:02:06.645
performs a training job,

41
00:02:06.645 --> 00:02:08.270
and tear down the cluster

42
00:02:08.270 --> 00:02:10.280
when the training
job is finished.

43
00:02:10.280 --> 00:02:12.800
So you only pay for
what you use and

44
00:02:12.800 --> 00:02:15.890
never worry about the
underlying infrastructure.

45
00:02:15.890 --> 00:02:19.430
In Amazon's SageMaker,
model training is flexible.

46
00:02:19.430 --> 00:02:21.885
You can certainly bring
arbitrary algorithms,

47
00:02:21.885 --> 00:02:24.020
either open-sourced
or developed by

48
00:02:24.020 --> 00:02:26.990
yourself in the form
of Docker images.

49
00:02:26.990 --> 00:02:29.600
Although Amazon SageMaker offers

50
00:02:29.600 --> 00:02:32.120
a range of built-in
high-performance machine

51
00:02:32.120 --> 00:02:35.390
learning algorithms that
have been optimized for

52
00:02:35.390 --> 00:02:37.490
this TPOT training
making it highly

53
00:02:37.490 --> 00:02:40.820
effective in training models
against the mark datasets.

54
00:02:40.820 --> 00:02:43.585
For those who want to train
your own neural networks,

55
00:02:43.585 --> 00:02:47.180
Amazon SageMaker makes it
super easy to directly submit

56
00:02:47.180 --> 00:02:49.955
your TensorFlow or
Apache amex nav scripts

57
00:02:49.955 --> 00:02:51.200
for this TPOT training

58
00:02:51.200 --> 00:02:52.760
and you can use alternative

59
00:02:52.760 --> 00:02:54.860
deep-learning
frameworks as well by

60
00:02:54.860 --> 00:02:57.080
packaging your own Docker images

61
00:02:57.080 --> 00:02:59.720
and bringing them to
Amazon's SageMaker.

62
00:02:59.720 --> 00:03:03.445
When you are ready to deploy
a model to production,

63
00:03:03.445 --> 00:03:05.269
you can simply indicate

64
00:03:05.269 --> 00:03:07.550
the compute resource
requirements for

65
00:03:07.550 --> 00:03:11.030
hosting the model and deploy
it with just one click.

66
00:03:11.030 --> 00:03:14.060
A HTTPS endpoint will then be

67
00:03:14.060 --> 00:03:16.220
created to achieve low latency,

68
00:03:16.220 --> 00:03:18.880
high throughput inferences.

69
00:03:18.880 --> 00:03:21.860
With Amazon SageMaker,
you can swap

70
00:03:21.860 --> 00:03:24.380
the model behind a
endpoint without

71
00:03:24.380 --> 00:03:27.140
any downtime or even
put multi models

72
00:03:27.140 --> 00:03:30.965
behind a endpoint for the
purpose of A/B testing.

73
00:03:30.965 --> 00:03:34.400
Next I'm going to show
you a quick demo.

74
00:03:34.400 --> 00:03:36.785
In this demo, I will create

75
00:03:36.785 --> 00:03:40.249
a notebook instance from
Amazon SageMaker console,

76
00:03:40.249 --> 00:03:41.720
build my workflow in

77
00:03:41.720 --> 00:03:43.280
the notebook instance to

78
00:03:43.280 --> 00:03:45.790
train a simple
classification model,

79
00:03:45.790 --> 00:03:48.410
and then deploy that model so

80
00:03:48.410 --> 00:03:50.900
that I can make
inferences against it.

81
00:03:50.900 --> 00:03:52.910
Please remember,
you can always use

82
00:03:52.910 --> 00:03:54.635
Amazon's SageMaker console or

83
00:03:54.635 --> 00:03:57.155
APIs to build your workflows,

84
00:03:57.155 --> 00:03:59.605
if you prefer to do that.

85
00:03:59.605 --> 00:04:02.905
Now, I'm on Amazon
SageMaker console

86
00:04:02.905 --> 00:04:06.170
and I am creating a
notebook instance.

87
00:04:06.170 --> 00:04:08.210
I will give it a name,

88
00:04:08.210 --> 00:04:11.635
for example, my
notebook instance-1.

89
00:04:11.635 --> 00:04:14.000
As you can see, I can pick

90
00:04:14.000 --> 00:04:16.505
the type of the
notebook instance here.

91
00:04:16.505 --> 00:04:18.590
Since I only plan to use

92
00:04:18.590 --> 00:04:21.890
a notebook instance as my
development environment and

93
00:04:21.890 --> 00:04:24.739
rely on the on-demand
training environment

94
00:04:24.739 --> 00:04:27.710
to execute heavy lifting
training jobs for me,

95
00:04:27.710 --> 00:04:32.585
I just pick the smallest
instance, which is Ml.t2.medium.

96
00:04:32.585 --> 00:04:34.910
I'm also granting permissions to

97
00:04:34.910 --> 00:04:38.180
the notebook instance through
IAM role so that I can

98
00:04:38.180 --> 00:04:41.060
access necessary
AWS resources from

99
00:04:41.060 --> 00:04:42.710
my notebook instance without

100
00:04:42.710 --> 00:04:45.710
the need to provide
my AWS credential.

101
00:04:45.710 --> 00:04:48.575
If you don't have
IAM role in place,

102
00:04:48.575 --> 00:04:50.480
Amazon SageMaker
will automatically

103
00:04:50.480 --> 00:04:53.480
create a role for you
with your permission.

104
00:04:53.480 --> 00:04:57.325
For those who want to access
resources in your VPCs,

105
00:04:57.325 --> 00:04:59.540
you can specify which VPC you

106
00:04:59.540 --> 00:05:02.895
want to be able to connect
from the notebook instance.

107
00:05:02.895 --> 00:05:05.720
You can also secure your data in

108
00:05:05.720 --> 00:05:09.050
the notebook instance
leveraging KMS encryption.

109
00:05:09.050 --> 00:05:10.730
I will go ahead,

110
00:05:10.730 --> 00:05:13.520
hit the Create Notebook
instance button and

111
00:05:13.520 --> 00:05:15.020
this notebook instance will be

112
00:05:15.020 --> 00:05:18.300
created and
automatically started.

113
00:05:18.370 --> 00:05:22.130
Now that my notebook
instance is up and running,

114
00:05:22.130 --> 00:05:25.445
I will just click on the
"Open button" on the right,

115
00:05:25.445 --> 00:05:28.925
which brings me to the
Jupiter notebook dashboard.

116
00:05:28.925 --> 00:05:32.059
For those who are not familiar
with Jupiter notebooks,

117
00:05:32.059 --> 00:05:34.940
it is an open source web
application that allows

118
00:05:34.940 --> 00:05:39.100
users to author and execute
code interactively.

119
00:05:39.100 --> 00:05:43.190
It is very widely used by the
Data Scientist community.

120
00:05:43.190 --> 00:05:45.715
From the Jupiter
notebook dashboard,

121
00:05:45.715 --> 00:05:47.030
I can see a list of

122
00:05:47.030 --> 00:05:50.150
pre-populated example
notebooks showing me how to

123
00:05:50.150 --> 00:05:52.220
use Amazon sage maker to

124
00:05:52.220 --> 00:05:54.905
build all kinds of machine
learning solutions.

125
00:05:54.905 --> 00:05:58.420
I can easily make my own
version based on one of them.

126
00:05:58.420 --> 00:06:01.640
This example notebooks
are developed by subject

127
00:06:01.640 --> 00:06:04.280
matter experts across Amazon and

128
00:06:04.280 --> 00:06:07.565
we will continue adding
more examples over time.

129
00:06:07.565 --> 00:06:09.830
Let's go through one of
the example notebooks

130
00:06:09.830 --> 00:06:11.120
here which uses

131
00:06:11.120 --> 00:06:14.210
XG boost implementation
of boosted trees

132
00:06:14.210 --> 00:06:18.115
algorithm to build a
direct marketing model.

133
00:06:18.115 --> 00:06:19.950
XG boost is

134
00:06:19.950 --> 00:06:23.285
an extremely popular open-source
package for gradient

135
00:06:23.285 --> 00:06:25.280
boosted Trees which is widely

136
00:06:25.280 --> 00:06:28.765
used in building
classification models.

137
00:06:28.765 --> 00:06:33.040
Amazon SageMaker offers
XG boost as a built in

138
00:06:33.040 --> 00:06:37.115
algorithm so that customer
can access it more easily.

139
00:06:37.115 --> 00:06:39.335
In this notebook, we will build

140
00:06:39.335 --> 00:06:41.780
a model to predict
if our customer will

141
00:06:41.780 --> 00:06:44.180
enroll for a term deposit at

142
00:06:44.180 --> 00:06:47.555
a bank after one or more
outreach phone costs.

143
00:06:47.555 --> 00:06:50.690
We will use a public
data set published by

144
00:06:50.690 --> 00:06:53.675
UC Irvine which
contains information

145
00:06:53.675 --> 00:06:55.880
about historical
customer outreach

146
00:06:55.880 --> 00:06:57.650
and whether customers have

147
00:06:57.650 --> 00:06:59.915
subscribed to the term deposit

148
00:06:59.915 --> 00:07:02.450
offered by a bank in Europe.

149
00:07:02.450 --> 00:07:04.100
We will first set up

150
00:07:04.100 --> 00:07:06.290
some variables that
are being used in

151
00:07:06.290 --> 00:07:10.430
this notebook and download the
dataset from the internet.

152
00:07:10.430 --> 00:07:13.955
After that, we will perform
some data exploration

153
00:07:13.955 --> 00:07:17.390
and transformation to prepare
the data for training.

154
00:07:17.390 --> 00:07:19.700
We will then kick
off a training job

155
00:07:19.700 --> 00:07:22.415
into Amazon SageMaker
training environment.

156
00:07:22.415 --> 00:07:24.320
Last but not the least,

157
00:07:24.320 --> 00:07:25.930
after the training is done,

158
00:07:25.930 --> 00:07:27.410
we will deploy the model to

159
00:07:27.410 --> 00:07:29.585
Amazon SageMaker hosting and make

160
00:07:29.585 --> 00:07:34.030
inferences against the
degenerated HTTPS endpoint.

161
00:07:34.030 --> 00:07:37.865
To start, let's set up
some variables such as

162
00:07:37.865 --> 00:07:39.320
the IAM role that

163
00:07:39.320 --> 00:07:42.275
Amazon SageMaker can use
during training and hosting.

164
00:07:42.275 --> 00:07:43.775
Depending on your preference,

165
00:07:43.775 --> 00:07:46.100
it can be the same
or different from

166
00:07:46.100 --> 00:07:49.400
the role that you have passed
to the notebook instance.

167
00:07:49.400 --> 00:07:51.950
I'm also setting up
the S3 bucket in

168
00:07:51.950 --> 00:07:54.905
my account which I am
using to store the dataset

169
00:07:54.905 --> 00:07:57.935
as well as the model artifacts
that will be written

170
00:07:57.935 --> 00:08:01.385
by Amazon SageMaker once the
training job is finished.

171
00:08:01.385 --> 00:08:04.055
I'm also importing
some Python libraries

172
00:08:04.055 --> 00:08:06.335
to be used in this notebook.

173
00:08:06.335 --> 00:08:09.880
Next, I'm going to
download the data set from

174
00:08:09.880 --> 00:08:12.445
UC Irvine's Machine
Learning Data Repository

175
00:08:12.445 --> 00:08:15.455
and take a look at
a sample set of it.

176
00:08:15.455 --> 00:08:17.505
As you can see here,

177
00:08:17.505 --> 00:08:20.105
there are over 40,000 customer

178
00:08:20.105 --> 00:08:24.205
records and 20 features
associated with each customer,

179
00:08:24.205 --> 00:08:28.225
including age, marital
status, education level,

180
00:08:28.225 --> 00:08:30.085
number of contacts performed,

181
00:08:30.085 --> 00:08:32.560
external environments
factors such

182
00:08:32.560 --> 00:08:34.885
as consumer confidence index.

183
00:08:34.885 --> 00:08:38.950
Last column of this dataset
is what we call [inaudible].

184
00:08:38.950 --> 00:08:42.250
It tells us whether or not a
customer has subscribed to

185
00:08:42.250 --> 00:08:45.220
the term deposit which
will be our inference or

186
00:08:45.220 --> 00:08:48.955
prediction targets for
new data points as well.

187
00:08:48.955 --> 00:08:51.250
I then go ahead to

188
00:08:51.250 --> 00:08:54.085
some data cleansing
and transformations.

189
00:08:54.085 --> 00:08:56.750
Not going into the detail here,

190
00:08:56.750 --> 00:08:59.080
but the goal is to
better prepare the data

191
00:08:59.080 --> 00:09:01.975
set so as to generate
a better model.

192
00:09:01.975 --> 00:09:03.820
As a common practice,

193
00:09:03.820 --> 00:09:05.455
after I have completed

194
00:09:05.455 --> 00:09:08.140
the data exploration
and the transformation,

195
00:09:08.140 --> 00:09:11.120
I split the data set
into training data,

196
00:09:11.120 --> 00:09:13.885
validation data, and test data.

197
00:09:13.885 --> 00:09:16.255
Training data and validation data

198
00:09:16.255 --> 00:09:18.550
will be used in the
training process.

199
00:09:18.550 --> 00:09:21.580
I am going to use
the test data to

200
00:09:21.580 --> 00:09:25.630
evaluate model performance
after it is deployed.

201
00:09:25.630 --> 00:09:28.420
I then upload the datasets to

202
00:09:28.420 --> 00:09:32.035
my S3 bucket and move on
to the training step.

203
00:09:32.035 --> 00:09:34.060
Creating your training job in

204
00:09:34.060 --> 00:09:36.985
Amazon SageMaker is
pretty straightforward.

205
00:09:36.985 --> 00:09:41.440
This cell contains the
parameters I need to set up,

206
00:09:41.440 --> 00:09:43.660
such as the IAM role,

207
00:09:43.660 --> 00:09:47.650
the training AWS image which
is provided and managed by

208
00:09:47.650 --> 00:09:49.540
Amazon SageMaker since I am

209
00:09:49.540 --> 00:09:52.675
using the built in
XG boost algorithm.

210
00:09:52.675 --> 00:09:56.380
The compute resource needed
to run the training jar.

211
00:09:56.380 --> 00:09:58.570
In this case, I'm running the job

212
00:09:58.570 --> 00:10:02.210
on two MLC42XLR instances.

213
00:10:02.210 --> 00:10:06.775
Input data config specify where
the training data set is.

214
00:10:06.775 --> 00:10:10.930
While output config specified
where the training result,

215
00:10:10.930 --> 00:10:14.500
which we call model artifacts
should be written to.

216
00:10:14.500 --> 00:10:18.725
I'm also setting up a bunch
of hyperparameters which are

217
00:10:18.725 --> 00:10:21.050
algorithms specific
and a stopping

218
00:10:21.050 --> 00:10:25.325
condition to avoid endless
training job execution.

219
00:10:25.325 --> 00:10:28.655
After our parameters are set,

220
00:10:28.655 --> 00:10:32.630
I initiate an Amazon
SageMaker Model Client and

221
00:10:32.630 --> 00:10:33.905
call the create training job

222
00:10:33.905 --> 00:10:37.210
API to kickoff the training job.

223
00:10:37.210 --> 00:10:39.745
For this particular training job,

224
00:10:39.745 --> 00:10:42.455
it will just take a few minutes.

225
00:10:42.455 --> 00:10:44.495
As you can see here,

226
00:10:44.495 --> 00:10:47.040
the training job has
been completed and

227
00:10:47.040 --> 00:10:49.130
model artifacts have been written

228
00:10:49.130 --> 00:10:51.515
to my S3 bucket in the back end.

229
00:10:51.515 --> 00:10:54.920
I'm now ready to deploy
it into production.

230
00:10:54.920 --> 00:10:57.275
I will first create a model in

231
00:10:57.275 --> 00:11:00.230
Amazon SageMaker
hosting by specifying

232
00:11:00.230 --> 00:11:03.320
the model artifacts
location as well as

233
00:11:03.320 --> 00:11:06.995
the inference image which
contains the inference code.

234
00:11:06.995 --> 00:11:11.330
Again, since I use the
built-in XG boost algorithm,

235
00:11:11.330 --> 00:11:14.195
the inference image
here is provided

236
00:11:14.195 --> 00:11:17.330
and managed by Amazon
SageMaker as well.

237
00:11:17.330 --> 00:11:20.390
If you were to bring your
own model to hosting,

238
00:11:20.390 --> 00:11:23.180
you need to provide your
own inference image here.

239
00:11:23.180 --> 00:11:25.190
I will then create a endpoints,

240
00:11:25.190 --> 00:11:27.000
but before that, I need to set

241
00:11:27.000 --> 00:11:29.445
up a endpoint
configuration first.

242
00:11:29.445 --> 00:11:31.150
This is to specify

243
00:11:31.150 --> 00:11:33.655
how many models I'm
going to put behind

244
00:11:33.655 --> 00:11:36.745
a endpoint and the
compute resources

245
00:11:36.745 --> 00:11:39.290
I need for each of the model.

246
00:11:39.290 --> 00:11:42.655
In this demo, I'm only
putting one model

247
00:11:42.655 --> 00:11:46.045
which is the one I just
trend behind endpoint.

248
00:11:46.045 --> 00:11:51.880
I am using one MLC4X large
instance to host that model.

249
00:11:51.880 --> 00:11:54.770
After that, I just call create

250
00:11:54.770 --> 00:11:58.910
endpoint API to
create the endpoint.

251
00:11:59.580 --> 00:12:03.250
Now, the endpoint is created and

252
00:12:03.250 --> 00:12:06.055
I can make inferences
against it in real time.

253
00:12:06.055 --> 00:12:08.890
What I'm doing here is
getting the test data I

254
00:12:08.890 --> 00:12:11.890
have prepared during the
data preparation step,

255
00:12:11.890 --> 00:12:15.770
making inferences for
each single one record in

256
00:12:15.770 --> 00:12:20.305
that dataset and comparing the
results with the branches.

257
00:12:20.305 --> 00:12:22.180
As you can see here,

258
00:12:22.180 --> 00:12:25.815
I managed to get an Error
Rate which is not bad.

259
00:12:25.815 --> 00:12:29.840
Although, I may need more
iterations to improve it.

260
00:12:29.840 --> 00:12:33.710
That is all I have for
this introductory video.

261
00:12:33.710 --> 00:12:37.430
I hope you enjoyed learning
about Amazon SageMaker.

262
00:12:37.430 --> 00:12:41.100
I'm Fan Li and thank
you for watching.