WEBVTT

1
00:00:05.150 --> 00:00:07.680
Hi. I'm David Arpin from

2
00:00:07.680 --> 00:00:09.330
the Amazon SageMaker Team and

3
00:00:09.330 --> 00:00:11.525
I'm here to talk to you
about a new capability,

4
00:00:11.525 --> 00:00:14.155
SageMakers automated
model tuning.

5
00:00:14.155 --> 00:00:17.310
So Amazon SageMaker
at a high level is

6
00:00:17.310 --> 00:00:18.690
Machine Learning
platform that we've

7
00:00:18.690 --> 00:00:20.875
designed to make it very
easy to build, train,

8
00:00:20.875 --> 00:00:23.700
and deploy your Machine Learning
models to get them from

9
00:00:23.700 --> 00:00:27.390
idea into production as quickly
and easily as possible.

10
00:00:27.390 --> 00:00:29.340
These three components are

11
00:00:29.340 --> 00:00:31.470
interconnected, but independent.

12
00:00:31.470 --> 00:00:35.550
So you can use one or more
of them to suit your needs.

13
00:00:35.550 --> 00:00:38.910
For the build component, we
have the ability to very

14
00:00:38.910 --> 00:00:40.110
quickly and easily set up

15
00:00:40.110 --> 00:00:42.310
an instance that's running
a Jupyter notebook server.

16
00:00:42.310 --> 00:00:44.640
Which is an interactive
environment designed

17
00:00:44.640 --> 00:00:47.645
for data scientists
to explore data,

18
00:00:47.645 --> 00:00:50.165
create Markdown
and documentation,

19
00:00:50.165 --> 00:00:52.850
and interactive
visualizations of data.

20
00:00:52.850 --> 00:00:56.300
The next component would
be training which is

21
00:00:56.300 --> 00:00:58.055
a distributed managed environment

22
00:00:58.055 --> 00:00:59.720
that when you create
a training job,

23
00:00:59.720 --> 00:01:03.125
we spin up a cluster of
training instances for you.

24
00:01:03.125 --> 00:01:05.270
We load a Docker container

25
00:01:05.270 --> 00:01:07.505
that has an algorithm within it.

26
00:01:07.505 --> 00:01:09.425
We bring in data from S3,

27
00:01:09.425 --> 00:01:10.760
we train that algorithm,

28
00:01:10.760 --> 00:01:13.100
we output the artifacts
back to S3 and then

29
00:01:13.100 --> 00:01:14.210
tear-down the cluster without you

30
00:01:14.210 --> 00:01:15.725
having to think
about any of that.

31
00:01:15.725 --> 00:01:18.370
We manage that process
on your behalf.

32
00:01:18.370 --> 00:01:21.450
The deploy component is

33
00:01:21.450 --> 00:01:22.750
that once you've
trained your model,

34
00:01:22.750 --> 00:01:24.380
you can easily deploy it to

35
00:01:24.380 --> 00:01:26.510
a real-time production endpoint.

36
00:01:26.510 --> 00:01:28.490
Then invoke that endpoint to get

37
00:01:28.490 --> 00:01:32.095
real-time predictions from
that machine learning model.

38
00:01:32.095 --> 00:01:34.210
On top of these core components,

39
00:01:34.210 --> 00:01:35.975
we have additional layers.

40
00:01:35.975 --> 00:01:37.850
So we have Custom Provided

41
00:01:37.850 --> 00:01:39.980
SageMaker Algorithms
and these have been

42
00:01:39.980 --> 00:01:41.960
designed from the ground up with

43
00:01:41.960 --> 00:01:44.960
advancements in science
and engineering.

44
00:01:44.960 --> 00:01:47.120
The methodology is
slightly different.

45
00:01:47.120 --> 00:01:49.430
It's designed to be more
scalable, more efficient,

46
00:01:49.430 --> 00:01:52.850
as well as engineering
advancements to use things like

47
00:01:52.850 --> 00:01:56.885
GPU acceleration and train
in a distributed setting.

48
00:01:56.885 --> 00:01:58.670
We also have pre-built

49
00:01:58.670 --> 00:02:00.875
deep learning frameworks
for TensorFlow,

50
00:02:00.875 --> 00:02:03.680
MXNet, Pytorch, and Chainer.

51
00:02:03.680 --> 00:02:07.520
These frameworks allow you
to very quickly and easily

52
00:02:07.520 --> 00:02:08.930
write the code that you would

53
00:02:08.930 --> 00:02:11.565
naturally for those
deep-learning frameworks.

54
00:02:11.565 --> 00:02:13.010
Then deploy them to SageMaker

55
00:02:13.010 --> 00:02:14.360
without having to
think about Managing

56
00:02:14.360 --> 00:02:16.130
the Container and knowing

57
00:02:16.130 --> 00:02:17.900
that the container that
we've set up for you and

58
00:02:17.900 --> 00:02:20.210
pre-built will allow
you to train in

59
00:02:20.210 --> 00:02:22.370
a distributed setting and take

60
00:02:22.370 --> 00:02:23.585
advantage of other nice

61
00:02:23.585 --> 00:02:25.925
functionalities within
each framework.

62
00:02:25.925 --> 00:02:28.100
Then finally, you have

63
00:02:28.100 --> 00:02:30.325
the ability to bring your
own Docker container.

64
00:02:30.325 --> 00:02:32.800
So you can code up
your own algorithm,

65
00:02:32.800 --> 00:02:34.280
package it up in a
Docker container,

66
00:02:34.280 --> 00:02:36.490
and still take
advantage of SageMakers

67
00:02:36.490 --> 00:02:39.170
managed training and
hosting environments.

68
00:02:39.170 --> 00:02:41.880
Then on top of all
of those is tuning.

69
00:02:41.880 --> 00:02:45.450
So SageMakers automated
model tuning is

70
00:02:45.450 --> 00:02:47.360
a service that wraps up

71
00:02:47.360 --> 00:02:50.419
training jobs and works
with the custom algorithms,

72
00:02:50.419 --> 00:02:52.055
the pre-built deep
learning frameworks,

73
00:02:52.055 --> 00:02:53.270
or bring your own,

74
00:02:53.270 --> 00:02:56.240
in order to help you find
the best hyperparameters

75
00:02:56.240 --> 00:02:57.845
and improve the performance

76
00:02:57.845 --> 00:02:59.645
of your machine learning model.

77
00:02:59.645 --> 00:03:01.370
So let's dig into

78
00:03:01.370 --> 00:03:04.535
SageMaker's automated model
tuning in more detail.

79
00:03:04.535 --> 00:03:07.565
What are hyperparameters first?

80
00:03:07.565 --> 00:03:10.040
Hyperparameters help you tune

81
00:03:10.040 --> 00:03:11.570
your machine learning model in

82
00:03:11.570 --> 00:03:13.010
order to get the
best performance.

83
00:03:13.010 --> 00:03:14.960
So if you're building
a neural network,

84
00:03:14.960 --> 00:03:17.060
you may want to tune
your learning rate,

85
00:03:17.060 --> 00:03:21.635
which is the size of
updates you make to

86
00:03:21.635 --> 00:03:24.650
the weights each
iteration or each

87
00:03:24.650 --> 00:03:28.420
pass through your network.

88
00:03:28.420 --> 00:03:31.190
The number of layers, so how deep

89
00:03:31.190 --> 00:03:33.875
or shallow your
neural network is.

90
00:03:33.875 --> 00:03:36.560
Whether you're using
regularization and drop-out,

91
00:03:36.560 --> 00:03:39.725
which regularization
will penalize

92
00:03:39.725 --> 00:03:42.330
large weights and drop-out

93
00:03:42.330 --> 00:03:45.995
will actually drop nodes

94
00:03:45.995 --> 00:03:48.440
out of your network to
prevent over fitting.

95
00:03:48.440 --> 00:03:51.410
So these hyperparameters
are important to make

96
00:03:51.410 --> 00:03:52.640
sure that you get the best

97
00:03:52.640 --> 00:03:54.980
predictive performance out
of your neural network.

98
00:03:54.980 --> 00:03:56.960
Hyperparameters are also used in

99
00:03:56.960 --> 00:03:59.285
other machine learning
algorithms things like trees.

100
00:03:59.285 --> 00:04:01.550
So if you're fitting
a decision tree or

101
00:04:01.550 --> 00:04:05.525
a random forest or a gradient
boosted ensemble of trees.

102
00:04:05.525 --> 00:04:08.030
The number of trees is important.

103
00:04:08.030 --> 00:04:09.985
So do you want one
tree or many tree,

104
00:04:09.985 --> 00:04:11.810
how deep each tree should be.

105
00:04:11.810 --> 00:04:14.210
So do you want a smaller
number of very deep trees

106
00:04:14.210 --> 00:04:17.790
or a much larger number
of very shallow trees?

107
00:04:18.010 --> 00:04:20.540
What your boosting step size is,

108
00:04:20.540 --> 00:04:22.460
so from round to
round of boosting,

109
00:04:22.460 --> 00:04:24.245
how much you change.

110
00:04:24.245 --> 00:04:27.950
These things are all
very impactful from

111
00:04:27.950 --> 00:04:29.990
a supervised learning
perspective in

112
00:04:29.990 --> 00:04:33.205
how your model performs.

113
00:04:33.205 --> 00:04:36.830
But even with unsupervised
machine learning techniques

114
00:04:36.830 --> 00:04:39.575
like clustering there are
still hyperparameters.

115
00:04:39.575 --> 00:04:41.750
For clustering, maybe the number

116
00:04:41.750 --> 00:04:43.430
of clusters you want at the end,

117
00:04:43.430 --> 00:04:44.600
how you want to initialize

118
00:04:44.600 --> 00:04:46.790
the seeds in your
clustering process,

119
00:04:46.790 --> 00:04:49.610
and whether you want to
pre-transform or pre-process

120
00:04:49.610 --> 00:04:53.390
the data prior to using
the clustering algorithm.

121
00:04:53.390 --> 00:04:56.720
So when we think about the
hyperparameter space and

122
00:04:56.720 --> 00:04:57.980
what we need to do to change

123
00:04:57.980 --> 00:05:00.320
these hyperparameters in
order to get a better fit,

124
00:05:00.320 --> 00:05:04.250
it's important to think
and realize that there's

125
00:05:04.250 --> 00:05:06.320
a very large influence of

126
00:05:06.320 --> 00:05:08.915
hyperparameters on the
overall model performance.

127
00:05:08.915 --> 00:05:10.610
At the plot on the
right, you can see that

128
00:05:10.610 --> 00:05:12.230
we're training the
neural network and we're

129
00:05:12.230 --> 00:05:15.845
varying the embedding
size and the hidden size.

130
00:05:15.845 --> 00:05:20.490
You can see the
validation F1 score,

131
00:05:20.490 --> 00:05:23.000
which is a measure of our
accuracy plot along the z-axis.

132
00:05:23.000 --> 00:05:24.990
So we want the best accuracy

133
00:05:24.990 --> 00:05:27.105
we can get out of this model and

134
00:05:27.105 --> 00:05:28.400
we've tested a lot of

135
00:05:28.400 --> 00:05:30.290
different potential options for

136
00:05:30.290 --> 00:05:31.925
these two hyperparameter values.

137
00:05:31.925 --> 00:05:34.415
You can see that
the difference is

138
00:05:34.415 --> 00:05:37.835
10 points on this F1 score.

139
00:05:37.835 --> 00:05:40.489
So varying these hyperparameters

140
00:05:40.489 --> 00:05:42.710
can make a very big difference
in performance which

141
00:05:42.710 --> 00:05:44.840
can obviously make
a big difference in

142
00:05:44.840 --> 00:05:46.400
your bottom line
from the predictions

143
00:05:46.400 --> 00:05:49.080
that you're generating
from these models.

144
00:05:49.360 --> 00:05:52.470
The hyperparameter space
also grows exponentially.

145
00:05:52.470 --> 00:05:55.190
So here we've plotted
to hyperparameters.

146
00:05:55.190 --> 00:05:58.325
But if we wanted to train
more hyperparameters,

147
00:05:58.325 --> 00:06:01.690
we would have to evaluate
more and more and more points

148
00:06:01.690 --> 00:06:05.185
every time in order to
create a plot like this.

149
00:06:05.185 --> 00:06:07.975
So it gets very hard
as we want to tune

150
00:06:07.975 --> 00:06:11.840
a large number of hyperparameters
to do that efficiently.

151
00:06:12.260 --> 00:06:14.525
There's also non-linearities and

152
00:06:14.525 --> 00:06:16.420
interactions between
the hyperparameters.

153
00:06:16.420 --> 00:06:17.620
So you can see that

154
00:06:17.620 --> 00:06:19.540
the embedding size
and hidden size have

155
00:06:19.540 --> 00:06:21.250
some interaction
where when you change

156
00:06:21.250 --> 00:06:24.435
one parameter without
changing the other,

157
00:06:24.435 --> 00:06:26.010
you have one result,

158
00:06:26.010 --> 00:06:27.210
but when you change
them both together,

159
00:06:27.210 --> 00:06:28.845
you get a different result.

160
00:06:28.845 --> 00:06:30.790
In addition, there's
non-linearities,

161
00:06:30.790 --> 00:06:33.430
meaning that you can't
just continue to

162
00:06:33.430 --> 00:06:36.295
increase one hyperparameter value

163
00:06:36.295 --> 00:06:38.755
and always expect to
get better performance.

164
00:06:38.755 --> 00:06:40.675
You'll increase it up
to a certain point,

165
00:06:40.675 --> 00:06:41.865
at which point in time,

166
00:06:41.865 --> 00:06:43.310
you'll have either
diminishing returns or

167
00:06:43.310 --> 00:06:45.295
even have worse results.

168
00:06:45.295 --> 00:06:47.990
Finally, each point along

169
00:06:47.990 --> 00:06:50.070
this chart would be an
expensive evaluation.

170
00:06:50.070 --> 00:06:51.740
We will have to
retrain the model for

171
00:06:51.740 --> 00:06:53.990
those hyperparameter
value combinations.

172
00:06:53.990 --> 00:06:57.680
So depending on our model
complexity and our data size,

173
00:06:57.680 --> 00:06:59.840
that can be very expensive to

174
00:06:59.840 --> 00:07:03.215
calculate different types of
hyperparameter combinations.

175
00:07:03.215 --> 00:07:05.675
So we want to

176
00:07:05.675 --> 00:07:09.020
take a process of tuning
these hyperparameters.

177
00:07:09.020 --> 00:07:11.795
There are several common
ways of tuning them.

178
00:07:11.795 --> 00:07:13.175
The first would be manual.

179
00:07:13.175 --> 00:07:16.375
You start with the default
values of the hyperparameters,

180
00:07:16.375 --> 00:07:19.535
you make a couple of
guesses and check and

181
00:07:19.535 --> 00:07:21.920
eventually converge on a set

182
00:07:21.920 --> 00:07:24.155
of hyperparameter values that
you're comfortable with.

183
00:07:24.155 --> 00:07:26.390
The second would be to use

184
00:07:26.390 --> 00:07:28.400
a data scientist
experience or intuition.

185
00:07:28.400 --> 00:07:30.790
If they've seen a problem
like this before in the past,

186
00:07:30.790 --> 00:07:32.960
they may be able to pick
hyperparameters more

187
00:07:32.960 --> 00:07:35.705
successfully for future cases.

188
00:07:35.705 --> 00:07:39.380
There also may be heuristics
where you can tune

189
00:07:39.380 --> 00:07:41.150
one hyperparameter
value first and

190
00:07:41.150 --> 00:07:44.485
then subsequently train a
second and then a third.

191
00:07:44.485 --> 00:07:47.690
These tend to require someone
with an advanced skill

192
00:07:47.690 --> 00:07:50.900
set in machine learning to
do this process though.

193
00:07:50.900 --> 00:07:53.300
The second method would be

194
00:07:53.300 --> 00:07:56.910
brute force and there are
three common sub-classes.

195
00:07:56.910 --> 00:07:59.340
This is grid, random, and sobol.

196
00:07:59.340 --> 00:08:02.705
With grid, what we do is we try

197
00:08:02.705 --> 00:08:05.465
a specific subset of

198
00:08:05.465 --> 00:08:07.805
hyperparameter values
for each hyperparameter.

199
00:08:07.805 --> 00:08:09.980
So you can see on the
chart at the right that we

200
00:08:09.980 --> 00:08:13.700
have tried three values
of one hyperparameter and

201
00:08:13.700 --> 00:08:15.530
three values of
another and we make

202
00:08:15.530 --> 00:08:17.510
the combinatorial combination of

203
00:08:17.510 --> 00:08:20.570
nine possible
hyperparameter pairs.

204
00:08:20.570 --> 00:08:24.535
We try each of these in
a brute force sense.

205
00:08:24.535 --> 00:08:27.045
The next method would be random,

206
00:08:27.045 --> 00:08:29.030
where we just
randomly pick values

207
00:08:29.030 --> 00:08:31.520
for each of the two
hyperparameters.

208
00:08:31.520 --> 00:08:33.425
Although this sounds naive,

209
00:08:33.425 --> 00:08:35.615
it's actually quite successful.

210
00:08:35.615 --> 00:08:37.700
Then there's also
methodologies like

211
00:08:37.700 --> 00:08:39.920
sobol which try and
blend the two of these,

212
00:08:39.920 --> 00:08:43.430
where you're trying to fill
up the space like a grid,

213
00:08:43.430 --> 00:08:45.360
but you add some randomness in.

214
00:08:45.360 --> 00:08:46.430
The reason you want to add that

215
00:08:46.430 --> 00:08:47.600
randomness in and the reason

216
00:08:47.600 --> 00:08:49.490
that random hyperparameter
search can be

217
00:08:49.490 --> 00:08:51.650
so effective is that very often,

218
00:08:51.650 --> 00:08:53.540
we have one hyperparameter that

219
00:08:53.540 --> 00:08:56.000
has a much larger impact on

220
00:08:56.000 --> 00:08:58.685
the objective or
the training loss

221
00:08:58.685 --> 00:09:00.770
or accuracy of our model.

222
00:09:00.770 --> 00:09:04.960
Whatever value you are
trying to improve most.

223
00:09:04.960 --> 00:09:08.660
Because of that,
random will actually

224
00:09:08.660 --> 00:09:10.100
try a much larger number of

225
00:09:10.100 --> 00:09:11.750
distinct values in that parameter

226
00:09:11.750 --> 00:09:13.340
and will explore that
space better than

227
00:09:13.340 --> 00:09:15.110
grid where you've predefined

228
00:09:15.110 --> 00:09:17.750
a very specific subset
of values which

229
00:09:17.750 --> 00:09:21.870
may miss out on the
best performance.

230
00:09:22.170 --> 00:09:25.165
Finally, there's the
meta model approach,

231
00:09:25.165 --> 00:09:26.470
which actually tries
to build another

232
00:09:26.470 --> 00:09:27.490
machine learning model on

233
00:09:27.490 --> 00:09:30.160
top of your first machine
learning model to predict which

234
00:09:30.160 --> 00:09:31.885
hyperparameters
are going to yield

235
00:09:31.885 --> 00:09:36.895
the best potential accuracy
or objective metric results.

236
00:09:36.895 --> 00:09:39.520
That's the method
that SageMaker takes,

237
00:09:39.520 --> 00:09:43.480
and SageMaker uses Gaussian
process regression model

238
00:09:43.480 --> 00:09:45.790
to model objective metric

239
00:09:45.790 --> 00:09:47.680
as a function of your
hyperparameters.

240
00:09:47.680 --> 00:09:50.380
So it's trying to predict
what your accuracy or what

241
00:09:50.380 --> 00:09:51.970
your training loss
is going to be as

242
00:09:51.970 --> 00:09:54.520
a function of
hyperparameter values.

243
00:09:54.520 --> 00:09:58.420
This is a good method that
assumes some smoothness.

244
00:09:58.420 --> 00:10:01.210
So for a small change in
hyperparameter value,

245
00:10:01.210 --> 00:10:04.090
you won't have a
drastically wild change

246
00:10:04.090 --> 00:10:07.015
in your objective metric.

247
00:10:07.015 --> 00:10:08.635
It works with very low data,

248
00:10:08.635 --> 00:10:11.050
which is important because
as we mentioned earlier,

249
00:10:11.050 --> 00:10:15.505
it's expensive to continue
training these models.

250
00:10:15.505 --> 00:10:20.080
It provides you with some
estimate of confidence or

251
00:10:20.080 --> 00:10:22.480
uncertainty as to what the value

252
00:10:22.480 --> 00:10:25.525
of an objective metric would
be at different points,

253
00:10:25.525 --> 00:10:28.160
at different
hyperparameter values.

254
00:10:28.560 --> 00:10:31.210
Then once we build
that predictive model,

255
00:10:31.210 --> 00:10:32.755
we use Bayesian optimization

256
00:10:32.755 --> 00:10:34.795
to decide where we search next,

257
00:10:34.795 --> 00:10:38.830
which hyperparameter value
combination we test next.

258
00:10:38.830 --> 00:10:41.110
Bayesian optimization is great

259
00:10:41.110 --> 00:10:43.480
because it works
in an explore and

260
00:10:43.480 --> 00:10:47.470
exploit method where you

261
00:10:47.470 --> 00:10:48.820
are both trying out a lot

262
00:10:48.820 --> 00:10:50.365
of different
hyperparameter values,

263
00:10:50.365 --> 00:10:52.165
and then when you find good ones,

264
00:10:52.165 --> 00:10:54.115
you're testing very near,

265
00:10:54.115 --> 00:10:56.980
nearby points that maybe
just slightly better.

266
00:10:56.980 --> 00:10:59.320
So it does this very
natural nice combination

267
00:10:59.320 --> 00:11:01.480
of using both of
those techniques.

268
00:11:01.480 --> 00:11:02.860
It's also gradient free,

269
00:11:02.860 --> 00:11:05.770
so we don't have to
understand exactly

270
00:11:05.770 --> 00:11:10.150
how our objective metric
relates to our hyperparameters.

271
00:11:10.150 --> 00:11:11.905
Because usually, that's unknown.

272
00:11:11.905 --> 00:11:12.880
So it's important to have a

273
00:11:12.880 --> 00:11:16.435
gradient free method
of optimization.

274
00:11:16.435 --> 00:11:19.480
So going through a
very simple example,

275
00:11:19.480 --> 00:11:21.610
we can see the dashed green line

276
00:11:21.610 --> 00:11:23.455
here would be our objective.

277
00:11:23.455 --> 00:11:24.580
In this case maybe that's

278
00:11:24.580 --> 00:11:26.665
our Machine Learning
models training loss

279
00:11:26.665 --> 00:11:28.225
or validation loss,

280
00:11:28.225 --> 00:11:29.770
and we want to minimize that.

281
00:11:29.770 --> 00:11:31.540
We want to have the
best model that

282
00:11:31.540 --> 00:11:34.075
we can on our validation dataset,

283
00:11:34.075 --> 00:11:37.645
and we have sampled
a couple of points.

284
00:11:37.645 --> 00:11:40.750
One point at 0.5, and
one point at zero,

285
00:11:40.750 --> 00:11:42.865
and our Gaussian
process regression

286
00:11:42.865 --> 00:11:44.440
has built that black line,

287
00:11:44.440 --> 00:11:46.270
which is its prediction

288
00:11:46.270 --> 00:11:49.860
of what the objective
value might look like,

289
00:11:49.860 --> 00:11:52.290
and you can see we
have those bands that

290
00:11:52.290 --> 00:11:55.710
delineate how certain we are of

291
00:11:55.710 --> 00:11:59.250
the values in-between the
points that we've tested along

292
00:11:59.250 --> 00:12:03.570
this one hyperparameter that
varies between zero and 1.0.

293
00:12:03.570 --> 00:12:06.510
The bottom plot has our
expected improvement,

294
00:12:06.510 --> 00:12:07.800
and this is calculated by

295
00:12:07.800 --> 00:12:09.570
the Bayesian
optimization portion,

296
00:12:09.570 --> 00:12:11.205
to try and blend

297
00:12:11.205 --> 00:12:14.400
the uncertainty that we
have with our prediction of

298
00:12:14.400 --> 00:12:16.510
how good that loss might

299
00:12:16.510 --> 00:12:19.765
be or how good our
objective metric might be,

300
00:12:19.765 --> 00:12:22.930
in order to define where
the next point we test is.

301
00:12:22.930 --> 00:12:26.125
So we always test
the next point where

302
00:12:26.125 --> 00:12:31.255
we find our peak
expected improvement.

303
00:12:31.255 --> 00:12:32.575
You can see in this case,

304
00:12:32.575 --> 00:12:34.090
it's all of it at 1.0.

305
00:12:34.090 --> 00:12:36.010
So we're trying the
ends of the spectrum.

306
00:12:36.010 --> 00:12:38.215
We're still in the
exploration phase

307
00:12:38.215 --> 00:12:40.435
of hyperparameter tuning.

308
00:12:40.435 --> 00:12:43.900
We test at 1.0 and

309
00:12:43.900 --> 00:12:45.280
find that our objective metric

310
00:12:45.280 --> 00:12:46.885
is not very good in that space.

311
00:12:46.885 --> 00:12:49.195
So the next space that we test,

312
00:12:49.195 --> 00:12:54.145
is in-between our first
two points, zero and 0.5,

313
00:12:54.145 --> 00:12:58.930
and we test that out
and find that we don't

314
00:12:58.930 --> 00:13:01.090
have much improvement over our

315
00:13:01.090 --> 00:13:03.610
pre-existing best
objective value.

316
00:13:03.610 --> 00:13:07.645
So now we'll test in
between 0.5 and 1.0.

317
00:13:07.645 --> 00:13:10.120
Now, we're starting to
see a real improvement.

318
00:13:10.120 --> 00:13:12.250
So with a very small
number of iterations,

319
00:13:12.250 --> 00:13:15.385
we've landed on a
hyperparameter setting

320
00:13:15.385 --> 00:13:16.930
that would be very
good at optimizing

321
00:13:16.930 --> 00:13:19.240
this objective function.

322
00:13:19.240 --> 00:13:21.580
Now we transition into

323
00:13:21.580 --> 00:13:23.470
the exploit phase and

324
00:13:23.470 --> 00:13:25.930
it's all done naturally
by the algorithm,

325
00:13:25.930 --> 00:13:27.820
where we're testing very close to

326
00:13:27.820 --> 00:13:29.950
that new optimal
point that we found.

327
00:13:29.950 --> 00:13:32.530
We test a little bit to
the left, no improvement.

328
00:13:32.530 --> 00:13:34.015
So now, we test a
little bit to the right

329
00:13:34.015 --> 00:13:35.695
and we find an improvement.

330
00:13:35.695 --> 00:13:38.170
Within a very few
number of iterations,

331
00:13:38.170 --> 00:13:41.605
we've already found this
optimum for this function,

332
00:13:41.605 --> 00:13:44.665
despite the fact that we knew
nothing about it going in.

333
00:13:44.665 --> 00:13:46.270
We didn't know its
functional form,

334
00:13:46.270 --> 00:13:47.770
so we couldn't use a regular

335
00:13:47.770 --> 00:13:50.125
optimization technique on it.

336
00:13:50.125 --> 00:13:53.260
So how do we integrate SageMaker

337
00:13:53.260 --> 00:13:56.200
hyperparameter tuning in to

338
00:13:56.200 --> 00:13:59.680
the SageMaker automated
model tuning capability.

339
00:13:59.680 --> 00:14:01.480
We mentioned before that it works

340
00:14:01.480 --> 00:14:03.220
with SageMaker algorithms,

341
00:14:03.220 --> 00:14:06.580
frameworks and bring
your own container.

342
00:14:06.580 --> 00:14:08.200
So we think that's
very important.

343
00:14:08.200 --> 00:14:09.340
It treats your algorithm like

344
00:14:09.340 --> 00:14:12.940
a black box that it
can optimize over.

345
00:14:12.940 --> 00:14:15.480
It doesn't have to know
exactly what's going on in

346
00:14:15.480 --> 00:14:16.740
your algorithm in order to be

347
00:14:16.740 --> 00:14:19.690
effective at tuning
its hyperparameters.

348
00:14:19.940 --> 00:14:25.080
We also require flat
hyperparameters, are provided.

349
00:14:25.080 --> 00:14:28.740
So those hyperparameters
can either be continuous.

350
00:14:28.740 --> 00:14:30.795
They can take a
continuous numeric value,

351
00:14:30.795 --> 00:14:32.610
they can take an
integer value or they

352
00:14:32.610 --> 00:14:34.425
can take a categorical value,

353
00:14:34.425 --> 00:14:39.415
one of a subset of
potential distinct values.

354
00:14:39.415 --> 00:14:43.210
Then we have the fact

355
00:14:43.210 --> 00:14:46.000
that we need your objective
metrics logged to CloudWatch.

356
00:14:46.000 --> 00:14:48.760
So this happens naturally
with SageMaker training jobs.

357
00:14:48.760 --> 00:14:52.825
Anything that you output will
be reported in CloudWatch.

358
00:14:52.825 --> 00:14:54.535
So you can easily output

359
00:14:54.535 --> 00:14:57.625
the objective metrics that
you want to optimize on,

360
00:14:57.625 --> 00:15:02.965
and use a regex to scrape
that information out.

361
00:15:02.965 --> 00:15:04.630
This is what allows us to be so

362
00:15:04.630 --> 00:15:07.930
flexible and work in so
many different use cases,

363
00:15:07.930 --> 00:15:11.050
is that these are the only
requirements in order to

364
00:15:11.050 --> 00:15:15.745
train a model and use SageMaker's
automated model tuning.

365
00:15:15.745 --> 00:15:19.150
So with that, let's transition
into a demonstration

366
00:15:19.150 --> 00:15:20.890
of what SageMaker
automated model tuning

367
00:15:20.890 --> 00:15:22.690
will look like in practice.

368
00:15:22.690 --> 00:15:26.335
So we've opened up Amazon
SageMaker in the AWS console,

369
00:15:26.335 --> 00:15:29.875
and I'll open up a notebook
instance that we have,

370
00:15:29.875 --> 00:15:33.865
and go to one of the
examples that are available.

371
00:15:33.865 --> 00:15:35.410
In this case, we're going to use

372
00:15:35.410 --> 00:15:39.415
hyperparameter tuning with
mxnet gluon on cifar10.

373
00:15:39.415 --> 00:15:41.890
As I mentioned,
hyperparameter tuning

374
00:15:41.890 --> 00:15:44.080
works with the pre-built
pointing frameworks,

375
00:15:44.080 --> 00:15:47.455
the custom algorithms and
bring your own capabilities.

376
00:15:47.455 --> 00:15:48.970
But for this case, we'll use

377
00:15:48.970 --> 00:15:52.645
our pre-built mxnet
container to do the project.

378
00:15:52.645 --> 00:15:54.715
So we've opened up

379
00:15:54.715 --> 00:15:57.445
the example notebook
that we have here,

380
00:15:57.445 --> 00:16:01.330
and you can see we've pre-run
this notebook because,

381
00:16:01.330 --> 00:16:03.610
as I mentioned,
hyperparameter tuning runs

382
00:16:03.610 --> 00:16:04.870
a lot of different training jobs

383
00:16:04.870 --> 00:16:06.295
which can be quite timely.

384
00:16:06.295 --> 00:16:10.285
So we'll walk through a
pre-run version of this.

385
00:16:10.285 --> 00:16:11.860
To give you a little
bit of background

386
00:16:11.860 --> 00:16:13.390
on the tests that were
trying to complete,

387
00:16:13.390 --> 00:16:15.940
is training a convolutional
neural network,

388
00:16:15.940 --> 00:16:19.494
a ResNet 34 V2 on CIFAR10,

389
00:16:19.494 --> 00:16:20.890
which is very common
computer vision

390
00:16:20.890 --> 00:16:22.735
and classification benchmark.

391
00:16:22.735 --> 00:16:27.235
We're going to compare
default hyperparameter values

392
00:16:27.235 --> 00:16:30.595
to brute force tuning
through random search,

393
00:16:30.595 --> 00:16:32.800
and then Amazon SageMaker is

394
00:16:32.800 --> 00:16:35.785
automated model tuning as well.

395
00:16:35.785 --> 00:16:40.690
So the first step in any
SageMaker notebook is set up.

396
00:16:40.690 --> 00:16:43.240
So I'll import some of
the libraries that I

397
00:16:43.240 --> 00:16:45.910
need and set up an IAM role,

398
00:16:45.910 --> 00:16:47.290
which will grant me permissions

399
00:16:47.290 --> 00:16:49.045
and access that I
need in order to

400
00:16:49.045 --> 00:16:53.240
get my data and create
SageMaker training clusters.

401
00:16:53.730 --> 00:16:56.575
I'll import some more libraries.

402
00:16:56.575 --> 00:16:58.225
Here, I've brought in

403
00:16:58.225 --> 00:17:00.970
an MXNet estimator from

404
00:17:00.970 --> 00:17:03.880
a SageMaker Python SDK
that we've created.

405
00:17:03.880 --> 00:17:06.790
I've brought in things

406
00:17:06.790 --> 00:17:08.860
from the SageMaker
Python SDK that help us

407
00:17:08.860 --> 00:17:10.900
with hyperparameter tuning and

408
00:17:10.900 --> 00:17:14.620
running automated model
tuning jobs in SageMaker,

409
00:17:14.620 --> 00:17:16.750
and then we've also

410
00:17:16.750 --> 00:17:18.700
written our own library
called Random Tuner,

411
00:17:18.700 --> 00:17:20.350
which I'll walk
through in more detail

412
00:17:20.350 --> 00:17:21.730
and then I've brought in

413
00:17:21.730 --> 00:17:24.550
some standard Python data
science libraries like

414
00:17:24.550 --> 00:17:29.200
Pandas and Matplotlib.So digging

415
00:17:29.200 --> 00:17:31.015
into the dataset that we have,

416
00:17:31.015 --> 00:17:32.815
we're going to use CIFAR-10,

417
00:17:32.815 --> 00:17:34.300
which as I mentioned
is very common

418
00:17:34.300 --> 00:17:37.750
computer vision standard dataset,

419
00:17:37.750 --> 00:17:40.420
and it's 32 by 32 pixel images.

420
00:17:40.420 --> 00:17:43.840
They're evenly distributed
across 10 classes with

421
00:17:43.840 --> 00:17:46.090
50,000 images that we'll

422
00:17:46.090 --> 00:17:48.595
use to train our
convolutional network,

423
00:17:48.595 --> 00:17:52.000
and then 10,000 images
that we'll use to test

424
00:17:52.000 --> 00:17:53.680
its performance to see

425
00:17:53.680 --> 00:17:55.615
how it performs on
images that hasn't seen,

426
00:17:55.615 --> 00:17:57.580
and you can see a snapshot of

427
00:17:57.580 --> 00:18:00.415
a sampling of some of
the images that we have.

428
00:18:00.415 --> 00:18:02.065
Small images with

429
00:18:02.065 --> 00:18:04.510
a pretty well-defined
pictures of airplanes,

430
00:18:04.510 --> 00:18:07.150
automobile, bird, cat, deer,

431
00:18:07.150 --> 00:18:09.880
dog, frog, horse, ship and truck.

432
00:18:09.880 --> 00:18:12.520
So a broad spectrum

433
00:18:12.520 --> 00:18:15.520
of items in the real world

434
00:18:15.520 --> 00:18:19.310
that it has to classify into
one of those 10 classes.

435
00:18:20.220 --> 00:18:23.470
We use a library to download

436
00:18:23.470 --> 00:18:27.025
that data and now we
upload it back to S3

437
00:18:27.025 --> 00:18:29.680
and by uploading back
to S3 this allows

438
00:18:29.680 --> 00:18:31.390
SageMaker training to pick it

439
00:18:31.390 --> 00:18:34.070
up and use it in
training scripts.

440
00:18:34.950 --> 00:18:38.890
The next step is to
define our MXNet script,

441
00:18:38.890 --> 00:18:41.215
and with the pre-built deep
learning framework containers

442
00:18:41.215 --> 00:18:42.775
in SageMaker,

443
00:18:42.775 --> 00:18:47.395
we have this capability to
write MXNet code naturally,

444
00:18:47.395 --> 00:18:48.820
and then wrap it
up in a few small

445
00:18:48.820 --> 00:18:50.440
functions to submit it to

446
00:18:50.440 --> 00:18:52.945
our container which
allows us to train

447
00:18:52.945 --> 00:18:56.260
in SageMaker's training
managed environment.

448
00:18:56.260 --> 00:18:59.770
So the first function
we create is train,

449
00:18:59.770 --> 00:19:02.635
and this function
defines our network.

450
00:19:02.635 --> 00:19:04.195
You can see we're bringing in

451
00:19:04.195 --> 00:19:09.040
ResNet34 from the
glue on Models Zoo,

452
00:19:09.040 --> 00:19:10.600
it brings in our data,

453
00:19:10.600 --> 00:19:13.930
and it actually starts
the training process.

454
00:19:13.930 --> 00:19:18.535
The next function
that we have is Save.

455
00:19:18.535 --> 00:19:20.710
So once we've returned

456
00:19:20.710 --> 00:19:23.500
our trained neural network
from that training function,

457
00:19:23.500 --> 00:19:25.945
we're going to save it in

458
00:19:25.945 --> 00:19:28.570
a manner that we can then

459
00:19:28.570 --> 00:19:32.020
access it later as a
model artifact in S3.

460
00:19:32.020 --> 00:19:33.730
We also have a couple of other

461
00:19:33.730 --> 00:19:35.200
helper functions called get data,

462
00:19:35.200 --> 00:19:37.150
get test data, and
get train data,

463
00:19:37.150 --> 00:19:40.030
and these are just ways of
getting those images that

464
00:19:40.030 --> 00:19:44.275
are in S3 that SageMaker loads
to the training cluster,

465
00:19:44.275 --> 00:19:46.060
and bringing them into MXNet in

466
00:19:46.060 --> 00:19:49.460
an efficient iterative fashion,

467
00:19:49.860 --> 00:19:54.310
and then we have our test
function which allows us to

468
00:19:54.310 --> 00:19:59.110
monitor our accuracy on our
holdout sample of images,

469
00:19:59.110 --> 00:20:00.984
and monitor our performance

470
00:20:00.984 --> 00:20:03.265
of our network as we train it.

471
00:20:03.265 --> 00:20:08.110
Then we also have hosting
functions here that

472
00:20:08.110 --> 00:20:10.090
you don't need to use
for this example because

473
00:20:10.090 --> 00:20:12.565
we're just doing
hyperparameter tuning,

474
00:20:12.565 --> 00:20:15.520
but if you wanted to
host the output of

475
00:20:15.520 --> 00:20:19.435
that model in a SageMaker
real-time endpoint,

476
00:20:19.435 --> 00:20:23.270
you can do so by using
these hosting functions.

477
00:20:24.420 --> 00:20:27.790
The next step is an
initial training job.

478
00:20:27.790 --> 00:20:30.040
So what we'll do

479
00:20:30.040 --> 00:20:32.530
here is we'll create
this MXNet estimator,

480
00:20:32.530 --> 00:20:34.420
we'll pass in that

481
00:20:34.420 --> 00:20:37.120
cifar10.py script that we

482
00:20:37.120 --> 00:20:39.715
just talked through which
has our MXNet code,

483
00:20:39.715 --> 00:20:41.440
we'll provide an [inaudible] role

484
00:20:41.440 --> 00:20:44.305
for our access management,

485
00:20:44.305 --> 00:20:46.855
we'll provide

486
00:20:46.855 --> 00:20:49.750
the specifications for
our training cluster.

487
00:20:49.750 --> 00:20:53.755
We're going to train on
one p3.8x large node,

488
00:20:53.755 --> 00:20:57.100
and we provide a subset
of hyperparameters,

489
00:20:57.100 --> 00:20:59.260
and here we're going
to specify batch size.

490
00:20:59.260 --> 00:21:02.425
So it'll do 1,024
images per mini-batch.

491
00:21:02.425 --> 00:21:05.110
We'll specify number of epochs,

492
00:21:05.110 --> 00:21:06.940
so we're going to
train for 50 epochs.

493
00:21:06.940 --> 00:21:10.090
So that's 50 full passes
through the dataset,

494
00:21:10.090 --> 00:21:12.070
and then we'll specify

495
00:21:12.070 --> 00:21:14.770
three key hyperparameters for

496
00:21:14.770 --> 00:21:16.555
our convolutional neural network.

497
00:21:16.555 --> 00:21:19.524
So the first is learning
rate which controls

498
00:21:19.524 --> 00:21:24.145
our update size that we make
to the weights, each pass.

499
00:21:24.145 --> 00:21:26.620
The second is momentum which uses

500
00:21:26.620 --> 00:21:28.630
information from
our previous step

501
00:21:28.630 --> 00:21:31.190
to inform our current step.

502
00:21:31.470 --> 00:21:35.260
The final is weight decay

503
00:21:35.260 --> 00:21:38.305
which penalizes weights
when they grow too large.

504
00:21:38.305 --> 00:21:40.884
So we've created this
MXNet estimator,

505
00:21:40.884 --> 00:21:42.640
and then by using.fit,

506
00:21:42.640 --> 00:21:45.630
we pass in that S3 location,

507
00:21:45.630 --> 00:21:48.660
the location of our images in S3,

508
00:21:48.660 --> 00:21:51.030
and so we use.fit and

509
00:21:51.030 --> 00:21:54.390
that creates the
SageMaker training job,

510
00:21:54.390 --> 00:21:57.170
and it will go provision
those instances,

511
00:21:57.170 --> 00:21:59.800
load that to pre-built deep
learning framework container,

512
00:21:59.800 --> 00:22:04.060
start executing that
cifar10.py script,

513
00:22:04.060 --> 00:22:06.835
and as that's executing,

514
00:22:06.835 --> 00:22:08.830
logs from CloudWatch will be

515
00:22:08.830 --> 00:22:11.530
printed here in the
example notebook.

516
00:22:11.530 --> 00:22:15.550
So we can see our
setup and then we

517
00:22:15.550 --> 00:22:18.280
can track from epoch

518
00:22:18.280 --> 00:22:20.800
to epoch how our
performance is doing,

519
00:22:20.800 --> 00:22:22.180
and we can see we start off with

520
00:22:22.180 --> 00:22:24.040
a pretty low
validation accuracy of

521
00:22:24.040 --> 00:22:25.150
only about 10 percent which

522
00:22:25.150 --> 00:22:27.440
is about as good as
random guessing.

523
00:22:28.050 --> 00:22:30.550
It's a rather long training job.

524
00:22:30.550 --> 00:22:32.500
As I mentioned we do 50 epochs,

525
00:22:32.500 --> 00:22:35.080
and so if we scroll
all the way to

526
00:22:35.080 --> 00:22:39.500
the bottom to find out
how our training job did,

527
00:22:40.980 --> 00:22:43.600
we can see that when we use

528
00:22:43.600 --> 00:22:48.775
the default values for
MXNet's optimization routine,

529
00:22:48.775 --> 00:22:50.680
we really only get an accuracy of

530
00:22:50.680 --> 00:22:53.425
about 53 percent
which isn't great.

531
00:22:53.425 --> 00:22:55.975
So C4 has a very hard problem

532
00:22:55.975 --> 00:22:57.490
but we should still
try and do better.

533
00:22:57.490 --> 00:22:59.395
So let's see if we can change

534
00:22:59.395 --> 00:23:01.630
the values for learning
rate momentum and

535
00:23:01.630 --> 00:23:03.790
weight decay from
the default values

536
00:23:03.790 --> 00:23:06.370
to something else to
get a better result.

537
00:23:06.370 --> 00:23:08.680
So the first thing
that we'll do is do

538
00:23:08.680 --> 00:23:11.800
a random search
hyperparameter tuning.

539
00:23:11.800 --> 00:23:13.750
As we mentioned it seems

540
00:23:13.750 --> 00:23:16.180
naive but it performs
surprisingly well and it's

541
00:23:16.180 --> 00:23:18.265
a very good baseline for how good

542
00:23:18.265 --> 00:23:21.880
another hyperparameter
tuning model does.

543
00:23:21.880 --> 00:23:24.220
So we've created
this simple random

544
00:23:24.220 --> 00:23:26.500
tuner.py script that has

545
00:23:26.500 --> 00:23:28.585
a few helper functions
that we'll need.

546
00:23:28.585 --> 00:23:31.630
We start out by defining
a few classes for

547
00:23:31.630 --> 00:23:34.900
categorical integer parameters
and we can use parameters,

548
00:23:34.900 --> 00:23:36.790
where we can specify
the ranges or

549
00:23:36.790 --> 00:23:39.160
distinct values
that each parameter

550
00:23:39.160 --> 00:23:42.085
should be allowed to take
in that random search.

551
00:23:42.085 --> 00:23:45.715
Now we have two other functions,

552
00:23:45.715 --> 00:23:50.349
one that takes in the
hyperparameters that you specify

553
00:23:50.349 --> 00:23:52.150
and randomly samples

554
00:23:52.150 --> 00:23:53.710
one of the values in the range or

555
00:23:53.710 --> 00:23:55.945
distinct set that you've given,

556
00:23:55.945 --> 00:23:58.480
and the second that
actually kicks off

557
00:23:58.480 --> 00:24:00.850
the training jobs and

558
00:24:00.850 --> 00:24:03.190
then kicks off many
of them each with

559
00:24:03.190 --> 00:24:06.560
randomly sampled values
for the hyperparameters.

560
00:24:07.380 --> 00:24:09.940
Then we have a couple
more functions

561
00:24:09.940 --> 00:24:11.695
here to help us
analyze that output.

562
00:24:11.695 --> 00:24:13.900
As we mentioned, logs

563
00:24:13.900 --> 00:24:16.690
from training jobs and
SageMaker go to CloudWatch.

564
00:24:16.690 --> 00:24:19.210
So we've created a couple
of functions to go scrape

565
00:24:19.210 --> 00:24:22.735
those logs and pull
out using a regex,

566
00:24:22.735 --> 00:24:24.610
the validation accuracy that we

567
00:24:24.610 --> 00:24:26.960
saw at the bottom of
our last training job.

568
00:24:26.960 --> 00:24:28.635
So in order to start using this,

569
00:24:28.635 --> 00:24:31.500
we'll define one
more simple function

570
00:24:31.500 --> 00:24:32.805
which is fit random.

571
00:24:32.805 --> 00:24:34.680
This will take in a job name,

572
00:24:34.680 --> 00:24:37.455
and hyperparameter dictionary,

573
00:24:37.455 --> 00:24:40.310
and startup model training.

574
00:24:40.310 --> 00:24:41.860
We set weight equals false here,

575
00:24:41.860 --> 00:24:43.150
so that we can train as

576
00:24:43.150 --> 00:24:45.920
many models in the
background is we'd like.

577
00:24:45.990 --> 00:24:49.315
Now, we specify a dictionary
of hyperparameters.

578
00:24:49.315 --> 00:24:53.500
Similar to above, we'll use
the same batch size of 1,024,

579
00:24:53.500 --> 00:24:55.150
same number of epochs.

580
00:24:55.150 --> 00:24:56.860
But this time, we're
going to define

581
00:24:56.860 --> 00:24:58.990
our learning rate as taking
a value somewhere between

582
00:24:58.990 --> 00:25:01.870
0.001 to an order of
magnitude smaller than

583
00:25:01.870 --> 00:25:05.695
our first example and
somewhere between 0.5.

584
00:25:05.695 --> 00:25:10.615
Then we'll define
momentum between 0.99,

585
00:25:10.615 --> 00:25:12.100
which is a pretty wide spectrum

586
00:25:12.100 --> 00:25:13.945
of possible values for that,

587
00:25:13.945 --> 00:25:18.295
then wait to k
between 0 and 0.0.1.

588
00:25:18.295 --> 00:25:21.700
So now we'll start
our random search,

589
00:25:21.700 --> 00:25:23.905
and we're going to pass in

590
00:25:23.905 --> 00:25:26.620
that function that
creates a training job,

591
00:25:26.620 --> 00:25:27.970
our hyperparameters,

592
00:25:27.970 --> 00:25:29.770
which have the defined
ranges in them,

593
00:25:29.770 --> 00:25:31.690
how many jobs we
want to run at most,

594
00:25:31.690 --> 00:25:33.580
and how many jobs we
want to run at once.

595
00:25:33.580 --> 00:25:35.110
We could set this
max parallel jobs

596
00:25:35.110 --> 00:25:36.670
to a 120 if we're
comfortable with

597
00:25:36.670 --> 00:25:40.630
spinning up a 120 P3 ADX
large as all at one time.

598
00:25:40.630 --> 00:25:42.685
But for the purposes
of this notebook,

599
00:25:42.685 --> 00:25:44.710
will just set it to eight.

600
00:25:44.710 --> 00:25:47.725
Once we submit that training job,

601
00:25:47.725 --> 00:25:50.725
we can see that it's
going to kick off

602
00:25:50.725 --> 00:25:54.280
a hundred and twenty
subsequent training jobs,

603
00:25:54.280 --> 00:25:56.950
and now we can go
and summarize and

604
00:25:56.950 --> 00:25:59.530
compare the validation
accuracy across all of

605
00:25:59.530 --> 00:26:01.510
those training jobs that
we've kicked off with

606
00:26:01.510 --> 00:26:04.000
random hyperparameter
values sampled

607
00:26:04.000 --> 00:26:06.205
from the range as we specified.

608
00:26:06.205 --> 00:26:09.730
So you can see, we've

609
00:26:09.730 --> 00:26:12.160
used our other functions
table metrics and get

610
00:26:12.160 --> 00:26:14.995
metrics to return a
Pandas DataFrame of

611
00:26:14.995 --> 00:26:18.625
each training job and
the objective score.

612
00:26:18.625 --> 00:26:20.965
So you can see that, at best,

613
00:26:20.965 --> 00:26:22.885
we're over 20 percentage points

614
00:26:22.885 --> 00:26:25.990
better than the default values,

615
00:26:25.990 --> 00:26:29.395
which is a huge improvement
in accuracy for this problem.

616
00:26:29.395 --> 00:26:34.150
However, at worst, we're
only 23 percent accurate.

617
00:26:34.150 --> 00:26:36.505
So randomly sampling
hyperparameters I use,

618
00:26:36.505 --> 00:26:39.640
it still gets a lot of
inaccurate results as well.

619
00:26:39.640 --> 00:26:43.060
We're just barely better
than random guessing there.

620
00:26:43.060 --> 00:26:46.040
So you can see that
there's a huge spectrum of

621
00:26:46.040 --> 00:26:48.840
possible objective values based

622
00:26:48.840 --> 00:26:50.385
on the hyperparameter values.

623
00:26:50.385 --> 00:26:52.725
Had you inaccurately started

624
00:26:52.725 --> 00:26:55.770
with the values that give you

625
00:26:55.770 --> 00:26:58.860
23 percent accuracy as
your hyperparameters,

626
00:26:58.860 --> 00:27:02.545
you would be doomed to
having poor performance.

627
00:27:02.545 --> 00:27:04.075
The next thing that we'll look at

628
00:27:04.075 --> 00:27:06.970
is hyperparameter relationships.

629
00:27:06.970 --> 00:27:10.600
So we've created a scatter plot

630
00:27:10.600 --> 00:27:12.550
of our objective function,

631
00:27:12.550 --> 00:27:16.840
as well as each of
our hyperparameters,

632
00:27:16.840 --> 00:27:18.880
learning rate momentum,
and weight decay.

633
00:27:18.880 --> 00:27:22.390
You can see that as learning
rate increases, in general,

634
00:27:22.390 --> 00:27:25.764
we're getting better
validation accuracy

635
00:27:25.764 --> 00:27:26.935
until a certain point,

636
00:27:26.935 --> 00:27:28.540
and then actually toward the end,

637
00:27:28.540 --> 00:27:30.415
as the learning rate
gets too large,

638
00:27:30.415 --> 00:27:32.500
we see a little bit
more volatility

639
00:27:32.500 --> 00:27:35.740
in our potential accuracies.

640
00:27:35.740 --> 00:27:37.540
Momentum, on the other hand,

641
00:27:37.540 --> 00:27:41.305
has a little bit of
noise at low values,

642
00:27:41.305 --> 00:27:44.530
maybe has a peak here
in terms of the best

643
00:27:44.530 --> 00:27:45.580
value being somewhere around

644
00:27:45.580 --> 00:27:48.100
0.8 and then quickly diminishes,

645
00:27:48.100 --> 00:27:52.555
so this nonlinear effect
that we see of momentum.

646
00:27:52.555 --> 00:27:55.690
In addition, weight decay

647
00:27:55.690 --> 00:28:00.670
has not as much of a
noticeable effect,

648
00:28:00.670 --> 00:28:02.620
and then we can see that

649
00:28:02.620 --> 00:28:06.070
this last plot is about
the number of jobs,

650
00:28:06.070 --> 00:28:09.355
and the job number,
and our objective.

651
00:28:09.355 --> 00:28:11.230
You can see that
we don't actually

652
00:28:11.230 --> 00:28:12.925
get any better over time.

653
00:28:12.925 --> 00:28:14.500
We're randomly sampling.

654
00:28:14.500 --> 00:28:15.850
This is exactly what
we would expect,

655
00:28:15.850 --> 00:28:18.775
as we randomly sample
hyperparameter values,

656
00:28:18.775 --> 00:28:21.520
sometimes our best job
might occur the first time

657
00:28:21.520 --> 00:28:24.175
and sometimes it might
occur on a 120th job,

658
00:28:24.175 --> 00:28:27.775
and we don't have much
control over that.

659
00:28:27.775 --> 00:28:30.250
We can also see that
we're just randomly

660
00:28:30.250 --> 00:28:32.890
sampling the
hyperparameter values

661
00:28:32.890 --> 00:28:34.045
relative to themselves.

662
00:28:34.045 --> 00:28:35.950
We're not taking into account

663
00:28:35.950 --> 00:28:37.720
that learning rate and momentum

664
00:28:37.720 --> 00:28:39.205
are actually related
to one another,

665
00:28:39.205 --> 00:28:43.070
so we're just randomly
trying values.

666
00:28:44.400 --> 00:28:46.840
Now, let's see what
happens when we tune

667
00:28:46.840 --> 00:28:49.135
with SageMaker
Automated Model Tuning.

668
00:28:49.135 --> 00:28:51.055
As we mentioned, we use

669
00:28:51.055 --> 00:28:52.990
a Gaussian process
regression model to predict

670
00:28:52.990 --> 00:28:54.550
what good hyperparameters
values might

671
00:28:54.550 --> 00:28:56.739
be and Bayesian Optimization,

672
00:28:56.739 --> 00:28:59.200
which does a good
job of exploring

673
00:28:59.200 --> 00:29:02.230
and exploiting
potential optimists.

674
00:29:02.230 --> 00:29:04.180
The tuner that we have built is

675
00:29:04.180 --> 00:29:06.010
built into the
SageMaker Python SDK,

676
00:29:06.010 --> 00:29:07.735
so we imported that at the top.

677
00:29:07.735 --> 00:29:11.120
We can start by just defining
this new MXNet estimator,

678
00:29:11.120 --> 00:29:13.355
again we pass the
same training script,

679
00:29:13.355 --> 00:29:18.580
the same role, instance
count and instance type.

680
00:29:18.580 --> 00:29:21.070
We defined our two static
hyperparameters here.

681
00:29:21.070 --> 00:29:24.850
So we defined 1,024

682
00:29:24.850 --> 00:29:27.820
as our batch size and
50 is our epochs again,

683
00:29:27.820 --> 00:29:29.650
and now we'll move on to

684
00:29:29.650 --> 00:29:32.125
actually defining our
hyperparameter ranges.

685
00:29:32.125 --> 00:29:33.610
So you can see that the classes

686
00:29:33.610 --> 00:29:34.690
here function very similarly.

687
00:29:34.690 --> 00:29:37.540
Random tuner was written
with this in mind to

688
00:29:37.540 --> 00:29:39.340
take a similar format

689
00:29:39.340 --> 00:29:41.425
to how the SageMaker
Python SDK works.

690
00:29:41.425 --> 00:29:44.410
We define the minimum value
of our learning rate and

691
00:29:44.410 --> 00:29:47.605
the maximum value in this
continuous parameter class.

692
00:29:47.605 --> 00:29:51.610
So we next create an
objective metrics.

693
00:29:51.610 --> 00:29:54.220
So we define what we want
our objective metric to be.

694
00:29:54.220 --> 00:29:55.675
So that's validation accuracy,

695
00:29:55.675 --> 00:29:57.670
and we write the Regex in order

696
00:29:57.670 --> 00:30:00.220
to pass that from our
CloudWatch logs here.

697
00:30:00.220 --> 00:30:02.530
Now, we create a tuner object

698
00:30:02.530 --> 00:30:04.540
from the SageMaker Python SDK,

699
00:30:04.540 --> 00:30:07.870
we pass it our estimator,
our information,

700
00:30:07.870 --> 00:30:09.400
our objective metrics, and

701
00:30:09.400 --> 00:30:12.650
our hyperparameter ranges
that we want to test.

702
00:30:13.050 --> 00:30:17.650
We also pass in the max jobs
and the max parallel jobs.

703
00:30:17.650 --> 00:30:20.725
As you can see here, we've
defined a much smaller number

704
00:30:20.725 --> 00:30:22.390
of jobs because

705
00:30:22.390 --> 00:30:24.010
SageMaker Automated
Model Tuning is going to

706
00:30:24.010 --> 00:30:26.500
be much more effective than
random search at finding

707
00:30:26.500 --> 00:30:31.390
a potential good value
for objective metric.

708
00:30:31.390 --> 00:30:33.880
We've to find a
much smaller number

709
00:30:33.880 --> 00:30:34.900
of parallel jobs as well.

710
00:30:34.900 --> 00:30:36.745
So instead of eight,
we're not training two.

711
00:30:36.745 --> 00:30:39.205
That's important for SageMaker
Automated Model Tuning

712
00:30:39.205 --> 00:30:41.635
because we learn
from our past runs,

713
00:30:41.635 --> 00:30:43.510
so it's important that

714
00:30:43.510 --> 00:30:44.980
we not try to do too
many of those in

715
00:30:44.980 --> 00:30:47.920
parallel because we may

716
00:30:47.920 --> 00:30:50.455
increase how fast we do
the overall tuning job,

717
00:30:50.455 --> 00:30:52.450
but we may lose out on accuracy,

718
00:30:52.450 --> 00:30:55.280
as we can't predict
far enough ahead.

719
00:30:55.800 --> 00:30:59.110
The next step is to
summarize accuracy

720
00:30:59.110 --> 00:31:01.945
from our SageMaker Model
Tuning jobs after that's run.

721
00:31:01.945 --> 00:31:04.180
You can see again, we have

722
00:31:04.180 --> 00:31:09.070
a final objective value
of actually 74 percent,

723
00:31:09.070 --> 00:31:12.490
so better within 30
jobs than we've found

724
00:31:12.490 --> 00:31:16.600
within a hundred and twenty
jobs of random search.

725
00:31:16.600 --> 00:31:21.625
We have a worst
performance of 0.41,

726
00:31:21.625 --> 00:31:23.230
so still better than

727
00:31:23.230 --> 00:31:26.515
our worst training
job in random search.

728
00:31:26.515 --> 00:31:29.980
So it's important to
realize that SageMaker is

729
00:31:29.980 --> 00:31:33.730
actively trying to test good
hyperparameter combinations,

730
00:31:33.730 --> 00:31:35.140
and so in the worst case,

731
00:31:35.140 --> 00:31:37.120
it's probably going
to do better, and in

732
00:31:37.120 --> 00:31:38.920
far fewer training jobs

733
00:31:38.920 --> 00:31:40.915
with one-fourth of
the training jobs,

734
00:31:40.915 --> 00:31:42.730
we actually found
a model that has

735
00:31:42.730 --> 00:31:46.040
better accuracy
than random search.

736
00:31:46.050 --> 00:31:48.340
Now if we compare

737
00:31:48.340 --> 00:31:50.410
the hyperparameter values
across training jobs,

738
00:31:50.410 --> 00:31:52.780
we see the same
relationship between

739
00:31:52.780 --> 00:31:55.345
our objective metric and
hyperparameter values,

740
00:31:55.345 --> 00:31:57.730
except there seems
to be less noise,

741
00:31:57.730 --> 00:32:01.180
and the hyperparameters
themselves don't have as random

742
00:32:01.180 --> 00:32:04.645
of a scattering of values
relative to one another.

743
00:32:04.645 --> 00:32:10.855
That's because, when we are
trying to explore that space,

744
00:32:10.855 --> 00:32:13.015
we're looking to adjust

745
00:32:13.015 --> 00:32:15.670
multiple hyperparameter
values at one time,

746
00:32:15.670 --> 00:32:19.285
and understand the relationships
between those things.

747
00:32:19.285 --> 00:32:23.770
So we can also see up here that,

748
00:32:23.770 --> 00:32:25.600
in general, our accuracy
is getting more

749
00:32:25.600 --> 00:32:27.970
consistent and
improving over time.

750
00:32:27.970 --> 00:32:30.115
So that's another feature of

751
00:32:30.115 --> 00:32:33.640
SageMaker Automated Model
Tuning is that as it learns,

752
00:32:33.640 --> 00:32:35.470
which hyperparameter
values are successful,

753
00:32:35.470 --> 00:32:38.020
it will start exploiting that

754
00:32:38.020 --> 00:32:41.330
and fitting ever better models.

755
00:32:41.330 --> 00:32:44.700
So in conclusion, we've seen

756
00:32:44.700 --> 00:32:45.930
that hyperparameter tuning is

757
00:32:45.930 --> 00:32:48.990
a very important aspect
of machine learning.

758
00:32:48.990 --> 00:32:50.550
If we do it naively,

759
00:32:50.550 --> 00:32:51.600
it can be quite costly.

760
00:32:51.600 --> 00:32:54.360
It can be four times
more jobs to do

761
00:32:54.360 --> 00:32:55.665
random search than it was to

762
00:32:55.665 --> 00:32:58.300
use SageMaker Automated
Model Tuning.

763
00:32:58.710 --> 00:33:02.605
If you wanted to extend on
this, what we've done today,

764
00:33:02.605 --> 00:33:03.910
you could try and
other brute force

765
00:33:03.910 --> 00:33:05.515
method like grid search

766
00:33:05.515 --> 00:33:07.360
to see how that
performs relative to

767
00:33:07.360 --> 00:33:09.580
both random search and
Automating Model Tuning.

768
00:33:09.580 --> 00:33:12.250
You could try tuning a larger
number of hyperparameters,

769
00:33:12.250 --> 00:33:15.505
you could use the first
round of tuning to inform

770
00:33:15.505 --> 00:33:17.980
a second round or you could

771
00:33:17.980 --> 00:33:21.820
also apply hyperparameter
tuning to your own problem.

772
00:33:21.820 --> 00:33:23.890
If you'd like more information on

773
00:33:23.890 --> 00:33:26.020
SageMaker Automated Model Tuning,

774
00:33:26.020 --> 00:33:28.375
please see the
example notebooks and

775
00:33:28.375 --> 00:33:30.865
documentation on AWS's website.

776
00:33:30.865 --> 00:33:34.105
So thank you for
joining us today.

777
00:33:34.105 --> 00:33:36.205
I'm David [inaudible] from
the Amazon SageMaker team.

778
00:33:36.205 --> 00:33:39.010
We hope you enjoy SageMaker
Automated Model Tuning

779
00:33:39.010 --> 00:33:40.360
and use it to improve

780
00:33:40.360 --> 00:33:42.980
your machine learning
model fits today.