WEBVTT

1
00:00:07.340 --> 00:00:10.920
Welcome to this course on
Machine Learning Algorithms.

2
00:00:10.920 --> 00:00:12.570
My name is Denis Batalov,

3
00:00:12.570 --> 00:00:15.240
I've been with Amazon 13
years and currently work as

4
00:00:15.240 --> 00:00:16.800
a principle solutions architect

5
00:00:16.800 --> 00:00:18.540
specializing in Machine Learning,

6
00:00:18.540 --> 00:00:20.840
and I even have a
PhD in this field.

7
00:00:20.840 --> 00:00:23.150
You are now ready to
start diving into

8
00:00:23.150 --> 00:00:26.045
machine learning
algorithms, exciting times.

9
00:00:26.045 --> 00:00:27.950
Many customers are
struggling with

10
00:00:27.950 --> 00:00:29.570
understanding how to translate

11
00:00:29.570 --> 00:00:31.070
their business problems into

12
00:00:31.070 --> 00:00:32.180
an IT solution that

13
00:00:32.180 --> 00:00:34.175
somehow incorporates
machine learning.

14
00:00:34.175 --> 00:00:36.560
So in this course,
we're going to review

15
00:00:36.560 --> 00:00:38.090
the different types
of machine learning

16
00:00:38.090 --> 00:00:40.520
algorithms and the
problems they solve.

17
00:00:40.520 --> 00:00:43.475
Perhaps you've already heard
about supervised learning,

18
00:00:43.475 --> 00:00:44.810
unsupervised learning,

19
00:00:44.810 --> 00:00:47.195
reinforcement learning,
and deep learning.

20
00:00:47.195 --> 00:00:48.440
By the end of this course,

21
00:00:48.440 --> 00:00:49.670
you should be able to speak

22
00:00:49.670 --> 00:00:51.425
confidently about
these categories of

23
00:00:51.425 --> 00:00:53.870
ML algorithms with
your customers and

24
00:00:53.870 --> 00:00:56.630
help them determine the category
that fits their problem.

25
00:00:56.630 --> 00:00:58.585
So let's get going.

26
00:00:58.585 --> 00:01:02.050
Before we can intelligently
speak of Machine Learning,

27
00:01:02.050 --> 00:01:04.115
let's recall what
artificial intelligence

28
00:01:04.115 --> 00:01:06.215
or machine intelligence
is all about.

29
00:01:06.215 --> 00:01:09.125
A system exhibiting
intelligent behavior

30
00:01:09.125 --> 00:01:12.755
normally needs to possess
two fundamental faculties.

31
00:01:12.755 --> 00:01:14.690
First is the ability to

32
00:01:14.690 --> 00:01:16.925
acquire and
systematize knowledge.

33
00:01:16.925 --> 00:01:20.150
This is relying on the so-called
inductive reasoning or

34
00:01:20.150 --> 00:01:21.590
coming up with rules that would

35
00:01:21.590 --> 00:01:23.840
explain the individual
observations.

36
00:01:23.840 --> 00:01:25.970
Of course simple facts or

37
00:01:25.970 --> 00:01:28.250
truths need to be
acquired as well.

38
00:01:28.250 --> 00:01:30.140
But if a rule can be learned so

39
00:01:30.140 --> 00:01:32.240
that many truths can
be derived from it,

40
00:01:32.240 --> 00:01:33.500
it would be easier to remember

41
00:01:33.500 --> 00:01:34.955
a single rule, wouldn't it?

42
00:01:34.955 --> 00:01:37.470
For example, as
you hear me speak,

43
00:01:37.470 --> 00:01:38.600
you don't need to constantly

44
00:01:38.600 --> 00:01:39.830
remind yourself that there is

45
00:01:39.830 --> 00:01:41.420
no life person in front of you

46
00:01:41.420 --> 00:01:43.880
because you know how
video recordings work.

47
00:01:43.880 --> 00:01:48.380
Now, the second faculty is
inference or the ability to

48
00:01:48.380 --> 00:01:50.180
use the acquired
knowledge to derive

49
00:01:50.180 --> 00:01:53.120
the truths when needed
like making predictions,

50
00:01:53.120 --> 00:01:55.760
choose actions, or
make complex plans.

51
00:01:55.760 --> 00:01:58.760
This ability relies on
deductive reasoning,

52
00:01:58.760 --> 00:02:01.400
which was popularized
so much by Conan Doyle.

53
00:02:01.400 --> 00:02:03.950
You heard me use the
terms learning and

54
00:02:03.950 --> 00:02:06.340
predictions when describing
these faculties,

55
00:02:06.340 --> 00:02:08.495
and this is of
course no accident.

56
00:02:08.495 --> 00:02:10.310
All machine learning algorithms

57
00:02:10.310 --> 00:02:12.800
must possess them in some form.

58
00:02:12.800 --> 00:02:15.590
Early algorithms in
the AI space were

59
00:02:15.590 --> 00:02:17.945
relying primarily on
the second faculty of

60
00:02:17.945 --> 00:02:19.955
inference by having humans

61
00:02:19.955 --> 00:02:21.395
acquire and feed

62
00:02:21.395 --> 00:02:23.590
all the necessary knowledge
into the machine.

63
00:02:23.590 --> 00:02:25.895
This unfortunately proved to be

64
00:02:25.895 --> 00:02:28.295
impossible for most
practical problems,

65
00:02:28.295 --> 00:02:30.590
and that's why ML algorithms rule

66
00:02:30.590 --> 00:02:33.935
the day, machines
learn automatically.

67
00:02:33.935 --> 00:02:37.775
We're now ready to discuss the
different categories of ML

68
00:02:37.775 --> 00:02:41.660
based on how the machine
learns and what it can infer.

69
00:02:41.660 --> 00:02:45.050
Currently, when people
think of machine learning,

70
00:02:45.050 --> 00:02:48.050
they typically think of
supervised learning because of

71
00:02:48.050 --> 00:02:51.635
its wide applicability and
many successful applications.

72
00:02:51.635 --> 00:02:53.750
It's called supervised because

73
00:02:53.750 --> 00:02:55.475
there needs to be a supervisor,

74
00:02:55.475 --> 00:02:57.200
a teacher or trainer

75
00:02:57.200 --> 00:02:59.570
showing the right answers
during the learning.

76
00:02:59.570 --> 00:03:01.190
No wonder we also call it

77
00:03:01.190 --> 00:03:03.335
training a machine
learning model.

78
00:03:03.335 --> 00:03:05.600
A model, because the algorithm is

79
00:03:05.600 --> 00:03:09.380
effectively able to simulate
or model the teacher.

80
00:03:09.380 --> 00:03:12.740
Oftentimes, the teacher
is simply not there

81
00:03:12.740 --> 00:03:15.950
and all we're left with is
just observations or data.

82
00:03:15.950 --> 00:03:17.810
Can something useful be learned

83
00:03:17.810 --> 00:03:19.670
from the data in such a case?

84
00:03:19.670 --> 00:03:21.620
You guessed it. This is

85
00:03:21.620 --> 00:03:24.215
the domain of the so-called
unsupervised learning.

86
00:03:24.215 --> 00:03:25.790
One typical example from

87
00:03:25.790 --> 00:03:28.670
this category is a
clustering algorithm,

88
00:03:28.670 --> 00:03:30.440
which divides the
observations into

89
00:03:30.440 --> 00:03:32.465
what appear to be
different clusters.

90
00:03:32.465 --> 00:03:34.610
We will see others later.

91
00:03:34.610 --> 00:03:37.070
I should point out
that there exists

92
00:03:37.070 --> 00:03:39.965
so-called mixed or
semi-supervised algorithms,

93
00:03:39.965 --> 00:03:42.935
but let's not over
complicate things for now.

94
00:03:42.935 --> 00:03:45.830
Another kind of learning
that has been gaining in

95
00:03:45.830 --> 00:03:47.180
popularity recently is

96
00:03:47.180 --> 00:03:49.135
the so-called
reinforcement learning.

97
00:03:49.135 --> 00:03:50.660
In some sense, this type of

98
00:03:50.660 --> 00:03:52.190
an algorithm is
attempting to solve

99
00:03:52.190 --> 00:03:54.440
the complete AI
problem of building

100
00:03:54.440 --> 00:03:55.790
an agent capable of

101
00:03:55.790 --> 00:03:58.189
exhibiting entire
intelligent behaviors,

102
00:03:58.189 --> 00:04:00.530
not just making
isolated decisions.

103
00:04:00.530 --> 00:04:03.485
This is why it's an
exciting area of research,

104
00:04:03.485 --> 00:04:05.000
but that's what makes it also

105
00:04:05.000 --> 00:04:07.205
difficult in applied
practical settings.

106
00:04:07.205 --> 00:04:08.860
In reinforcement learning,

107
00:04:08.860 --> 00:04:11.900
the agent controlled by the
algorithm is interacting

108
00:04:11.900 --> 00:04:14.390
with the possibly completely
unknown environment

109
00:04:14.390 --> 00:04:17.705
and is learning optimal
actions via trial and error.

110
00:04:17.705 --> 00:04:20.360
Here, there's no
explicit teacher telling

111
00:04:20.360 --> 00:04:23.650
the agent what is the right
action at any given time.

112
00:04:23.650 --> 00:04:26.030
Instead, the agent is getting

113
00:04:26.030 --> 00:04:29.240
an often delayed reward or
penalty called reinforcement,

114
00:04:29.240 --> 00:04:32.345
and is designed to maximize
long-term rewards.

115
00:04:32.345 --> 00:04:33.890
Think of playing a computer game

116
00:04:33.890 --> 00:04:35.990
with possibly unknown rules,

117
00:04:35.990 --> 00:04:38.975
but your goal is to
get maximum points.

118
00:04:38.975 --> 00:04:41.120
Not surprisingly,
this approach has

119
00:04:41.120 --> 00:04:43.085
been rather popular in game-play

120
00:04:43.085 --> 00:04:44.540
from early successes with

121
00:04:44.540 --> 00:04:47.405
simple games like tic
tac toe in the 1960's,

122
00:04:47.405 --> 00:04:49.595
to bag gammon in 1990's,

123
00:04:49.595 --> 00:04:52.400
down to the very recent and
highly publicized triumph

124
00:04:52.400 --> 00:04:54.865
of ML over the game of Go.

125
00:04:54.865 --> 00:04:58.535
Now, let's look closer
at supervised learning.

126
00:04:58.535 --> 00:05:01.370
Suppose we want a machine
learning algorithm

127
00:05:01.370 --> 00:05:04.235
to distinguish between
circles and squares,

128
00:05:04.235 --> 00:05:06.260
a supervised learning algorithm

129
00:05:06.260 --> 00:05:07.940
would require many examples of

130
00:05:07.940 --> 00:05:09.755
both figures and a teacher

131
00:05:09.755 --> 00:05:11.645
who would tell it which is which.

132
00:05:11.645 --> 00:05:13.715
After the training is finished,

133
00:05:13.715 --> 00:05:15.860
a successful learning
algorithm would be able

134
00:05:15.860 --> 00:05:17.825
to decide on its own whether

135
00:05:17.825 --> 00:05:19.790
any given figure is a circle or

136
00:05:19.790 --> 00:05:22.024
square with sufficient accuracy,

137
00:05:22.024 --> 00:05:24.920
hopefully substantially
better than random guessing.

138
00:05:24.920 --> 00:05:26.990
It would do so even
for circles or

139
00:05:26.990 --> 00:05:29.750
squares that it has never
seen during training,

140
00:05:29.750 --> 00:05:33.020
and this is ultimately the
power of supervised learning.

141
00:05:33.020 --> 00:05:35.510
Do the correct answer is always

142
00:05:35.510 --> 00:05:37.565
need to come from
a human teacher?

143
00:05:37.565 --> 00:05:40.595
The notion of a teacher
may be generalized to

144
00:05:40.595 --> 00:05:41.960
any complex system or

145
00:05:41.960 --> 00:05:44.450
phenomena that
consists of machines,

146
00:05:44.450 --> 00:05:46.795
humans, or natural processes.

147
00:05:46.795 --> 00:05:49.160
Here, you see a Rube
Goldberg machine

148
00:05:49.160 --> 00:05:51.575
to represent this complex system.

149
00:05:51.575 --> 00:05:54.980
You can view this system
as a function that accepts

150
00:05:54.980 --> 00:05:58.145
input parameters and produces
an outcome of sorts.

151
00:05:58.145 --> 00:06:01.505
The known outcomes form the
so-called ground truths,

152
00:06:01.505 --> 00:06:03.875
and this set of
historic observations

153
00:06:03.875 --> 00:06:06.420
is our training dataset.

154
00:06:06.670 --> 00:06:09.485
We then train the
machine learning model

155
00:06:09.485 --> 00:06:12.500
by feeding it this
training dataset.

156
00:06:12.500 --> 00:06:15.485
The resulting model
is set to predict

157
00:06:15.485 --> 00:06:17.030
the same outcome based on

158
00:06:17.030 --> 00:06:19.325
previously unseen
input parameters.

159
00:06:19.325 --> 00:06:21.470
Hopefully, the model
prediction is the

160
00:06:21.470 --> 00:06:22.700
same or close to what

161
00:06:22.700 --> 00:06:24.775
the original system
would have produced.

162
00:06:24.775 --> 00:06:27.080
The reason we're
interested in building

163
00:06:27.080 --> 00:06:30.110
such models is that the
original system is either

164
00:06:30.110 --> 00:06:33.410
impossible or expensive
to procure and scale

165
00:06:33.410 --> 00:06:35.120
or takes too long to produce

166
00:06:35.120 --> 00:06:37.625
the outcome which we
want to obtain sooner.

167
00:06:37.625 --> 00:06:39.710
If the predicted value is of

168
00:06:39.710 --> 00:06:43.310
binary nature as was the case
with circles and squares,

169
00:06:43.310 --> 00:06:44.420
we say that the model is

170
00:06:44.420 --> 00:06:46.735
performing a binary
classification,

171
00:06:46.735 --> 00:06:48.380
in other words, labeling

172
00:06:48.380 --> 00:06:51.080
the observation in
two possible ways.

173
00:06:51.080 --> 00:06:52.820
This is just a special case of

174
00:06:52.820 --> 00:06:55.220
multi-class prediction
where the data point

175
00:06:55.220 --> 00:06:56.300
can belong to one of

176
00:06:56.300 --> 00:06:58.955
many different and
mutually exclusive classes

177
00:06:58.955 --> 00:07:02.180
such as circles,
squares, or triangles.

178
00:07:02.180 --> 00:07:04.250
Mutual exclusivity is tricky to

179
00:07:04.250 --> 00:07:06.470
assure though as this
picture makes clear.

180
00:07:06.470 --> 00:07:10.170
Sometimes it's entirely
a matter of perspective.

181
00:07:10.170 --> 00:07:14.090
Now, if the variable being
predicted is numeric,

182
00:07:14.090 --> 00:07:17.450
then the model is set to be
solving a regression problem.

183
00:07:17.450 --> 00:07:19.379
In other words, determining

184
00:07:19.379 --> 00:07:20.635
the unknown value of

185
00:07:20.635 --> 00:07:23.395
the dependent variable
based on input parameters.

186
00:07:23.395 --> 00:07:26.195
Let us now look at some examples.

187
00:07:26.195 --> 00:07:30.400
For instance, we might
have historical records of

188
00:07:30.400 --> 00:07:32.800
volcano eruptions and
various observations

189
00:07:32.800 --> 00:07:34.705
and measurements
leading up to them.

190
00:07:34.705 --> 00:07:36.215
We can imagine training

191
00:07:36.215 --> 00:07:37.600
a machine learning model

192
00:07:37.600 --> 00:07:40.505
capable of predicting
future eruptions.

193
00:07:40.505 --> 00:07:42.970
The teacher, with a
source of truth in

194
00:07:42.970 --> 00:07:45.955
this case is simply
nature itself.

195
00:07:45.955 --> 00:07:49.345
Another example is when we
want the model to predict

196
00:07:49.345 --> 00:07:51.550
impending equipment failure so

197
00:07:51.550 --> 00:07:53.575
that we could make
pro active repairs,

198
00:07:53.575 --> 00:07:55.955
this is known as
predictive maintenance.

199
00:07:55.955 --> 00:07:58.120
Similarly, we may want to

200
00:07:58.120 --> 00:08:00.470
build a model that predicts
which of our clients

201
00:08:00.470 --> 00:08:01.835
are about to stop purchasing

202
00:08:01.835 --> 00:08:05.119
our services possibly
leaving for a competitor,

203
00:08:05.119 --> 00:08:07.930
this is known as customer
churn prediction.

204
00:08:07.930 --> 00:08:10.560
In these examples, all we

205
00:08:10.560 --> 00:08:13.030
need to do to obtain a
good training dataset with

206
00:08:13.030 --> 00:08:15.490
properly labeled
observations is to

207
00:08:15.490 --> 00:08:18.170
systematically record
historical observations,

208
00:08:18.170 --> 00:08:19.780
together with observed value of

209
00:08:19.780 --> 00:08:22.205
variables we ultimately
want to predict.

210
00:08:22.205 --> 00:08:24.850
You will eventually
know when a volcano

211
00:08:24.850 --> 00:08:27.670
erupts or when your
equipment breaks down.

212
00:08:27.670 --> 00:08:29.705
That is only possible if

213
00:08:29.705 --> 00:08:31.480
the real system we're
trying to model

214
00:08:31.480 --> 00:08:33.520
is already functioning on

215
00:08:33.520 --> 00:08:36.415
a regular basis and
is easy to observe.

216
00:08:36.415 --> 00:08:39.635
If say we want to train
a model to label people

217
00:08:39.635 --> 00:08:42.680
in photos as either
smiling or frowning,

218
00:08:42.680 --> 00:08:44.980
then we would first need
to have someone go through

219
00:08:44.980 --> 00:08:48.310
a large number of photos
and label them manually.

220
00:08:48.310 --> 00:08:52.100
If such human labeling process
was not already in place,

221
00:08:52.100 --> 00:08:53.895
obtaining a training dataset

222
00:08:53.895 --> 00:08:56.150
could be difficult
and time-consuming.

223
00:08:56.150 --> 00:08:58.690
Fortunately, tools to crowdsource

224
00:08:58.690 --> 00:09:00.580
human decisions are
available such as

225
00:09:00.580 --> 00:09:03.250
Amazon Mechanical Turk
or similar offerings

226
00:09:03.250 --> 00:09:06.350
from AWS partners such as
figure eight and others.

227
00:09:06.350 --> 00:09:07.745
So how many different

228
00:09:07.745 --> 00:09:09.885
supervised learning
algorithms are there.

229
00:09:09.885 --> 00:09:12.340
There's literally hundreds
of them in existence,

230
00:09:12.340 --> 00:09:14.435
so there's no point in
mentioning them all.

231
00:09:14.435 --> 00:09:17.560
Instead, we can focus
on a few families of

232
00:09:17.560 --> 00:09:18.730
algorithms that are most

233
00:09:18.730 --> 00:09:21.865
popular and have proven
to be successful.

234
00:09:21.865 --> 00:09:25.355
One of the earliest and
simplest algorithms

235
00:09:25.355 --> 00:09:28.535
is based on learning parameters
of a linear function,

236
00:09:28.535 --> 00:09:31.375
likely in a
multi-dimensional space.

237
00:09:31.375 --> 00:09:34.120
You've already seen an
example of regression where

238
00:09:34.120 --> 00:09:37.125
we find the linear function
that best fits the data.

239
00:09:37.125 --> 00:09:39.215
When it comes to
predicting a category,

240
00:09:39.215 --> 00:09:42.125
or a class as in
circles or squares,

241
00:09:42.125 --> 00:09:45.545
we typically want to find
a hyperplane also known as

242
00:09:45.545 --> 00:09:48.364
a decision boundary
that best separates

243
00:09:48.364 --> 00:09:49.990
the data samples belonging to

244
00:09:49.990 --> 00:09:52.265
the classes as shown
in this picture.

245
00:09:52.265 --> 00:09:54.490
If there exists a linear surface

246
00:09:54.490 --> 00:09:56.115
that separates the two classes,

247
00:09:56.115 --> 00:09:58.985
we say that they are
linearly separable.

248
00:09:58.985 --> 00:10:00.610
This is rarely the
case in practice,

249
00:10:00.610 --> 00:10:03.385
however so some errors
must be expected.

250
00:10:03.385 --> 00:10:05.970
To arrive at the
binary classifier,

251
00:10:05.970 --> 00:10:08.150
we can apply a logistic function

252
00:10:08.150 --> 00:10:10.150
to the output of a
linear combination

253
00:10:10.150 --> 00:10:11.830
of input parameters in order to

254
00:10:11.830 --> 00:10:14.455
restrict the values to the
range from zero to one.

255
00:10:14.455 --> 00:10:16.480
This forms the basis of

256
00:10:16.480 --> 00:10:19.080
the so-called logistic
regression algorithm.

257
00:10:19.080 --> 00:10:21.430
In fact, Amazon SageMaker has

258
00:10:21.430 --> 00:10:23.605
a built-in algorithm
called linear learner,

259
00:10:23.605 --> 00:10:25.090
which is effectively
a combination

260
00:10:25.090 --> 00:10:27.105
of linear and
logistic regression.

261
00:10:27.105 --> 00:10:29.795
But other linear
algorithms exist as well.

262
00:10:29.795 --> 00:10:33.625
You may have heard about
Support Vector Machines or SVMs

263
00:10:33.625 --> 00:10:35.615
which strive to find a hyperplane

264
00:10:35.615 --> 00:10:38.015
with maximum margin
between classes.

265
00:10:38.015 --> 00:10:40.170
More modern variants
of the algorithm also

266
00:10:40.170 --> 00:10:42.805
introduce non-linearity
with kernel functions.

267
00:10:42.805 --> 00:10:44.590
So strictly speaking would not

268
00:10:44.590 --> 00:10:47.030
belong to pure linear
methods anymore.

269
00:10:47.030 --> 00:10:48.640
Perceptron is

270
00:10:48.640 --> 00:10:50.945
another rather simple linear
classifier that forms

271
00:10:50.945 --> 00:10:52.260
the foundational unit of

272
00:10:52.260 --> 00:10:54.695
the so-called artificial
neural networks,

273
00:10:54.695 --> 00:10:57.715
which we'll look at
later in this course.

274
00:10:57.715 --> 00:11:00.220
As I pointed out earlier,

275
00:11:00.220 --> 00:11:01.975
in most practical settings,

276
00:11:01.975 --> 00:11:04.575
we're not dealing with
linearly separable classes

277
00:11:04.575 --> 00:11:06.190
as demonstrated here.

278
00:11:06.190 --> 00:11:09.245
A circular decision
boundary would work here,

279
00:11:09.245 --> 00:11:11.500
but so would a
square-shaped one aligned

280
00:11:11.500 --> 00:11:14.025
with coordinate axis shown here.

281
00:11:14.025 --> 00:11:16.975
This is exactly the
decision boundary used by

282
00:11:16.975 --> 00:11:18.335
algorithms that end up

283
00:11:18.335 --> 00:11:21.110
constructing the
so-called decision trees.

284
00:11:21.110 --> 00:11:23.405
In order to make
a classification,

285
00:11:23.405 --> 00:11:26.579
we start with the root
of the tree and descend

286
00:11:26.579 --> 00:11:27.995
through the decision nodes

287
00:11:27.995 --> 00:11:30.565
until we arrive at
a classification.

288
00:11:30.565 --> 00:11:32.615
In this example, points with

289
00:11:32.615 --> 00:11:34.450
X coordinate outside of the range

290
00:11:34.450 --> 00:11:38.295
from X1 to X2 are immediately
classified as red.

291
00:11:38.295 --> 00:11:40.365
But for those in the range,

292
00:11:40.365 --> 00:11:42.960
we need to additionally
consult the y-coordinate and

293
00:11:42.960 --> 00:11:45.695
check against Y1 and Y2 values.

294
00:11:45.695 --> 00:11:47.895
Instead of constructing
a single tree,

295
00:11:47.895 --> 00:11:50.690
the algorithms from the
tree family often construct

296
00:11:50.690 --> 00:11:54.400
many trees and combine their
predictions in some ways.

297
00:11:54.400 --> 00:11:56.770
Algorithms such as Random Forest

298
00:11:56.770 --> 00:12:00.120
and XGboost are based
on these approaches.

299
00:12:00.120 --> 00:12:04.625
In fact, Amazon SageMaker
includes the XGboost algorithm.

300
00:12:04.625 --> 00:12:07.210
It is based on the
idea of building

301
00:12:07.210 --> 00:12:08.830
a strong classifier out of

302
00:12:08.830 --> 00:12:11.855
many weak classifiers in
the form of decision trees.

303
00:12:11.855 --> 00:12:14.495
Such an approach is
called boosting.

304
00:12:14.495 --> 00:12:18.475
XGboost is a general-purpose
supervised algorithm.

305
00:12:18.475 --> 00:12:22.240
Factorization Machines algorithm
on the other hand works

306
00:12:22.240 --> 00:12:25.645
best when we deal with large
amounts of spars data,

307
00:12:25.645 --> 00:12:28.345
such as the case with the
problem of click prediction for

308
00:12:28.345 --> 00:12:31.555
online advertising or
recommendations in general.

309
00:12:31.555 --> 00:12:35.535
Factorization Machines is
also built into SageMaker.

310
00:12:35.535 --> 00:12:38.350
As usual, many other approaches

311
00:12:38.350 --> 00:12:39.900
and algorithms exist as well.

312
00:12:39.900 --> 00:12:42.640
For example, we could define
the decision boundary to be

313
00:12:42.640 --> 00:12:46.460
polynomial as in circular
or parabolic boundaries.

314
00:12:46.460 --> 00:12:48.220
Of course, we'll come back to

315
00:12:48.220 --> 00:12:50.945
neural networks later
in this course.

316
00:12:50.945 --> 00:12:55.460
Let us now examine the
unsupervised learning algorithms.

317
00:12:55.460 --> 00:12:57.900
Clustering is an
especially popular type

318
00:12:57.900 --> 00:12:59.920
of an unsupervised algorithm.

319
00:12:59.920 --> 00:13:02.135
Given a collection
of data points,

320
00:13:02.135 --> 00:13:05.585
we're trying to divide them
into groups or clusters with

321
00:13:05.585 --> 00:13:07.295
the assumption that
points belonging to

322
00:13:07.295 --> 00:13:09.580
the same cluster are
somehow similar,

323
00:13:09.580 --> 00:13:10.900
whereas those belonging to

324
00:13:10.900 --> 00:13:13.660
different clusters are
somehow dissimilar.

325
00:13:13.660 --> 00:13:15.220
We're still required to give

326
00:13:15.220 --> 00:13:16.630
some guidance to the algorithms

327
00:13:16.630 --> 00:13:18.040
such as specifying the number

328
00:13:18.040 --> 00:13:20.065
of clusters we're looking for.

329
00:13:20.065 --> 00:13:23.160
One problem with clustering
algorithms is that

330
00:13:23.160 --> 00:13:26.100
we usually don't know how
many clusters to pick.

331
00:13:26.100 --> 00:13:28.040
Here's an example of the result

332
00:13:28.040 --> 00:13:30.460
if we request just two clusters.

333
00:13:30.460 --> 00:13:31.950
But depending on various

334
00:13:31.950 --> 00:13:33.580
parameters and distance measures,

335
00:13:33.580 --> 00:13:36.130
a differently tuned
algorithm might

336
00:13:36.130 --> 00:13:37.410
provide a different answer for

337
00:13:37.410 --> 00:13:39.855
the same two clusters requested.

338
00:13:39.855 --> 00:13:42.040
If for this dataset we

339
00:13:42.040 --> 00:13:44.425
request four different
clusters instead,

340
00:13:44.425 --> 00:13:47.215
we might get something
that looks like this.

341
00:13:47.215 --> 00:13:50.199
This points to another
problem with such algorithms,

342
00:13:50.199 --> 00:13:52.810
it is ultimately up to
us how to interpret

343
00:13:52.810 --> 00:13:55.030
the results and assign

344
00:13:55.030 --> 00:13:57.650
meaning to the
discovered clusters.

345
00:13:57.650 --> 00:14:00.560
Another entirely
different family of

346
00:14:00.560 --> 00:14:03.025
unsupervised algorithms
attempts to detect

347
00:14:03.025 --> 00:14:06.450
anomalies or generally
find outliers in the data.

348
00:14:06.450 --> 00:14:10.000
In this picture, we see the
red green and blue lines

349
00:14:10.000 --> 00:14:12.140
representing different
sensory readouts

350
00:14:12.140 --> 00:14:13.820
of the electrocardiogram,

351
00:14:13.820 --> 00:14:16.295
while the top magenta
line corresponds to

352
00:14:16.295 --> 00:14:17.950
the anomaly scores produced by

353
00:14:17.950 --> 00:14:20.090
the algorithm after
observing the data.

354
00:14:20.090 --> 00:14:21.295
The higher the score,

355
00:14:21.295 --> 00:14:23.270
the more pronounced
the anomaly is.

356
00:14:23.270 --> 00:14:24.985
There's no explicit teacher

357
00:14:24.985 --> 00:14:27.395
labeling the historic
data as anomalous,

358
00:14:27.395 --> 00:14:29.880
instead the algorithm
learns on its

359
00:14:29.880 --> 00:14:33.840
own what normal looks like by
simply observing the data.

360
00:14:33.840 --> 00:14:36.545
One anomaly detection
algorithm was

361
00:14:36.545 --> 00:14:38.810
developed by scientists
working at Amazon.

362
00:14:38.810 --> 00:14:40.975
So it's worth taking
a closer look.

363
00:14:40.975 --> 00:14:43.435
It's called Random Cut Forest.

364
00:14:43.435 --> 00:14:45.510
The algorithm works
by constructing

365
00:14:45.510 --> 00:14:49.120
a forest of the so-called
random cut trees.

366
00:14:49.120 --> 00:14:50.680
Each tree is constructed by

367
00:14:50.680 --> 00:14:52.150
a recursive procedure which

368
00:14:52.150 --> 00:14:53.560
first surrounds the data points

369
00:14:53.560 --> 00:14:56.440
with a bounding box and
then cuts or splits at

370
00:14:56.440 --> 00:15:00.010
along the coordinate axis by
picking cut points randomly.

371
00:15:00.010 --> 00:15:02.215
The procedure is repeated until

372
00:15:02.215 --> 00:15:05.575
every point is sorted into a
particular leaf of the tree.

373
00:15:05.575 --> 00:15:08.620
For full details you can
read the paper presented on

374
00:15:08.620 --> 00:15:10.585
the International Conference
for Machine Learning

375
00:15:10.585 --> 00:15:12.370
in 2016.

376
00:15:12.370 --> 00:15:15.545
Yet another example of
unsupervised algorithm is

377
00:15:15.545 --> 00:15:17.080
the so-called topic modeling for

378
00:15:17.080 --> 00:15:19.050
documents with text content.

379
00:15:19.050 --> 00:15:21.100
The algorithm is the basis of

380
00:15:21.100 --> 00:15:24.650
the eponymous feature in the
Amazon comprehend service.

381
00:15:24.650 --> 00:15:26.875
Given a collection of documents,

382
00:15:26.875 --> 00:15:28.900
news articles for example and

383
00:15:28.900 --> 00:15:31.115
the number of topics we
would like to discover,

384
00:15:31.115 --> 00:15:33.260
the algorithm produces
the top words

385
00:15:33.260 --> 00:15:35.080
that appear to define the topic,

386
00:15:35.080 --> 00:15:37.300
together with the
weight that each

387
00:15:37.300 --> 00:15:39.665
of these words has in
relation to the topic.

388
00:15:39.665 --> 00:15:41.350
In this case you see top words

389
00:15:41.350 --> 00:15:43.685
that likely pertained to sports.

390
00:15:43.685 --> 00:15:45.815
As with clustering in general,

391
00:15:45.815 --> 00:15:47.620
the approach is sensitive
to the number of

392
00:15:47.620 --> 00:15:49.865
topics requested and its still

393
00:15:49.865 --> 00:15:51.730
requires us to assign
the meaning to

394
00:15:51.730 --> 00:15:55.775
the discovered topic such as
health, sports or politics.

395
00:15:55.775 --> 00:15:58.720
To summarize, Amazon
SageMaker includes

396
00:15:58.720 --> 00:16:01.735
a popular clustering
algorithm called k-means.

397
00:16:01.735 --> 00:16:03.605
It's an Amazon improvement over

398
00:16:03.605 --> 00:16:05.530
the well-known and
scalable algorithm

399
00:16:05.530 --> 00:16:07.685
called Web-scale k-means.

400
00:16:07.685 --> 00:16:10.805
Another member of the
unsupervised family is called

401
00:16:10.805 --> 00:16:13.865
Principal Component
Analysis or PCA for short.

402
00:16:13.865 --> 00:16:16.225
Likewise available in SageMaker.

403
00:16:16.225 --> 00:16:18.310
It's especially
useful in reducing

404
00:16:18.310 --> 00:16:20.740
the dimensionality of
the dataset and is often

405
00:16:20.740 --> 00:16:22.585
used as a feature
engineering step

406
00:16:22.585 --> 00:16:25.750
before passing the data to
a supervised algorithms.

407
00:16:25.750 --> 00:16:28.944
Latent Dirichlet
Allocation or LDA

408
00:16:28.944 --> 00:16:31.775
is the name of a particular
topic modelings algorithm.

409
00:16:31.775 --> 00:16:35.105
A variant is used by the topic
modeling feature of Amazon

410
00:16:35.105 --> 00:16:36.910
comprehend and the algorithm

411
00:16:36.910 --> 00:16:39.140
is also available in sage maker.

412
00:16:39.140 --> 00:16:41.435
The Random cut forest algorithm

413
00:16:41.435 --> 00:16:43.150
for anomaly detection
is available in

414
00:16:43.150 --> 00:16:46.615
SageMaker as well as an Amazon
Kinesis Data Analytics,

415
00:16:46.615 --> 00:16:49.715
for easy application
to streaming data.

416
00:16:49.715 --> 00:16:53.495
Kinesis Data Analytics also
features hotspot detection,

417
00:16:53.495 --> 00:16:54.670
another example of

418
00:16:54.670 --> 00:16:56.770
an unsupervised learning
algorithm which

419
00:16:56.770 --> 00:16:58.000
you can use to identify

420
00:16:58.000 --> 00:17:00.350
relatively dense
regions in your data.

421
00:17:00.350 --> 00:17:03.220
Okay, time for a quick quiz.

422
00:17:03.220 --> 00:17:06.100
Suppose we have a
problem of predicting

423
00:17:06.100 --> 00:17:09.010
future values of a
time series data.

424
00:17:09.010 --> 00:17:11.230
For example, suppose that we

425
00:17:11.230 --> 00:17:13.835
want to predict the future
sales of some item.

426
00:17:13.835 --> 00:17:16.930
We have observed historical
daily sales figures up to

427
00:17:16.930 --> 00:17:18.580
today and now want to

428
00:17:18.580 --> 00:17:21.380
predict what the sales figures
would be in the future.

429
00:17:21.380 --> 00:17:25.835
Is this a supervised or
unsupervised learning task?

430
00:17:25.835 --> 00:17:28.325
At first we might be tempted to

431
00:17:28.325 --> 00:17:30.335
answer that it is unsupervised,

432
00:17:30.335 --> 00:17:32.205
similar to Anomaly Detection,

433
00:17:32.205 --> 00:17:35.245
because there does not appear
to be a teacher anywhere.

434
00:17:35.245 --> 00:17:37.370
But this is a bit of
a trick question.

435
00:17:37.370 --> 00:17:40.960
You'd see all observations of
historical sales leading to

436
00:17:40.960 --> 00:17:44.555
a particular day D in
the past can be viewed

437
00:17:44.555 --> 00:17:46.370
as a training sample or

438
00:17:46.370 --> 00:17:48.730
observation with the
correct label being

439
00:17:48.730 --> 00:17:53.305
the actual recorded sale for
day D, eight in this case.

440
00:17:53.305 --> 00:17:55.150
In other words, just like with

441
00:17:55.150 --> 00:17:57.069
the historical volcano eruptions,

442
00:17:57.069 --> 00:18:00.640
the teacher here is the
external environment.

443
00:18:00.810 --> 00:18:03.515
Finally, it's time
for us to look at

444
00:18:03.515 --> 00:18:04.900
deep learning which is really

445
00:18:04.900 --> 00:18:06.785
a resurgence of neural networks.

446
00:18:06.785 --> 00:18:09.275
To understand them,
let's first look at

447
00:18:09.275 --> 00:18:12.100
an element of neural
networks called a neuron.

448
00:18:12.100 --> 00:18:14.865
Through a neuron you
see in this diagram,

449
00:18:14.865 --> 00:18:16.090
a data sample is seen as

450
00:18:16.090 --> 00:18:18.035
a vector of numeric input values,

451
00:18:18.035 --> 00:18:20.660
which are then linearly
combined with its weights.

452
00:18:20.660 --> 00:18:23.080
In other words, the neuron is

453
00:18:23.080 --> 00:18:25.715
computing a weighted
sum and then applies

454
00:18:25.715 --> 00:18:28.180
a so-called activation
function to

455
00:18:28.180 --> 00:18:31.000
produce output in the
range from zero to one.

456
00:18:31.000 --> 00:18:33.070
With proper thresholding this

457
00:18:33.070 --> 00:18:35.105
can work as a binary classifier.

458
00:18:35.105 --> 00:18:36.580
Remember the perceptron that I

459
00:18:36.580 --> 00:18:38.240
mentioned earlier in this course?

460
00:18:38.240 --> 00:18:41.065
This is effectively
what a perceptron is.

461
00:18:41.065 --> 00:18:43.960
Except, a single
neuron would not be

462
00:18:43.960 --> 00:18:46.285
sufficient for practical
classification needs.

463
00:18:46.285 --> 00:18:48.130
Instead, we could combine

464
00:18:48.130 --> 00:18:50.170
them into fully-connected
layers to produce

465
00:18:50.170 --> 00:18:52.525
the so-called artificial
neural networks

466
00:18:52.525 --> 00:18:55.665
also known as
multilayer perceptrons.

467
00:18:55.665 --> 00:18:57.495
In a feedforward pass,

468
00:18:57.495 --> 00:19:00.800
the network is turning the
input values into output,

469
00:19:00.800 --> 00:19:03.235
which forms the prediction
of the algorithm.

470
00:19:03.235 --> 00:19:05.310
A special technique called

471
00:19:05.310 --> 00:19:07.585
backpropagation is
then used to reduce

472
00:19:07.585 --> 00:19:10.260
the error between the
desired or true output

473
00:19:10.260 --> 00:19:12.805
and the actual one
produced by the network.

474
00:19:12.805 --> 00:19:16.085
Originally, these neural
networks were inspired

475
00:19:16.085 --> 00:19:19.000
by some aspects of a
biological nervous system but

476
00:19:19.000 --> 00:19:20.140
at this point there are really

477
00:19:20.140 --> 00:19:21.900
a computational apparatus for

478
00:19:21.900 --> 00:19:25.400
complex dependency modeling
and function approximation.

479
00:19:25.400 --> 00:19:27.970
What I showed so far
is an example of

480
00:19:27.970 --> 00:19:29.375
a traditional neural network

481
00:19:29.375 --> 00:19:31.670
prior to the advent
of deep-learning.

482
00:19:31.670 --> 00:19:34.015
Since a few years
back we have seen

483
00:19:34.015 --> 00:19:36.335
a resurgence of neural
networks rebranded as

484
00:19:36.335 --> 00:19:38.860
deep-learning due to
several important advances

485
00:19:38.860 --> 00:19:41.150
that relate to the
algorithms themselves.

486
00:19:41.150 --> 00:19:44.345
Accumulation of large amounts
of data for training and

487
00:19:44.345 --> 00:19:48.120
emergence of powerful specialized
hardware such as GPUs,

488
00:19:48.120 --> 00:19:49.330
which are able to crunch

489
00:19:49.330 --> 00:19:51.490
this massive amount
of data by passing

490
00:19:51.490 --> 00:19:53.105
it through very deep networks in

491
00:19:53.105 --> 00:19:55.295
terms of the sheer
number of layers.

492
00:19:55.295 --> 00:19:58.290
Some of the results proved
rather spectacular,

493
00:19:58.290 --> 00:20:00.309
enabling many
exciting applications

494
00:20:00.309 --> 00:20:02.485
such as an image and
speech recognition,

495
00:20:02.485 --> 00:20:05.465
natural language
processing and so on.

496
00:20:05.465 --> 00:20:07.430
So how deep is deep?

497
00:20:07.430 --> 00:20:08.970
Here's an example that is rather

498
00:20:08.970 --> 00:20:10.520
puny by modern standards.

499
00:20:10.520 --> 00:20:12.460
In fact, networks with over

500
00:20:12.460 --> 00:20:14.945
1,000 layers have been
experimented with.

501
00:20:14.945 --> 00:20:17.110
Such networks have billions of

502
00:20:17.110 --> 00:20:18.820
parameters and many millions

503
00:20:18.820 --> 00:20:20.795
of images could be
used in training.

504
00:20:20.795 --> 00:20:22.780
The shear computational power

505
00:20:22.780 --> 00:20:25.210
required to train such
networks is not cheap to

506
00:20:25.210 --> 00:20:27.400
procure and this is where eta BFS

507
00:20:27.400 --> 00:20:30.285
comes handy with GPU
based ec2 instances,

508
00:20:30.285 --> 00:20:32.710
housing powerful chipsets such

509
00:20:32.710 --> 00:20:35.505
as NVIDIA Volta in the P3 family.

510
00:20:35.505 --> 00:20:38.130
More importantly, you can
distribute the training

511
00:20:38.130 --> 00:20:40.900
across multiple GPUs
in order to speed it

512
00:20:40.900 --> 00:20:42.965
up and AWS makes it rather

513
00:20:42.965 --> 00:20:45.530
economical to set up
the hardware cluster

514
00:20:45.530 --> 00:20:47.920
just for the time of
training not having to

515
00:20:47.920 --> 00:20:50.980
worry about expensive hardware
sitting idly afterwards.

516
00:20:50.980 --> 00:20:52.960
One important breakthrough in

517
00:20:52.960 --> 00:20:54.485
deep learning was
the invention of

518
00:20:54.485 --> 00:20:56.615
the so-called Convolutional
Neural Networks

519
00:20:56.615 --> 00:20:58.270
or CNNs for short,

520
00:20:58.270 --> 00:21:01.580
which are especially useful
for image processing.

521
00:21:01.580 --> 00:21:05.740
The main idea behind CNNs is
that it is able to relate

522
00:21:05.740 --> 00:21:07.810
nearby pixels in the
image instead of

523
00:21:07.810 --> 00:21:10.340
treating them as completely
independent inputs,

524
00:21:10.340 --> 00:21:12.740
which was the case prior to CNNs.

525
00:21:12.740 --> 00:21:14.740
A special operation called

526
00:21:14.740 --> 00:21:17.980
convolution is applied to
the entire subsections of

527
00:21:17.980 --> 00:21:20.650
the image and more
importantly the parameters of

528
00:21:20.650 --> 00:21:24.050
these convolutions are also
being learned in the process.

529
00:21:24.050 --> 00:21:26.130
If several convolutional layers

530
00:21:26.130 --> 00:21:28.150
are stacked one after another,

531
00:21:28.150 --> 00:21:31.265
each convolutional layer learns
to recognize patterns of

532
00:21:31.265 --> 00:21:34.855
increasing complexity as we
move through the layers.

533
00:21:34.855 --> 00:21:37.300
We don't have time in
this course to dive

534
00:21:37.300 --> 00:21:39.620
into the details of
how CNN function.

535
00:21:39.620 --> 00:21:43.225
Our goal is to understand
some of the common use cases.

536
00:21:43.225 --> 00:21:45.880
Of course recognizing
objects and images and

537
00:21:45.880 --> 00:21:48.860
generally classifying images
is a very common use case.

538
00:21:48.860 --> 00:21:50.400
But CNNs enabled

539
00:21:50.400 --> 00:21:53.410
many other exciting
applications related to images.

540
00:21:53.410 --> 00:21:55.180
For example, they can be

541
00:21:55.180 --> 00:21:57.065
used for semantic segmentation or

542
00:21:57.065 --> 00:21:59.045
classification of
individual pixels as

543
00:21:59.045 --> 00:22:02.045
belonging or not belonging
to detected objects,

544
00:22:02.045 --> 00:22:04.060
a motorcyclist in this case.

545
00:22:04.060 --> 00:22:06.520
Furthermore, they
have been used for

546
00:22:06.520 --> 00:22:08.620
other novel applications such as

547
00:22:08.620 --> 00:22:11.480
artistic style transfer,
where one image,

548
00:22:11.480 --> 00:22:14.700
here a photo of a cat
is modified by applying

549
00:22:14.700 --> 00:22:16.780
an artistic style to it which was

550
00:22:16.780 --> 00:22:18.010
previously extracted from

551
00:22:18.010 --> 00:22:20.240
another image
typically a painting.

552
00:22:20.240 --> 00:22:22.145
In the bottom right corner,

553
00:22:22.145 --> 00:22:25.220
they have even been used to
generate photos of cats.

554
00:22:25.220 --> 00:22:27.730
These cats look
photorealistic and

555
00:22:27.730 --> 00:22:30.530
yet none of them actually
existed in real life.

556
00:22:30.530 --> 00:22:33.520
If we take the output of
a neuron and feed it as

557
00:22:33.520 --> 00:22:36.605
input to itself or neurons
from previous layers,

558
00:22:36.605 --> 00:22:39.735
we are creating the so-called
recurrent neural networks.

559
00:22:39.735 --> 00:22:42.520
It's as if the neuron
remembers its output from

560
00:22:42.520 --> 00:22:45.660
the previous iteration thus
creating a kind of memory.

561
00:22:45.660 --> 00:22:48.340
On the right-hand side
you see just one unit

562
00:22:48.340 --> 00:22:51.110
of a more complex
network called LSTM,

563
00:22:51.110 --> 00:22:53.620
which stands for Long
Short-Term Memory.

564
00:22:53.620 --> 00:22:55.085
It is commonly used for

565
00:22:55.085 --> 00:22:57.030
speech recognition
and translation.

566
00:22:57.030 --> 00:23:00.190
In fact, LSTMs are used
as a building block to

567
00:23:00.190 --> 00:23:01.835
the so-called
Sequence to Sequence

568
00:23:01.835 --> 00:23:05.265
modeling which is used in
neural machine translation.

569
00:23:05.265 --> 00:23:08.470
A high-level architecture in
the diagram shows how does

570
00:23:08.470 --> 00:23:10.060
include house input is

571
00:23:10.060 --> 00:23:12.185
being translated
into the greenhouse.

572
00:23:12.185 --> 00:23:14.230
Amazon released an entire library

573
00:23:14.230 --> 00:23:15.700
called Sockeye for state of

574
00:23:15.700 --> 00:23:17.350
the art sequence to sequence

575
00:23:17.350 --> 00:23:20.355
modeling tasks that customers
can use in their projects.

576
00:23:20.355 --> 00:23:21.985
We just looked at convolutional

577
00:23:21.985 --> 00:23:23.380
and recurrent neural networks

578
00:23:23.380 --> 00:23:24.715
which are the two most

579
00:23:24.715 --> 00:23:26.635
common families of
neural networks.

580
00:23:26.635 --> 00:23:27.940
But as you can see here

581
00:23:27.940 --> 00:23:32.015
many network topologies exist
and they're being studied.

582
00:23:32.015 --> 00:23:34.030
Amazon SageMaker

583
00:23:34.030 --> 00:23:36.130
conveniently provides
a built-in algorithm

584
00:23:36.130 --> 00:23:40.225
for image classification based
on Resnet, a kind of CNN,

585
00:23:40.225 --> 00:23:43.405
but it also provides a sequence
to sequence algorithm,

586
00:23:43.405 --> 00:23:45.735
a neural topic modeling algorithm

587
00:23:45.735 --> 00:23:48.475
to complement Latent
Dirichlet allocation

588
00:23:48.475 --> 00:23:50.804
and also DeepAR forecasting

589
00:23:50.804 --> 00:23:52.375
algorithm for time series

590
00:23:52.375 --> 00:23:54.120
prediction which we
already looked at.

591
00:23:54.120 --> 00:23:56.585
Remember the quiz? So our

592
00:23:56.585 --> 00:23:59.810
deep learning algorithm's
supervised or unsupervised?

593
00:23:59.810 --> 00:24:01.675
Well, they can be either.

594
00:24:01.675 --> 00:24:05.079
The algorithms shown in the
slide are all supervised

595
00:24:05.079 --> 00:24:06.450
except for neural topic

596
00:24:06.450 --> 00:24:09.745
modeling which the icons
on the left indicate.

597
00:24:09.745 --> 00:24:12.355
Deep learning algorithms
have even been

598
00:24:12.355 --> 00:24:14.010
employed as a key component

599
00:24:14.010 --> 00:24:16.170
of a reinforcement
learning algorithm.

600
00:24:16.170 --> 00:24:18.390
Well, this concludes
our review of

601
00:24:18.390 --> 00:24:20.165
various Machine
Learning algorithms.

602
00:24:20.165 --> 00:24:21.630
Hopefully, you've come to

603
00:24:21.630 --> 00:24:23.070
understand the
different categories of

604
00:24:23.070 --> 00:24:24.890
Machine Learning
algorithms and how they

605
00:24:24.890 --> 00:24:27.120
relate to the business
problems they help solve.

606
00:24:27.120 --> 00:24:29.850
Thanks for listening. You
can follow me on Twitter

607
00:24:29.850 --> 00:24:33.550
and please tune into other
courses in this series.