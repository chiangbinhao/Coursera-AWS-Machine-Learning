WEBVTT

1
00:00:10.900 --> 00:00:15.215
Hi, I'm Dan Mbaga with AWS AI.

2
00:00:15.215 --> 00:00:18.295
Welcome to the Introduction
to Deep Learning Course.

3
00:00:18.295 --> 00:00:21.090
I have been with as for four
years and I'm currently

4
00:00:21.090 --> 00:00:22.740
responsible for
business development

5
00:00:22.740 --> 00:00:24.705
management in machine learning.

6
00:00:24.705 --> 00:00:27.390
As part of AWS AI Team,

7
00:00:27.390 --> 00:00:28.710
have worked with our customers

8
00:00:28.710 --> 00:00:30.000
to build their machine learning

9
00:00:30.000 --> 00:00:33.900
products from conception
to production on AWS.

10
00:00:33.900 --> 00:00:36.270
In this video, you
will learn about

11
00:00:36.270 --> 00:00:38.600
deep-learning or DL and about

12
00:00:38.600 --> 00:00:40.820
the services that AWS offers for

13
00:00:40.820 --> 00:00:43.430
developing deep-learning
based applications.

14
00:00:43.430 --> 00:00:46.190
We'll also discuss
a case study where

15
00:00:46.190 --> 00:00:49.430
one of our customers is
innovating with Deep Learning.

16
00:00:49.430 --> 00:00:52.025
DL is a subset of
machine learning,

17
00:00:52.025 --> 00:00:55.460
which itself is a subset of
Artificial Intelligence.

18
00:00:55.460 --> 00:00:59.405
Scientists started studying
deep-learning in 1950s and

19
00:00:59.405 --> 00:01:01.250
devoted significant resources to

20
00:01:01.250 --> 00:01:03.290
wait for the next 70 years.

21
00:01:03.290 --> 00:01:05.990
The foundation for
the current error

22
00:01:05.990 --> 00:01:07.790
was laid in the 1980s and

23
00:01:07.790 --> 00:01:09.960
the 1990s with research from

24
00:01:09.960 --> 00:01:12.630
Yann LeCun on Convolutional
Neural Networks,

25
00:01:12.630 --> 00:01:14.870
and along short-term memories or

26
00:01:14.870 --> 00:01:18.400
LSTM by Sepp Hochreiter
and Juergen Schmidhuber.

27
00:01:18.400 --> 00:01:21.470
In 1986, the rediscovery of

28
00:01:21.470 --> 00:01:23.810
the backpropagation
training algorithm

29
00:01:23.810 --> 00:01:25.640
marked a significant milestone

30
00:01:25.640 --> 00:01:27.425
in the study of Deep Learning.

31
00:01:27.425 --> 00:01:31.155
The backpropagation algorithm
helps the model learn from

32
00:01:31.155 --> 00:01:34.715
his mistakes by leveraging the
chain rule of derivatives.

33
00:01:34.715 --> 00:01:37.220
But in the decades
that followed called

34
00:01:37.220 --> 00:01:40.835
a neural winter research into
Deep Learning dropped off.

35
00:01:40.835 --> 00:01:44.605
This was partly due to
limitations on data and compute.

36
00:01:44.605 --> 00:01:48.615
The introduction of the Internet,
smartphones, smart TVs,

37
00:01:48.615 --> 00:01:52.040
and the availability of
inexpensive digital cameras,

38
00:01:52.040 --> 00:01:54.080
meant that more and more data was

39
00:01:54.080 --> 00:01:57.400
available.. Computing
power was on the rise.

40
00:01:57.400 --> 00:01:59.640
CPUs were becoming faster,

41
00:01:59.640 --> 00:02:03.040
and GPUs became a
general-purpose computing tool.

42
00:02:03.040 --> 00:02:06.050
Both trends made neural
networks progress.

43
00:02:06.050 --> 00:02:09.670
In 1998, Yann LeCun
publish a paper

44
00:02:09.670 --> 00:02:11.440
on convolutional neural networks

45
00:02:11.440 --> 00:02:13.080
for image recognition tasks.

46
00:02:13.080 --> 00:02:15.280
But it wasn't until 2007

47
00:02:15.280 --> 00:02:17.980
when the research began
to accelerate again.

48
00:02:17.980 --> 00:02:22.160
The advent of GPU and
reduction in training time,

49
00:02:22.160 --> 00:02:24.005
ushered in the mainstay

50
00:02:24.005 --> 00:02:26.000
of Neural Networks
and Deep Learning.

51
00:02:26.000 --> 00:02:29.065
Both data and computing
power made the task

52
00:02:29.065 --> 00:02:32.195
that neural networks tackled
more and more interesting.

53
00:02:32.195 --> 00:02:34.859
With GPS becoming
increasingly popular,

54
00:02:34.859 --> 00:02:38.010
Neural Networks
resurface in 2008.

55
00:02:38.010 --> 00:02:41.470
Deep-learning uses Artificial
Neural Networks to

56
00:02:41.470 --> 00:02:44.005
process and evaluate
its reference data

57
00:02:44.005 --> 00:02:45.920
and come to a conclusion.

58
00:02:45.920 --> 00:02:49.235
Artificial Neural Networks or ANN

59
00:02:49.235 --> 00:02:50.590
are different from traditional

60
00:02:50.590 --> 00:02:52.555
Compute Processing Architectures.

61
00:02:52.555 --> 00:02:54.040
In that, they're designed to

62
00:02:54.040 --> 00:02:56.425
operate more like human brain.

63
00:02:56.425 --> 00:02:58.390
The more flexible and better

64
00:02:58.390 --> 00:03:00.700
at handling
unanticipated anomalies,

65
00:03:00.700 --> 00:03:02.470
and novelty is in a data.

66
00:03:02.470 --> 00:03:05.785
We'll talk more about Artificial
Neural Networks later.

67
00:03:05.785 --> 00:03:09.685
Deep Learning is a subset of
machine learning algorithms.

68
00:03:09.685 --> 00:03:11.740
Deep-learning uses layers of

69
00:03:11.740 --> 00:03:13.540
non-linear Processing Units for

70
00:03:13.540 --> 00:03:16.405
features extraction
and transformation.

71
00:03:16.405 --> 00:03:18.370
Each successive layer uses

72
00:03:18.370 --> 00:03:21.140
the output from the
previous layer as an input.

73
00:03:21.140 --> 00:03:23.585
The algorithms may
be supervised or

74
00:03:23.585 --> 00:03:27.305
unsupervised and applications
include pattern analysis,

75
00:03:27.305 --> 00:03:29.315
which is unsupervised, and

76
00:03:29.315 --> 00:03:32.765
classification which could be
supervised or unsupervised.

77
00:03:32.765 --> 00:03:34.580
These algorithms are also

78
00:03:34.580 --> 00:03:36.680
based on the
unsupervised learning of

79
00:03:36.680 --> 00:03:38.585
multiple levels of features

80
00:03:38.585 --> 00:03:40.970
or representations of the data.

81
00:03:40.970 --> 00:03:43.730
Higher-level features
are derived from

82
00:03:43.730 --> 00:03:47.720
low-level features to form a
hierarchical representation.

83
00:03:47.720 --> 00:03:49.970
Deep learning
algorithms are part of

84
00:03:49.970 --> 00:03:51.725
a broader Machine Learning field

85
00:03:51.725 --> 00:03:54.020
of learning
representations of data,

86
00:03:54.020 --> 00:03:55.730
and they learn multiple levels of

87
00:03:55.730 --> 00:03:58.220
representations
that correspond to

88
00:03:58.220 --> 00:04:00.110
different levels of abstraction.

89
00:04:00.110 --> 00:04:01.860
Where traditional
Machine Learning

90
00:04:01.860 --> 00:04:03.900
focuses on feature engineering,

91
00:04:03.900 --> 00:04:05.225
Deep Learning focuses on

92
00:04:05.225 --> 00:04:08.075
end-to-end learning
based on raw features.

93
00:04:08.075 --> 00:04:10.100
Each layer is responsible for

94
00:04:10.100 --> 00:04:13.250
analyzing additional complex
features in the data.

95
00:04:13.250 --> 00:04:15.650
A Neural network
is a collection of

96
00:04:15.650 --> 00:04:18.290
simple trainable
mathematical unit

97
00:04:18.290 --> 00:04:21.025
that collectively learn
complex functions.

98
00:04:21.025 --> 00:04:22.805
With enough training data,

99
00:04:22.805 --> 00:04:25.760
a Neural Network can perform
a decent job of mapping

100
00:04:25.760 --> 00:04:29.105
input data and features
to output decisions.

101
00:04:29.105 --> 00:04:31.640
It consists of multiple layers.

102
00:04:31.640 --> 00:04:33.260
There is an input layer,

103
00:04:33.260 --> 00:04:36.150
some hidden layers,
and an output layer.

104
00:04:36.150 --> 00:04:39.780
The basic unit of an
Artificial Neural Network is

105
00:04:39.780 --> 00:04:43.505
Artificial Neuron sometimes
also called a node.

106
00:04:43.505 --> 00:04:47.180
Like the biological neurons
for which they are named,

107
00:04:47.180 --> 00:04:50.320
artificial neurons have
several input channels.

108
00:04:50.320 --> 00:04:54.680
A neuron sums these inputs
inside of processing stage,

109
00:04:54.680 --> 00:04:56.960
and produces one output that can

110
00:04:56.960 --> 00:04:59.765
fan out to multiple other
artificial neurons.

111
00:04:59.765 --> 00:05:01.729
In this simplified example,

112
00:05:01.729 --> 00:05:03.680
the input values
are multiplied by

113
00:05:03.680 --> 00:05:06.460
the weight to get
their weighted value.

114
00:05:06.460 --> 00:05:09.635
Then if appropriate,
the node adds

115
00:05:09.635 --> 00:05:12.935
an offset vector to the
sum called the bias,

116
00:05:12.935 --> 00:05:14.450
which adjusts to sum to

117
00:05:14.450 --> 00:05:16.640
generate more
accurate predictions,

118
00:05:16.640 --> 00:05:18.080
based on the success or

119
00:05:18.080 --> 00:05:20.760
failure of the
product predictions.

120
00:05:20.760 --> 00:05:24.450
Once the inputs have been
weighted then sumed,

121
00:05:24.450 --> 00:05:27.165
and the bias is added
if appropriate,

122
00:05:27.165 --> 00:05:30.350
the neuron activates if the
final value produced by

123
00:05:30.350 --> 00:05:32.540
the preceding steps meets or

124
00:05:32.540 --> 00:05:35.675
exceeds the determined
activation threshold.

125
00:05:35.675 --> 00:05:37.975
That's called the
activation step.

126
00:05:37.975 --> 00:05:41.370
It's the final step before
an output is deliver.

127
00:05:41.370 --> 00:05:44.075
The Feedforward Neural Network is

128
00:05:44.075 --> 00:05:45.590
any neural network that doesn't

129
00:05:45.590 --> 00:05:47.540
form a cycle between neurons.

130
00:05:47.540 --> 00:05:49.880
This means data
moves from one input

131
00:05:49.880 --> 00:05:52.445
to output without
looping backward.

132
00:05:52.445 --> 00:05:55.829
This another Neural Network
that does look backwards,

133
00:05:55.829 --> 00:05:58.365
and that's called
Recurrent Neural Network.

134
00:05:58.365 --> 00:06:01.955
The primary value of a
recurrent neural network

135
00:06:01.955 --> 00:06:03.170
comes when processing

136
00:06:03.170 --> 00:06:05.300
sequential information
such as text,

137
00:06:05.300 --> 00:06:07.465
or speech, or handwriting.

138
00:06:07.465 --> 00:06:10.370
Where the ability to
predict the next word or

139
00:06:10.370 --> 00:06:13.505
later is vastly improve if
you're factoring in the words,

140
00:06:13.505 --> 00:06:15.295
or letters that come before it.

141
00:06:15.295 --> 00:06:18.140
Recurrent Neural Networks
became much more popular

142
00:06:18.140 --> 00:06:20.960
after 2007 when Long Short Term

143
00:06:20.960 --> 00:06:23.630
Memory or LSTM approaches

144
00:06:23.630 --> 00:06:26.905
revolutionized speech
recognition programs.

145
00:06:26.905 --> 00:06:29.360
LSTM is now the basis for many of

146
00:06:29.360 --> 00:06:31.400
today's most successful
applications

147
00:06:31.400 --> 00:06:33.290
in the speech recognition domain,

148
00:06:33.290 --> 00:06:34.640
text-to-speech domain,

149
00:06:34.640 --> 00:06:36.890
and handwriting
recognition domain.

150
00:06:36.890 --> 00:06:40.630
Use cases for Deep Learning
span multiple industries.

151
00:06:40.630 --> 00:06:42.100
Let's have a look at each.

152
00:06:42.100 --> 00:06:45.559
You can find text analysis
use cases in a finance,

153
00:06:45.559 --> 00:06:49.925
social, CRM, and insurance
domains to name a few.

154
00:06:49.925 --> 00:06:52.595
It's used to detect
insider training.

155
00:06:52.595 --> 00:06:54.865
Check for regulatory compliance.

156
00:06:54.865 --> 00:06:56.355
Brand affinity.

157
00:06:56.355 --> 00:06:57.795
Sentiment analysis.

158
00:06:57.795 --> 00:06:59.675
Intent Analysis, and more

159
00:06:59.675 --> 00:07:02.915
by essentially analyzing
blobs of text.

160
00:07:02.915 --> 00:07:05.750
Deep Learning is also
used to solve problems

161
00:07:05.750 --> 00:07:08.345
around time-series and
predictive analysis.

162
00:07:08.345 --> 00:07:10.075
It's using datacenters for

163
00:07:10.075 --> 00:07:12.960
Log Analysis and risk
fraud detection,

164
00:07:12.960 --> 00:07:16.055
by the supply chain industry
for resource planning,

165
00:07:16.055 --> 00:07:17.780
and in the IoT field for

166
00:07:17.780 --> 00:07:19.975
predictive analysis
using sensor data.

167
00:07:19.975 --> 00:07:21.855
It's also used in social media,

168
00:07:21.855 --> 00:07:25.115
and e-commerce for building
recommendation engines.

169
00:07:25.115 --> 00:07:27.570
It's using sound analysis too.

170
00:07:27.570 --> 00:07:29.660
You find Deep Learning
being used in

171
00:07:29.660 --> 00:07:31.865
the security domain
for voice recognition,

172
00:07:31.865 --> 00:07:33.440
voice analysis, and in

173
00:07:33.440 --> 00:07:35.890
the CRM domain for
sentiment analysis.

174
00:07:35.890 --> 00:07:37.940
You'll also find deep learning in

175
00:07:37.940 --> 00:07:41.014
both the automotive and
aviation industries,

176
00:07:41.014 --> 00:07:42.875
where it's used for Engine

177
00:07:42.875 --> 00:07:44.885
and instrument floor detection.

178
00:07:44.885 --> 00:07:46.220
You'll even find deep learning

179
00:07:46.220 --> 00:07:47.540
in the finance industry for

180
00:07:47.540 --> 00:07:50.545
credit card fraud detection
among other things.

181
00:07:50.545 --> 00:07:53.945
Finally, it's used
for image analysis.

182
00:07:53.945 --> 00:07:55.475
In the security domain,

183
00:07:55.475 --> 00:07:58.805
Deep Learning is used for
things like facial recognition.

184
00:07:58.805 --> 00:08:01.100
In social media, it's used for

185
00:08:01.100 --> 00:08:04.045
tagging and identifying
people in pictures..

186
00:08:04.045 --> 00:08:06.710
The challenge of course is scale.

187
00:08:06.710 --> 00:08:09.500
In 2012, AlexNet won

188
00:08:09.500 --> 00:08:12.740
the convolutional neural
network ImageNet competition.

189
00:08:12.740 --> 00:08:14.685
It consisted of eight layers,

190
00:08:14.685 --> 00:08:17.805
650,000 interconnected neurons,

191
00:08:17.805 --> 00:08:20.340
and almost 60 million parameters.

192
00:08:20.340 --> 00:08:22.560
Today, the complexity of

193
00:08:22.560 --> 00:08:24.915
Neural Networks has
increased significantly.

194
00:08:24.915 --> 00:08:28.110
With recent networks
such as Resnet 152,

195
00:08:28.110 --> 00:08:32.005
a Deep Residual Neural Network
which has a 152 layers,

196
00:08:32.005 --> 00:08:35.570
and millions more connected
neurons in parameters.

197
00:08:35.570 --> 00:08:38.300
The AWS Platform offers

198
00:08:38.300 --> 00:08:39.860
three advanced Deep Learning

199
00:08:39.860 --> 00:08:42.275
enabled managed API services.

200
00:08:42.275 --> 00:08:46.510
Amazon Lex, Amazon Polly,
and Amazon Rekognition.

201
00:08:46.510 --> 00:08:48.950
Amazon Lex is a
service for building

202
00:08:48.950 --> 00:08:52.205
conversational interfaces
into any application

203
00:08:52.205 --> 00:08:53.885
using voice and text.

204
00:08:53.885 --> 00:08:57.125
It provides that advanced
Deep Learning functionalities

205
00:08:57.125 --> 00:08:59.050
of automatic speech recognition,

206
00:08:59.050 --> 00:09:01.790
for converting
speech-to-text, and natural

207
00:09:01.790 --> 00:09:03.680
language understanding
to recognize

208
00:09:03.680 --> 00:09:04.990
the intent of the input.

209
00:09:04.990 --> 00:09:07.190
That enables you to
build applications with

210
00:09:07.190 --> 00:09:09.545
highly engaging user experiences,

211
00:09:09.545 --> 00:09:12.670
and life-like conversational
interactions.

212
00:09:12.670 --> 00:09:15.675
Amazon Polly turns tags
into lifelike speech.

213
00:09:15.675 --> 00:09:18.320
Allowing you to create
applications that talk,

214
00:09:18.320 --> 00:09:20.180
and build entirely new categories

215
00:09:20.180 --> 00:09:21.845
of speech enabled products.

216
00:09:21.845 --> 00:09:24.140
Amazon Rekognition
makes it easy to

217
00:09:24.140 --> 00:09:26.570
add image analysis to
your applications,

218
00:09:26.570 --> 00:09:29.255
so that your application
can detect objects,

219
00:09:29.255 --> 00:09:31.505
scenes, and faces, and images.

220
00:09:31.505 --> 00:09:34.040
You can also search
and compare faces,

221
00:09:34.040 --> 00:09:35.555
recognize celebrities,

222
00:09:35.555 --> 00:09:37.970
and identify
inappropriate content.

223
00:09:37.970 --> 00:09:40.900
Deep Learning can often be
technically challenging.

224
00:09:40.900 --> 00:09:42.500
Requiring you to understand

225
00:09:42.500 --> 00:09:44.915
the math of the
models themselves,

226
00:09:44.915 --> 00:09:47.450
and the experience in
skating, training,

227
00:09:47.450 --> 00:09:51.020
and inference across large
distributed systems.

228
00:09:51.020 --> 00:09:53.280
As a result, several

229
00:09:53.280 --> 00:09:55.095
Deep Learning frameworks
have emerged,

230
00:09:55.095 --> 00:09:56.660
which allow you to define

231
00:09:56.660 --> 00:09:59.060
models and then
train them at scale.

232
00:09:59.060 --> 00:10:00.755
You can build custom models

233
00:10:00.755 --> 00:10:03.710
using the Amazon
deep-learning AMIs.

234
00:10:03.710 --> 00:10:07.430
Built for Amazon
Linux and Ubuntu.

235
00:10:07.430 --> 00:10:09.635
The AWS Deep Learning AMIs come

236
00:10:09.635 --> 00:10:12.950
pre-configured with
Apache MXnet TensorFlow,

237
00:10:12.950 --> 00:10:15.485
the Microsoft Cognitive
Toolkit Caffe,

238
00:10:15.485 --> 00:10:20.035
Caffe2, theano, torch,
Pytorch and Keras.

239
00:10:20.035 --> 00:10:22.790
The Deep Learning AMIs
enable you to quickly

240
00:10:22.790 --> 00:10:25.805
deploy and run any of
these frameworks at scale.

241
00:10:25.805 --> 00:10:29.095
The Deep Learning AMIs can
help you get started quickly.

242
00:10:29.095 --> 00:10:31.970
They're provisioned with many
deep learning frameworks

243
00:10:31.970 --> 00:10:35.715
including tutorials that
demonstrate proper installation,

244
00:10:35.715 --> 00:10:37.965
configuration, and
model accuracy.

245
00:10:37.965 --> 00:10:40.350
The Deep Learning AMIs
install dependencies,

246
00:10:40.350 --> 00:10:44.355
track library versions, and
validate code compatibility.

247
00:10:44.355 --> 00:10:46.755
With updates to the
AMIs every month,

248
00:10:46.755 --> 00:10:48.770
you always have the
latest versions of

249
00:10:48.770 --> 00:10:51.155
the engines in data
science libraries.

250
00:10:51.155 --> 00:10:55.110
Whether you need Amazon
EC2 GPU or CPU Instances.

251
00:10:55.110 --> 00:10:58.280
There's no additional charge
for the deep learning AMIs.

252
00:10:58.280 --> 00:11:00.620
You only pay for the
AWS resources that you

253
00:11:00.620 --> 00:11:03.545
need to store and run
your applications.

254
00:11:03.545 --> 00:11:06.050
There are two ways to get started

255
00:11:06.050 --> 00:11:07.970
with AWS Deep Learning AMIs;

256
00:11:07.970 --> 00:11:09.350
you can deploy a deep-learning

257
00:11:09.350 --> 00:11:11.590
Compute Instance in one click.

258
00:11:11.590 --> 00:11:14.150
The AWS Deep Learning AMIs can

259
00:11:14.150 --> 00:11:16.880
quickly be launched
from AWS marketplace.

260
00:11:16.880 --> 00:11:19.625
You have the choice of GPUs
for large-scale training,

261
00:11:19.625 --> 00:11:22.645
and CPUs for running
predictions or inferences.

262
00:11:22.645 --> 00:11:25.230
Both of them give you
a stable, secured,

263
00:11:25.230 --> 00:11:27.740
and high-performance
execution environment to run

264
00:11:27.740 --> 00:11:30.980
your applications with
pay-as-you-go pricing model.

265
00:11:30.980 --> 00:11:32.780
The other way to get started with

266
00:11:32.780 --> 00:11:34.430
the Deep Learning AMIs is

267
00:11:34.430 --> 00:11:38.285
to launch an AWS Cloud Formation
Deep Learning template.

268
00:11:38.285 --> 00:11:41.110
To train over multiple instances,

269
00:11:41.110 --> 00:11:43.400
you use the Deep Learning
CloudFormation template

270
00:11:43.400 --> 00:11:44.540
for a simple way to

271
00:11:44.540 --> 00:11:45.980
launch all of your resources

272
00:11:45.980 --> 00:11:48.835
quickly using the
deep learning AMIs.

273
00:11:48.835 --> 00:11:50.925
Now, let's talk about a use case.

274
00:11:50.925 --> 00:11:54.020
C-Span is non-for-profits
surveys focused on

275
00:11:54.020 --> 00:11:57.230
broadcasting and archiving
US government proceedings.

276
00:11:57.230 --> 00:11:58.550
C-span had developed

277
00:11:58.550 --> 00:12:01.180
an automated facial
recognition solution

278
00:12:01.180 --> 00:12:03.815
to help human indexers,
but it will slow.

279
00:12:03.815 --> 00:12:05.660
They could only index half of

280
00:12:05.660 --> 00:12:07.504
the incoming content by speaker

281
00:12:07.504 --> 00:12:11.960
limiting the ability of users
to search archive content.

282
00:12:11.960 --> 00:12:14.600
So what are the solutions
and benefit that they had

283
00:12:14.600 --> 00:12:18.020
using Amazon Rekognition as
a Deep Learning Service?

284
00:12:18.020 --> 00:12:20.360
They implemented
Amazon recognition

285
00:12:20.360 --> 00:12:21.740
to automatically match,

286
00:12:21.740 --> 00:12:23.525
uploaded screenshots
of a collection

287
00:12:23.525 --> 00:12:25.900
of 97,000 known faces.

288
00:12:25.900 --> 00:12:28.025
That enables C-span to

289
00:12:28.025 --> 00:12:29.720
more than double the video index

290
00:12:29.720 --> 00:12:33.110
from $3,500-7,500 per year.

291
00:12:33.110 --> 00:12:36.410
It drove down to labor
required to index an hour of

292
00:12:36.410 --> 00:12:39.590
video content from 60
minutes to 20 minutes.

293
00:12:39.590 --> 00:12:42.035
They deployed it in
less than three weeks,

294
00:12:42.035 --> 00:12:43.940
and index at 97,000

295
00:12:43.940 --> 00:12:46.655
image collection in
less than two hours.

296
00:12:46.655 --> 00:12:48.875
I hope you learned
a little something,

297
00:12:48.875 --> 00:12:51.350
and we'll continue to
explore other courses.

298
00:12:51.350 --> 00:12:55.480
I'm Dan Mbaga with AWS AI,
and thanks for watching.