WEBVTT

1
00:00:04.370 --> 00:00:08.220
In this section, we'll
explore neural networks.

2
00:00:08.220 --> 00:00:11.040
The topics in this
section are perceptron,

3
00:00:11.040 --> 00:00:13.200
neural network architecture,

4
00:00:13.200 --> 00:00:15.825
convolutional neural networks,

5
00:00:15.825 --> 00:00:17.940
and recurrent neural networks.

6
00:00:17.940 --> 00:00:19.800
Neural networks have
been a buzzword

7
00:00:19.800 --> 00:00:21.150
recently especially within

8
00:00:21.150 --> 00:00:23.100
the deep learning revolution

9
00:00:23.100 --> 00:00:25.320
that we have seen in
the past few years.

10
00:00:25.320 --> 00:00:27.720
The concept of the neural
network first emerged

11
00:00:27.720 --> 00:00:29.910
in the 1950s and began to

12
00:00:29.910 --> 00:00:34.660
show up in commercial
applications as early as 1962.

13
00:00:34.660 --> 00:00:37.520
The simplest neural
network is a perceptron.

14
00:00:37.520 --> 00:00:40.220
Perceptron is a single
layer neural network

15
00:00:40.220 --> 00:00:42.560
that uses a list
of input features.

16
00:00:42.560 --> 00:00:44.800
For example, X1 to Xn.

17
00:00:44.800 --> 00:00:46.654
X1 can be the cost,

18
00:00:46.654 --> 00:00:48.494
Xn can be the ratings,

19
00:00:48.494 --> 00:00:50.330
and these are all
feature vectors.

20
00:00:50.330 --> 00:00:53.690
In this case, the goal
might be to determine

21
00:00:53.690 --> 00:00:55.430
whether the customer
will or will not

22
00:00:55.430 --> 00:00:57.910
buy something based
on the inputs.

23
00:00:57.910 --> 00:01:00.000
In addition to the
input features,

24
00:01:00.000 --> 00:01:01.915
there is also the bias term,

25
00:01:01.915 --> 00:01:03.590
which is like an intercept

26
00:01:03.590 --> 00:01:05.480
in your linear regression models.

27
00:01:05.480 --> 00:01:07.880
Actually, all those
features are combined

28
00:01:07.880 --> 00:01:11.105
together just as in
the linear regression.

29
00:01:11.105 --> 00:01:13.850
So we have this
linear combination of

30
00:01:13.850 --> 00:01:16.045
features from the input feature,

31
00:01:16.045 --> 00:01:18.610
space, as well as the intercept.

32
00:01:18.610 --> 00:01:21.000
Once we have the
linear combination,

33
00:01:21.000 --> 00:01:23.540
then we'll apply an
activation function.

34
00:01:23.540 --> 00:01:26.925
This activation function
is usually non-linear,

35
00:01:26.925 --> 00:01:31.535
and really depends on the
problem you're trying to solve.

36
00:01:31.535 --> 00:01:33.500
In this particular case,

37
00:01:33.500 --> 00:01:35.780
our response variable
is a binary,

38
00:01:35.780 --> 00:01:40.740
one for buy, or zero for not
to buy a particular product.

39
00:01:40.740 --> 00:01:42.050
The natural way to apply

40
00:01:42.050 --> 00:01:45.620
the activation function is by
using the sigmoid function,

41
00:01:45.620 --> 00:01:47.450
we saw in the last section.

42
00:01:47.450 --> 00:01:50.120
Perceptron is really a
very simple network.

43
00:01:50.120 --> 00:01:51.860
We only have one layer,

44
00:01:51.860 --> 00:01:53.360
which is composed of the sum of

45
00:01:53.360 --> 00:01:56.120
the linear inputs and
the activation function,

46
00:01:56.120 --> 00:01:58.415
which all connect
up to the output.

47
00:01:58.415 --> 00:02:02.510
The idea of perceptrons
was introduced a long time

48
00:02:02.510 --> 00:02:03.800
ago and over time

49
00:02:03.800 --> 00:02:06.905
people have added more
layers to the perceptron.

50
00:02:06.905 --> 00:02:10.740
For example, here we
have the input layer,

51
00:02:10.750 --> 00:02:13.250
we have the original layer,

52
00:02:13.250 --> 00:02:15.620
and we also have
some hidden layers.

53
00:02:15.620 --> 00:02:17.480
Once we have multiple layers,

54
00:02:17.480 --> 00:02:18.830
it becomes a neural network.

55
00:02:18.830 --> 00:02:20.570
Neural networks usually contains

56
00:02:20.570 --> 00:02:22.910
multiple layers and
within each layer,

57
00:02:22.910 --> 00:02:25.220
there are many nodes because

58
00:02:25.220 --> 00:02:28.150
the neural network structure
is rather complicated.

59
00:02:28.150 --> 00:02:31.280
Since there are a lot of
parameters in the model,

60
00:02:31.280 --> 00:02:34.055
neural networks are usually
very difficult to interpret.

61
00:02:34.055 --> 00:02:36.665
Neural networks are
also expensive to train

62
00:02:36.665 --> 00:02:38.540
because the structure
of the neural network

63
00:02:38.540 --> 00:02:39.805
can be complicated,

64
00:02:39.805 --> 00:02:41.420
and so the number of parameters

65
00:02:41.420 --> 00:02:43.075
to be estimated is very large.

66
00:02:43.075 --> 00:02:45.770
In Scikit-learn, there are indeed

67
00:02:45.770 --> 00:02:48.830
some neural network
frameworks for us to use.

68
00:02:48.830 --> 00:02:51.020
But since the deep
learning revolution,

69
00:02:51.020 --> 00:02:54.140
people have been developing
frameworks to design, train,

70
00:02:54.140 --> 00:02:55.340
and estimate, and implement

71
00:02:55.340 --> 00:02:57.950
neural networks in a
brand new fashion.

72
00:02:57.950 --> 00:02:59.540
This has led to the creation of

73
00:02:59.540 --> 00:03:01.790
many layers and
within each layer,

74
00:03:01.790 --> 00:03:02.990
we have many nodes,

75
00:03:02.990 --> 00:03:06.470
so the input data can be much
more larger than before.

76
00:03:06.470 --> 00:03:09.335
Because this training
data in neural network

77
00:03:09.335 --> 00:03:12.100
is very large and
very high-quality,

78
00:03:12.100 --> 00:03:13.485
can be very useful.

79
00:03:13.485 --> 00:03:15.275
In the deep learning
frameworks that are

80
00:03:15.275 --> 00:03:17.705
popular today including MX net,

81
00:03:17.705 --> 00:03:20.705
TensorFlow, Cafe, and PyTorch.

82
00:03:20.705 --> 00:03:23.629
Those are developed
from different sources,

83
00:03:23.629 --> 00:03:26.230
but all of them can be
accessed from Python.

84
00:03:26.230 --> 00:03:29.340
Using Python, we can
design, train, fit,

85
00:03:29.340 --> 00:03:31.520
and implement a
neural network very

86
00:03:31.520 --> 00:03:34.265
easily for any deep
learning framework.

87
00:03:34.265 --> 00:03:37.235
One specific neural network
that's very useful for

88
00:03:37.235 --> 00:03:40.385
image analysis is called
Convolutional Neural Networks.

89
00:03:40.385 --> 00:03:43.425
In convolution neural network,

90
00:03:43.425 --> 00:03:45.230
the input is either an image or

91
00:03:45.230 --> 00:03:47.690
a sequence image that has waited.

92
00:03:47.690 --> 00:03:50.060
For an image, we're using

93
00:03:50.060 --> 00:03:52.910
kernels as filters to
extract local features.

94
00:03:52.910 --> 00:03:55.135
In the example shown here,

95
00:03:55.135 --> 00:03:57.670
we have the input
image and we're using

96
00:03:57.670 --> 00:03:58.810
filters to convolve with

97
00:03:58.810 --> 00:04:00.985
the image to create
the next layer.

98
00:04:00.985 --> 00:04:03.880
Depending on how many
filters we're using,

99
00:04:03.880 --> 00:04:05.740
will have different layers or

100
00:04:05.740 --> 00:04:07.405
different channels in the output

101
00:04:07.405 --> 00:04:08.990
from the convolutional layer,

102
00:04:08.990 --> 00:04:10.965
one in this particular case.

103
00:04:10.965 --> 00:04:12.250
Another concept in your

104
00:04:12.250 --> 00:04:14.830
convolutional neural networks
is the pooling layer.

105
00:04:14.830 --> 00:04:17.080
Once you have a
particular output,

106
00:04:17.080 --> 00:04:19.360
you may want to reduce
the size of bit.

107
00:04:19.360 --> 00:04:22.980
To do this, we can use max
pooling or average pooling.

108
00:04:22.980 --> 00:04:26.920
We will reduce to just a
single scalar by taking

109
00:04:26.920 --> 00:04:29.080
the maximum of the two by two

110
00:04:29.080 --> 00:04:31.990
or taking the average
of the two-by-two.

111
00:04:31.990 --> 00:04:33.490
The pooling layer is virtually

112
00:04:33.490 --> 00:04:35.640
a dimension reduction process.

113
00:04:35.640 --> 00:04:37.279
Based on the application

114
00:04:37.279 --> 00:04:39.320
of convolutional neural networks,

115
00:04:39.320 --> 00:04:41.315
you really have a lot of layers

116
00:04:41.315 --> 00:04:43.835
and the number of
dimensions is pretty high.

117
00:04:43.835 --> 00:04:45.680
We need to reduce the size of

118
00:04:45.680 --> 00:04:48.245
the data for better convergence.

119
00:04:48.245 --> 00:04:50.270
We can add a few
different layers for

120
00:04:50.270 --> 00:04:51.875
the convolution neural networks,

121
00:04:51.875 --> 00:04:53.660
but at the end of the day,

122
00:04:53.660 --> 00:04:56.240
we are to convert the tensor into

123
00:04:56.240 --> 00:04:59.690
a vector and make it become
a fully-connected layer.

124
00:04:59.690 --> 00:05:01.250
The fully connected layer will be

125
00:05:01.250 --> 00:05:03.020
used to link to the output.

126
00:05:03.020 --> 00:05:06.170
The output is usually a
particular category of

127
00:05:06.170 --> 00:05:09.410
the graph or the image
that is contains.

128
00:05:09.410 --> 00:05:11.240
For example, the output from

129
00:05:11.240 --> 00:05:13.550
this image could be a digit zero.

130
00:05:13.550 --> 00:05:15.350
By the training process,

131
00:05:15.350 --> 00:05:17.830
we have a lot of
good labeled data.

132
00:05:17.830 --> 00:05:20.360
By using convolutional
neural networks,

133
00:05:20.360 --> 00:05:22.100
we can try to find out
the best number of

134
00:05:22.100 --> 00:05:24.680
filters and the
variance in the filters

135
00:05:24.680 --> 00:05:27.680
that will give us a near
human-level accuracy

136
00:05:27.680 --> 00:05:29.240
for image recognition.

137
00:05:29.240 --> 00:05:30.690
In this particular case,

138
00:05:30.690 --> 00:05:32.860
for handwritten
digit recognition,

139
00:05:32.860 --> 00:05:36.980
we can achieve a neural near
human level of accuracy.

140
00:05:36.980 --> 00:05:38.840
Another type of neural network is

141
00:05:38.840 --> 00:05:40.645
called Recurrent Neural Network.

142
00:05:40.645 --> 00:05:41.770
For the feed-forward,

143
00:05:41.770 --> 00:05:44.270
neural network and the
convolutional neural network,

144
00:05:44.270 --> 00:05:47.105
the input data is
relatively independent.

145
00:05:47.105 --> 00:05:48.995
The neural network cannot model

146
00:05:48.995 --> 00:05:50.630
the dependent structure among

147
00:05:50.630 --> 00:05:52.400
different input observations.

148
00:05:52.400 --> 00:05:55.095
But often for time, sharing data,

149
00:05:55.095 --> 00:05:56.690
or any other natural language

150
00:05:56.690 --> 00:05:58.950
processing or translation
applications,

151
00:05:58.950 --> 00:06:02.030
the sequence of input data
really means something.

152
00:06:02.030 --> 00:06:04.280
When the data involves
sequential features

153
00:06:04.280 --> 00:06:06.080
or time sharing features,

154
00:06:06.080 --> 00:06:08.855
the recurrent neural network
is the right way to go.

155
00:06:08.855 --> 00:06:12.530
For example, in this high-level
conceptual illustration

156
00:06:12.530 --> 00:06:15.320
of a recurrent neural network,

157
00:06:15.320 --> 00:06:17.060
we have the input layer,

158
00:06:17.060 --> 00:06:21.040
and output layer, and all
of the hidden layers.

159
00:06:21.040 --> 00:06:24.530
Within the input to this
recurrent neural network,

160
00:06:24.530 --> 00:06:26.330
there are a set of characters,

161
00:06:26.330 --> 00:06:29.240
but they do have
meanings as a sequence.

162
00:06:29.240 --> 00:06:31.310
So each individual word,

163
00:06:31.310 --> 00:06:34.130
each individual character
doesn't mean much until

164
00:06:34.130 --> 00:06:37.365
we have a sequential
relationship among them.

165
00:06:37.365 --> 00:06:39.230
During the training process,

166
00:06:39.230 --> 00:06:41.640
information flow is not
just in one direction.

167
00:06:41.640 --> 00:06:44.270
The information flow
is actually reused in

168
00:06:44.270 --> 00:06:46.220
propagating through
different nodes

169
00:06:46.220 --> 00:06:48.230
at different sequences.

170
00:06:48.230 --> 00:06:49.680
In the final result,

171
00:06:49.680 --> 00:06:51.755
the input layer and
output layer are actually

172
00:06:51.755 --> 00:06:54.890
connected with this
recurrent neural network.

173
00:06:54.890 --> 00:06:58.290
I'm [inaudible] and
thank you for watching.