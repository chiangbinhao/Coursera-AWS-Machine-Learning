WEBVTT

1
00:00:06.020 --> 00:00:12.060
Welcome. Welcome to an
introduction to machine learning.

2
00:00:12.060 --> 00:00:13.620
I'm Blaine Sundrud.

3
00:00:13.620 --> 00:00:16.260
I'm a Senior Instructional
Designer and a Technical

4
00:00:16.260 --> 00:00:19.545
Trainer at Amazon
Web Services, AWS.

5
00:00:19.545 --> 00:00:22.110
I am thrilled to
welcome you to one of

6
00:00:22.110 --> 00:00:26.505
the first sessions of
today's AI Innovate event.

7
00:00:26.505 --> 00:00:28.375
Over the next hour,

8
00:00:28.375 --> 00:00:29.540
I'm going to help you lay

9
00:00:29.540 --> 00:00:31.490
the framework that you're
going to need to start

10
00:00:31.490 --> 00:00:35.240
developing machine learning
solutions in your own work.

11
00:00:35.240 --> 00:00:37.130
I'll define some key terms.

12
00:00:37.130 --> 00:00:38.750
We're going to talk about
the different types of

13
00:00:38.750 --> 00:00:40.820
machine learning
algorithms that'll help

14
00:00:40.820 --> 00:00:42.680
you solve business problems that

15
00:00:42.680 --> 00:00:45.095
you're going to face
out in the real world.

16
00:00:45.095 --> 00:00:46.670
Then we're going to walk through

17
00:00:46.670 --> 00:00:48.875
the machine-learning pipeline.

18
00:00:48.875 --> 00:00:52.595
Let's start this 60 minute
journey off with a story.

19
00:00:52.595 --> 00:00:56.555
This is a use case of a
machine learning in action,

20
00:00:56.555 --> 00:00:59.150
one that took place
right here at Amazon.

21
00:00:59.150 --> 00:01:00.860
Let me head over
to the light board

22
00:01:00.860 --> 00:01:02.210
to illustrate the story for you.

23
00:01:02.210 --> 00:01:08.760
Hold on a second. All right.

24
00:01:08.760 --> 00:01:13.280
Several years ago,
Amazon, @amazon.com,

25
00:01:13.280 --> 00:01:15.595
needed to improve the way

26
00:01:15.595 --> 00:01:18.130
that it routed customer
service calls.

27
00:01:18.130 --> 00:01:20.575
So it looked to machine
learning for help.

28
00:01:20.575 --> 00:01:22.720
Now the original routing system

29
00:01:22.720 --> 00:01:23.980
works something like this,

30
00:01:23.980 --> 00:01:26.320
customer calls in,
greeted by the menu,

31
00:01:26.320 --> 00:01:27.805
"Press 1 for return.

32
00:01:27.805 --> 00:01:29.604
Press 2 for Kindle,

33
00:01:29.604 --> 00:01:32.110
3," but whatever.
You get the idea.

34
00:01:32.110 --> 00:01:34.795
The customer has to
make a selection,

35
00:01:34.795 --> 00:01:37.825
and then they're
sent to an agent.

36
00:01:37.825 --> 00:01:41.350
Agent. The agent is
there to go ahead,

37
00:01:41.350 --> 00:01:42.820
and they're trained
in the right skills

38
00:01:42.820 --> 00:01:44.275
to help them with the customer.

39
00:01:44.275 --> 00:01:47.085
Well the problem is,

40
00:01:47.085 --> 00:01:48.480
hey, you might have guessed,

41
00:01:48.480 --> 00:01:51.880
the kind of things that we
do and sell here at Amazon,

42
00:01:51.880 --> 00:01:53.705
there's a lot of stuff.

43
00:01:53.705 --> 00:01:56.090
So the list of things
that customer could be

44
00:01:56.090 --> 00:01:59.630
calling from is
practically endless.

45
00:01:59.630 --> 00:02:01.310
So if we didn't get

46
00:02:01.310 --> 00:02:03.680
the right option for the
customer to call in,

47
00:02:03.680 --> 00:02:05.615
then the customer is sent to

48
00:02:05.615 --> 00:02:08.495
a generalist as opposed
to a specialist.

49
00:02:08.495 --> 00:02:11.140
The generalist has to
figure out what they want.

50
00:02:11.140 --> 00:02:12.545
So they send him to

51
00:02:12.545 --> 00:02:15.610
another person who
hopefully is the right one.

52
00:02:15.610 --> 00:02:18.190
Maybe it is. Maybe it's got
the right skills. Maybe not.

53
00:02:18.190 --> 00:02:20.520
It's got to sent to
another one, and so on.

54
00:02:20.520 --> 00:02:22.400
You keep going through
all these pieces until

55
00:02:22.400 --> 00:02:25.130
eventually you get to

56
00:02:25.130 --> 00:02:26.810
the real happy person

57
00:02:26.810 --> 00:02:28.760
that's supposed to
help out the customer.

58
00:02:28.760 --> 00:02:31.430
Now for some businesses,

59
00:02:31.430 --> 00:02:33.500
that might not be the
end of the world.

60
00:02:33.500 --> 00:02:35.810
For Amazon, when
you're dealing with

61
00:02:35.810 --> 00:02:39.440
hundreds of millions of
customer calls every year,

62
00:02:39.440 --> 00:02:42.415
this path, that's inefficient.

63
00:02:42.415 --> 00:02:46.215
It cost a lot of
money, wasted time,

64
00:02:46.215 --> 00:02:48.350
and worst of all
it's not a good way

65
00:02:48.350 --> 00:02:50.990
to get our customers
the help they needed.

66
00:02:50.990 --> 00:02:54.035
Well you can probably guess
the rest of the story.

67
00:02:54.035 --> 00:02:56.330
Amazon built a system.

68
00:02:56.330 --> 00:02:58.160
They used machine learning to

69
00:02:58.160 --> 00:03:00.920
improve the whole routing system.

70
00:03:00.920 --> 00:03:04.100
So the idea was to
get rid of all of

71
00:03:04.100 --> 00:03:05.300
this extra stuff and go

72
00:03:05.300 --> 00:03:08.855
straight to the agent
that could help them.

73
00:03:08.855 --> 00:03:11.000
This made the customers happier.

74
00:03:11.000 --> 00:03:13.070
It made the call center
agents more productive.

75
00:03:13.070 --> 00:03:15.800
Basically, everyone lives
happily ever after.

76
00:03:15.800 --> 00:03:18.380
So I'm going to show you

77
00:03:18.380 --> 00:03:21.125
how Amazon actually
did it, how we did it.

78
00:03:21.125 --> 00:03:22.940
We're going to spend
the next 60 minutes

79
00:03:22.940 --> 00:03:25.550
walking through this
machine learning pipeline,

80
00:03:25.550 --> 00:03:29.320
and explain how we
deployed this smarter,

81
00:03:29.320 --> 00:03:32.355
more intelligent, customer
service routing system,

82
00:03:32.355 --> 00:03:35.390
this ML system, so that you can

83
00:03:35.390 --> 00:03:39.290
develop your own ML
solutions moving forward.

84
00:03:39.290 --> 00:03:41.180
So first, what does

85
00:03:41.180 --> 00:03:43.885
the machine learning
pipeline look like anyway?

86
00:03:43.885 --> 00:03:48.005
Well it starts with collecting
and integrating your data.

87
00:03:48.005 --> 00:03:50.090
Then you prepare the data,

88
00:03:50.090 --> 00:03:52.460
visualize it for analysis,

89
00:03:52.460 --> 00:03:54.410
then you select the
features you want

90
00:03:54.410 --> 00:03:57.255
to use and engineer some as well.

91
00:03:57.255 --> 00:03:59.505
Then you can train your model,

92
00:03:59.505 --> 00:04:02.535
evaluate it and deploy it.

93
00:04:02.535 --> 00:04:05.785
So at this point now,

94
00:04:05.785 --> 00:04:07.010
it's time to turn

95
00:04:07.010 --> 00:04:10.220
our business problems into
machine learning problems.

96
00:04:10.220 --> 00:04:13.145
So let's start with
the business problem.

97
00:04:13.145 --> 00:04:19.430
So our business
problem in this case

98
00:04:19.430 --> 00:04:22.860
is how are we going

99
00:04:22.860 --> 00:04:26.850
to route our customer
calls successfully?

100
00:04:26.850 --> 00:04:29.920
The machine learning problem,

101
00:04:32.360 --> 00:04:35.430
we'll get to that in a second.

102
00:04:35.430 --> 00:04:37.610
In fact, before you can do

103
00:04:37.610 --> 00:04:39.370
any of the things we're
going to talk about,

104
00:04:39.370 --> 00:04:42.280
we have to decide even
if a machine-learning,

105
00:04:42.280 --> 00:04:46.235
ML, is the right solution to
deploy in the first place.

106
00:04:46.235 --> 00:04:49.590
All right. So at this point,

107
00:04:49.590 --> 00:04:52.875
is machine learning an
appropriate solution?

108
00:04:52.875 --> 00:04:55.120
So let's break it down.

109
00:04:55.120 --> 00:04:57.829
Machine learning is a subset

110
00:04:57.829 --> 00:05:00.100
of artificial intelligence or AI.

111
00:05:00.100 --> 00:05:03.215
Machine learning uses data,

112
00:05:03.215 --> 00:05:08.580
and this data is going to
be used to train the model,

113
00:05:08.770 --> 00:05:13.295
and the model is then
used for predictions.

114
00:05:13.295 --> 00:05:16.285
Can I spell predictions?

115
00:05:16.285 --> 00:05:18.660
Close enough. Good enough.

116
00:05:18.660 --> 00:05:20.580
The predictions then, we

117
00:05:20.580 --> 00:05:23.440
can make those from
huge datasets,

118
00:05:23.440 --> 00:05:25.370
because the strength lies in

119
00:05:25.370 --> 00:05:27.770
its ability to extract
hidden patterns,

120
00:05:27.770 --> 00:05:29.975
structures from this data.

121
00:05:29.975 --> 00:05:32.905
Now common use case
for machine learning.

122
00:05:32.905 --> 00:05:35.420
Well let's call it a
credit card transactions.

123
00:05:35.420 --> 00:05:38.400
So card transactions.

124
00:05:39.020 --> 00:05:41.850
So there's card transactions.

125
00:05:41.850 --> 00:05:43.920
In this case, we

126
00:05:43.920 --> 00:05:46.100
find the appropriate
data because what we're

127
00:05:46.100 --> 00:05:51.415
looking for is to
determine fraud.

128
00:05:51.415 --> 00:05:53.715
The appropriate data is mind.

129
00:05:53.715 --> 00:05:55.430
We're identifying patterns among

130
00:05:55.430 --> 00:05:57.245
all of the card transactions,

131
00:05:57.245 --> 00:05:59.030
specifically looking
for patterns that

132
00:05:59.030 --> 00:06:01.460
indicate a fraudulent
transaction.

133
00:06:01.460 --> 00:06:04.010
With these patterns,
you can train

134
00:06:04.010 --> 00:06:07.805
the ML model to predict
future transactions as,

135
00:06:07.805 --> 00:06:11.540
yes, fraudulent, or
no, not fraudulent.

136
00:06:11.540 --> 00:06:13.490
So with that in mind,

137
00:06:13.490 --> 00:06:15.325
let's return to the question.

138
00:06:15.325 --> 00:06:18.920
Is machine learning an
appropriate solution

139
00:06:18.920 --> 00:06:20.615
for the business problem?

140
00:06:20.615 --> 00:06:24.290
Well in this case, it was
for the Amazon call center.

141
00:06:24.290 --> 00:06:25.910
They had millions of

142
00:06:25.910 --> 00:06:29.050
historical phone
calls as the dataset,

143
00:06:29.050 --> 00:06:32.840
but there was no single
indicator that they could use to

144
00:06:32.840 --> 00:06:36.875
get a customer directly to
an agent in just one step.

145
00:06:36.875 --> 00:06:38.900
It's more complicated than that.

146
00:06:38.900 --> 00:06:41.240
So we needed to identify patterns

147
00:06:41.240 --> 00:06:43.490
within the whole range of

148
00:06:43.490 --> 00:06:45.500
customer data that could help us

149
00:06:45.500 --> 00:06:48.680
route customer to the right
agents in a single-step.

150
00:06:48.680 --> 00:06:51.150
Tons of data, tons of data,

151
00:06:51.150 --> 00:06:53.030
all of which needed
to be analyzed for

152
00:06:53.030 --> 00:06:54.950
patterns that Amazon could

153
00:06:54.950 --> 00:06:57.185
use to make accurate predictions.

154
00:06:57.185 --> 00:06:59.270
Knock-knock, who's there?

155
00:06:59.270 --> 00:07:02.180
Machine learning. This is exactly

156
00:07:02.180 --> 00:07:05.320
the problem that machine
learning was built for.

157
00:07:05.320 --> 00:07:07.350
So in this case,

158
00:07:07.350 --> 00:07:10.880
the machine learning problem
that it's actually solving

159
00:07:10.880 --> 00:07:16.050
for is to predict
the agent skills.

160
00:07:16.580 --> 00:07:20.135
Now there are different types

161
00:07:20.135 --> 00:07:22.520
of machine learning
problems out there.

162
00:07:22.520 --> 00:07:24.450
So hypothetically
speaking, let's say

163
00:07:24.450 --> 00:07:26.355
that with our call
center problem,

164
00:07:26.355 --> 00:07:28.160
our original goal was just to

165
00:07:28.160 --> 00:07:29.900
predict whether a
customer was simply

166
00:07:29.900 --> 00:07:31.850
calling in about their Kindle or

167
00:07:31.850 --> 00:07:34.235
not calling in about kindle.

168
00:07:34.235 --> 00:07:36.170
This type of problem is

169
00:07:36.170 --> 00:07:39.110
considered a binary
classification.

170
00:07:39.110 --> 00:07:41.960
So it's binary. Let
me put it over here.

171
00:07:41.960 --> 00:07:49.320
So binary, Kindle or not Kindle.

172
00:07:49.320 --> 00:07:51.725
Simple as that. There's
only two outcomes.

173
00:07:51.725 --> 00:07:55.025
It's a classification problem
because we're predicting

174
00:07:55.025 --> 00:07:59.240
a category instead of a
real number like a price.

175
00:07:59.240 --> 00:08:00.875
Although it's simple,

176
00:08:00.875 --> 00:08:03.005
this basic classification task

177
00:08:03.005 --> 00:08:05.675
supports a wide
variety of elegant,

178
00:08:05.675 --> 00:08:09.290
scalable and actually very
powerful business solutions.

179
00:08:09.290 --> 00:08:14.835
For example, is this credit
card fraudulent or not?

180
00:08:14.835 --> 00:08:17.270
The other type of
problem you might

181
00:08:17.270 --> 00:08:20.100
run into is multi-class.

182
00:08:20.840 --> 00:08:24.450
A multi-class solution
in this case,

183
00:08:24.450 --> 00:08:26.725
we're still predicting
a category,

184
00:08:26.725 --> 00:08:28.700
but it's more than
just two outcomes.

185
00:08:28.700 --> 00:08:30.250
It's not Kindle or not Kindle.

186
00:08:30.250 --> 00:08:33.770
We might be looking at a
number of different choices.

187
00:08:33.770 --> 00:08:35.090
We're going to walk you through,

188
00:08:35.090 --> 00:08:39.095
today's example is actually
a multi-class solution.

189
00:08:39.095 --> 00:08:41.750
There's many ways of classifying
the type of skill that

190
00:08:41.750 --> 00:08:44.645
would be needed to solve this
particular customer call.

191
00:08:44.645 --> 00:08:47.734
Maybe they're looking
for a Kindle,

192
00:08:47.734 --> 00:08:50.569
but maybe it's to
return a product,

193
00:08:50.569 --> 00:08:54.350
maybe it's to answer a
question about Alexa,

194
00:08:54.350 --> 00:08:55.830
or whatever it might be.

195
00:08:55.830 --> 00:08:57.685
There could be any number,

196
00:08:57.685 --> 00:09:00.480
hundreds of different
things it could be.

197
00:09:00.480 --> 00:09:02.370
In these two examples,

198
00:09:02.370 --> 00:09:04.575
binary or multi-class solution,

199
00:09:04.575 --> 00:09:07.475
these are classification
problems.

200
00:09:07.475 --> 00:09:10.500
But there's also regression.

201
00:09:11.910 --> 00:09:14.365
In a regression problem,

202
00:09:14.365 --> 00:09:18.445
I'm no longer mapping to a
series of defined categories.

203
00:09:18.445 --> 00:09:22.375
Now, I'm looking for a continuous
values such as numbers,

204
00:09:22.375 --> 00:09:26.800
integers, 1, 2, 3, whatever.

205
00:09:26.800 --> 00:09:30.235
An example of machine
learning regression problem,

206
00:09:30.235 --> 00:09:32.935
predicting the price of
your company's stock.

207
00:09:32.935 --> 00:09:35.755
Let's get back to our
call center problem.

208
00:09:35.755 --> 00:09:39.055
We have determined our
machine learning problem.

209
00:09:39.055 --> 00:09:41.575
This is what we're looking
for. You've identified it,

210
00:09:41.575 --> 00:09:44.065
it's a multiclass problem.

211
00:09:44.065 --> 00:09:47.725
So we have a whole different
set of outputs it could be.

212
00:09:47.725 --> 00:09:51.220
At this stage, now it's
time for us to talk to

213
00:09:51.220 --> 00:09:54.595
our domain experts and
gather more information.

214
00:09:54.595 --> 00:09:57.160
Time to challenge
your assumptions.

215
00:09:57.160 --> 00:09:59.140
During this phase, some of

216
00:09:59.140 --> 00:10:01.525
the questions Amazon asked were,

217
00:10:01.525 --> 00:10:03.520
what exactly did these customer

218
00:10:03.520 --> 00:10:06.985
service agents skills represent?

219
00:10:06.985 --> 00:10:10.105
How much overlap were
there between the skills?

220
00:10:10.105 --> 00:10:11.980
Are they similar enough
where I might be able

221
00:10:11.980 --> 00:10:14.155
to possibly combine them?

222
00:10:14.155 --> 00:10:15.940
What happens when a customer's

223
00:10:15.940 --> 00:10:17.770
routed to an agent
with the wrong skill?

224
00:10:17.770 --> 00:10:19.330
Did that agent stand a chance of

225
00:10:19.330 --> 00:10:21.655
possibly answering
the question anyway?

226
00:10:21.655 --> 00:10:25.765
The more questions you ask
during this discovery stage,

227
00:10:25.765 --> 00:10:27.160
the more inputs you'll

228
00:10:27.160 --> 00:10:30.895
get and the more input that
the domain experts give you,

229
00:10:30.895 --> 00:10:32.170
the different people, then the

230
00:10:32.170 --> 00:10:34.880
better your model is going to be.

231
00:10:35.190 --> 00:10:38.680
All right. Let's go
back to the desk,

232
00:10:38.680 --> 00:10:44.800
put a few more things
on this. All right.

233
00:10:44.800 --> 00:10:47.740
Now, it's time to get started
with your ML pipeline.

234
00:10:47.740 --> 00:10:50.530
Now, it's all about the data and

235
00:10:50.530 --> 00:10:52.000
the training that data to

236
00:10:52.000 --> 00:10:54.640
enable the models to
make your predictions.

237
00:10:54.640 --> 00:10:58.885
Data is everywhere and
because it is everywhere,

238
00:10:58.885 --> 00:11:02.140
it can be collected from
multiple sources like Internet,

239
00:11:02.140 --> 00:11:04.495
Databases, other
types of Storage.

240
00:11:04.495 --> 00:11:06.250
Chances are very good.

241
00:11:06.250 --> 00:11:08.980
Some of the data your
team collects however,

242
00:11:08.980 --> 00:11:10.825
it's going to be noisy.

243
00:11:10.825 --> 00:11:14.590
Your data is possibly
incomplete even irrelevant.

244
00:11:14.590 --> 00:11:17.080
So wherever it comes from it will

245
00:11:17.080 --> 00:11:20.290
need to be compiled,
get integrated.

246
00:11:20.290 --> 00:11:23.500
Most importantly, you
have to clean the data.

247
00:11:23.500 --> 00:11:26.395
First, you need to collect and

248
00:11:26.395 --> 00:11:29.650
integrate the data that's
relevant to your problem.

249
00:11:29.650 --> 00:11:33.010
No matter what type of
data you're collecting,

250
00:11:33.010 --> 00:11:34.780
you're going to need to
make sure that you've

251
00:11:34.780 --> 00:11:36.610
got the proper tools and

252
00:11:36.610 --> 00:11:41.260
the knowledge to work with
all different datatypes.

253
00:11:41.260 --> 00:11:44.170
But let's go back to our
call center use case.

254
00:11:44.170 --> 00:11:46.810
The data we needed
came from answering

255
00:11:46.810 --> 00:11:50.950
questions like what were the
customer's recent orders?

256
00:11:50.950 --> 00:11:52.930
Does the customer own a Kindle?

257
00:11:52.930 --> 00:11:54.640
Are they a prime member?

258
00:11:54.640 --> 00:11:57.970
The historical customer
data that answers

259
00:11:57.970 --> 00:12:02.185
questions like these
are called Features.

260
00:12:02.185 --> 00:12:06.205
They could features as your
inputs to the problem.

261
00:12:06.205 --> 00:12:08.110
The machine learning model's job

262
00:12:08.110 --> 00:12:09.970
during training is
to learn which of

263
00:12:09.970 --> 00:12:11.530
these features are actually

264
00:12:11.530 --> 00:12:15.160
important to make the right
prediction for the future.

265
00:12:15.160 --> 00:12:18.430
If the value you're
looking for is know,

266
00:12:18.430 --> 00:12:20.140
like in a supervised learning,

267
00:12:20.140 --> 00:12:22.330
then that prediction
is called a label.

268
00:12:22.330 --> 00:12:24.175
But if the value isn't known,

269
00:12:24.175 --> 00:12:25.960
like in unsupervised learning,

270
00:12:25.960 --> 00:12:28.135
then it's called a target.

271
00:12:28.135 --> 00:12:30.235
We'll talk more
about supervised and

272
00:12:30.235 --> 00:12:32.440
unsupervised learning in a bit.

273
00:12:32.440 --> 00:12:35.005
Don't worry about
that. For right now,

274
00:12:35.005 --> 00:12:37.675
just know that in our
call center example,

275
00:12:37.675 --> 00:12:40.210
our label was the skill

276
00:12:40.210 --> 00:12:43.615
an agent needed to resolve
the customer call.

277
00:12:43.615 --> 00:12:47.980
All right. Together, the
label and the features,

278
00:12:47.980 --> 00:12:50.665
this makes up a
single data point.

279
00:12:50.665 --> 00:12:53.410
This is called an observation.

280
00:12:53.410 --> 00:12:57.805
Stack up a bunch of observations,
that's your dataset.

281
00:12:57.805 --> 00:13:00.625
Good data will contain

282
00:13:00.625 --> 00:13:03.700
a signal about the phenomenon
you're trying to model.

283
00:13:03.700 --> 00:13:07.210
For instance, let's say

284
00:13:07.210 --> 00:13:11.125
there's merchant trying to
forecast demand for products.

285
00:13:11.125 --> 00:13:15.400
They might track number of
sales they've had, good start.

286
00:13:15.400 --> 00:13:17.350
But what if they've forgotten to

287
00:13:17.350 --> 00:13:19.975
log when certain products
we're out of stock?

288
00:13:19.975 --> 00:13:22.060
If you're trying to
forecast demand,

289
00:13:22.060 --> 00:13:24.505
it's important to know when
you were out of stock,

290
00:13:24.505 --> 00:13:26.800
and therefore, critical
to have data that

291
00:13:26.800 --> 00:13:30.190
represents that as
one of your features.

292
00:13:30.190 --> 00:13:32.335
Here is a general rule of thumb.

293
00:13:32.335 --> 00:13:36.300
You need at least
10 times the number

294
00:13:36.300 --> 00:13:39.150
of data points as features.

295
00:13:39.150 --> 00:13:41.370
So if you've got five features,

296
00:13:41.370 --> 00:13:43.170
you should have 50 data points

297
00:13:43.170 --> 00:13:45.900
minimum in your training data.

298
00:13:45.900 --> 00:13:49.735
So data preparation,
as you can see,

299
00:13:49.735 --> 00:13:51.730
sometimes that very first dataset

300
00:13:51.730 --> 00:13:54.175
is not going to be enough
for a good prediction.

301
00:13:54.175 --> 00:13:57.340
As developers, it's
important to understand

302
00:13:57.340 --> 00:14:00.550
what data you're missing
so that you can access it.

303
00:14:00.550 --> 00:14:04.840
This is where the data
preparation phase comes in.

304
00:14:04.840 --> 00:14:09.040
First step, take a
small random sample

305
00:14:09.040 --> 00:14:11.830
of your data and you really
need to dig into it.

306
00:14:11.830 --> 00:14:15.340
Now, you probably need between
20 to 50 observations.

307
00:14:15.340 --> 00:14:18.235
Although again, that depends
how many features you have.

308
00:14:18.235 --> 00:14:21.370
Your job in the data
prep phase is to

309
00:14:21.370 --> 00:14:25.225
manually and critically
explore the data.

310
00:14:25.225 --> 00:14:26.395
You've got to look at it close.

311
00:14:26.395 --> 00:14:28.420
Ask yourself questions like this,

312
00:14:28.420 --> 00:14:30.100
what features are there?

313
00:14:30.100 --> 00:14:34.105
That Step 1. Does it
match your expectations?

314
00:14:34.105 --> 00:14:37.285
Is there enough information
to make accurate predictions?

315
00:14:37.285 --> 00:14:39.490
If you just looked at it,
what are you going to see?

316
00:14:39.490 --> 00:14:41.830
Here is a good rule of thumb.

317
00:14:41.830 --> 00:14:44.620
If a human, you could look at

318
00:14:44.620 --> 00:14:47.965
a given data point and
guess the correct label,

319
00:14:47.965 --> 00:14:52.600
then an ML algorithm should
be successful there too.

320
00:14:52.600 --> 00:14:54.550
Now, you might also want to

321
00:14:54.550 --> 00:14:57.025
critically think
about your labels.

322
00:14:57.025 --> 00:15:00.925
Ask yourself, are there
any labels that you

323
00:15:00.925 --> 00:15:02.470
want to exclude from

324
00:15:02.470 --> 00:15:04.825
the business model
for business reasons?

325
00:15:04.825 --> 00:15:08.635
Are there any labels that
aren't entirely accurate?

326
00:15:08.635 --> 00:15:10.675
In the call center use case,

327
00:15:10.675 --> 00:15:14.020
we asked some domain
experts key questions that

328
00:15:14.020 --> 00:15:17.725
helped inform this part
of Amazon's Analysis.

329
00:15:17.725 --> 00:15:19.840
For instance, we would ask,

330
00:15:19.840 --> 00:15:23.635
how much overlap was
there between skills?

331
00:15:23.635 --> 00:15:27.700
Were any skill similar
enough to be combined?

332
00:15:27.700 --> 00:15:29.770
If we did our homework and

333
00:15:29.770 --> 00:15:32.274
properly answered those
types of questions,

334
00:15:32.274 --> 00:15:34.990
we may have been able
to simplify our model

335
00:15:34.990 --> 00:15:37.705
by excluding a few labels.

336
00:15:37.705 --> 00:15:39.970
For instance, instead
of having labels that

337
00:15:39.970 --> 00:15:42.790
represent multiple Kindle skills,

338
00:15:42.790 --> 00:15:45.220
it might have made sense
to just combine those into

339
00:15:45.220 --> 00:15:47.995
one overarching
Kindle skill label.

340
00:15:47.995 --> 00:15:49.660
That way, every customer that

341
00:15:49.660 --> 00:15:51.250
had a problem with a Kindle can

342
00:15:51.250 --> 00:15:54.520
be routed to an agent trained
in all kindle issues,

343
00:15:54.520 --> 00:15:56.605
rather than tinker toy,

344
00:15:56.605 --> 00:15:58.165
little tiny things here or there.

345
00:15:58.165 --> 00:16:00.850
It can be hard to understand

346
00:16:00.850 --> 00:16:03.550
your data without
seeing the data.

347
00:16:03.550 --> 00:16:04.960
That's why you need
to do more than

348
00:16:04.960 --> 00:16:06.354
just a manual analysis,

349
00:16:06.354 --> 00:16:08.395
you need a programmatic analysis.

350
00:16:08.395 --> 00:16:11.155
This is what you get when
you visualize the data.

351
00:16:11.155 --> 00:16:12.625
I love visualization.

352
00:16:12.625 --> 00:16:14.215
Visualization is great.

353
00:16:14.215 --> 00:16:15.700
It's a technique that helps you

354
00:16:15.700 --> 00:16:19.555
understand the relationships
within your dataset.

355
00:16:19.555 --> 00:16:24.025
This leads to better
features, better models.

356
00:16:24.025 --> 00:16:28.360
When you can see the data
in a chart or plotted out,

357
00:16:28.360 --> 00:16:32.455
you can help unveil
previously unseen patterns.

358
00:16:32.455 --> 00:16:36.820
It reveals corrupt data or
outliers that you don't want,

359
00:16:36.820 --> 00:16:38.080
properties that could be very

360
00:16:38.080 --> 00:16:41.230
significant in your analysis.

361
00:16:41.230 --> 00:16:43.810
The Amazon example.

362
00:16:43.810 --> 00:16:47.470
A programmatic analysis
of the label might have

363
00:16:47.470 --> 00:16:50.980
shown 50 percent of the calls
were related to returns,

364
00:16:50.980 --> 00:16:53.935
40 percent were for
prime membership,

365
00:16:53.935 --> 00:16:57.730
30 percent related to
kindle, and so on.

366
00:16:57.730 --> 00:17:01.780
Basic stats like these can
be powerful methods to

367
00:17:01.780 --> 00:17:04.330
obtain quick feature and

368
00:17:04.330 --> 00:17:07.450
labeled summaries
to understand them.

369
00:17:07.450 --> 00:17:09.940
Two other common visualization

370
00:17:09.940 --> 00:17:11.365
techniques we're going to cover,

371
00:17:11.365 --> 00:17:16.160
histograms and scatter plots.
Let's take a look at that.

372
00:17:20.120 --> 00:17:24.495
All right. Let's talk
about histograms.

373
00:17:24.495 --> 00:17:27.450
By the way, thanks Tom for
pre-drawing one for me.

374
00:17:27.450 --> 00:17:30.750
Histograms are effective
visualizations

375
00:17:30.750 --> 00:17:33.045
for spotting outliers in data.

376
00:17:33.045 --> 00:17:35.835
For example, let's say
you're visualizing

377
00:17:35.835 --> 00:17:37.830
the distribution
of hours per week

378
00:17:37.830 --> 00:17:40.300
your company's employees
actually work.

379
00:17:40.300 --> 00:17:42.380
So you're trying to
make a prediction about

380
00:17:42.380 --> 00:17:44.300
salaries and you're going to base

381
00:17:44.300 --> 00:17:45.830
that on the number of hours

382
00:17:45.830 --> 00:17:48.505
your full-time employees
actually show up to work.

383
00:17:48.505 --> 00:17:50.310
So with this histogram,

384
00:17:50.310 --> 00:17:51.750
you can see that the majority

385
00:17:51.750 --> 00:17:53.010
of your employees are working

386
00:17:53.010 --> 00:17:56.085
between 35 and 55 hours a week.

387
00:17:56.085 --> 00:18:00.600
But you can also see there's
a lower outlier over here.

388
00:18:00.600 --> 00:18:05.460
Couple of your employees are
working 15-20 hours a week.

389
00:18:05.460 --> 00:18:09.990
Well, maybe you have some
part-time employees that for

390
00:18:09.990 --> 00:18:11.865
whatever reason got mixed

391
00:18:11.865 --> 00:18:14.310
into your dataset of your
full-time employees.

392
00:18:14.310 --> 00:18:16.080
If you want it base

393
00:18:16.080 --> 00:18:18.735
your prediction on
full-time employees only,

394
00:18:18.735 --> 00:18:21.030
then it's important
to identify and

395
00:18:21.030 --> 00:18:24.315
remove these part-time
employees from your dataset.

396
00:18:24.315 --> 00:18:26.490
Well, in this case,
you could just delete

397
00:18:26.490 --> 00:18:30.045
the outlier data or
you could cap it.,

398
00:18:30.045 --> 00:18:32.100
so you don't see any data
for any employee who

399
00:18:32.100 --> 00:18:34.815
worked less than 35
hours this week.

400
00:18:34.815 --> 00:18:37.830
This solution would help
you ensure you're only

401
00:18:37.830 --> 00:18:40.965
looking at that
full-time employee set.

402
00:18:40.965 --> 00:18:44.400
But there's other solutions.

403
00:18:44.400 --> 00:18:48.420
For instance, in the multi-class
classification problem,

404
00:18:48.420 --> 00:18:49.860
you're going to want
to figure out how

405
00:18:49.860 --> 00:18:51.750
to actually combine this

406
00:18:51.750 --> 00:18:53.790
outlier data with
other data classes

407
00:18:53.790 --> 00:18:56.430
rather than just ignoring
it or deleting it.

408
00:18:56.430 --> 00:18:59.670
For example, in the
call center example,

409
00:18:59.670 --> 00:19:02.835
we had multiple Kindle skills.

410
00:19:02.835 --> 00:19:07.260
Well, ultimately, Amazon
just decided to combine

411
00:19:07.260 --> 00:19:09.540
specific Kindle skills into

412
00:19:09.540 --> 00:19:13.185
a single general Kindle skills
for its model training.

413
00:19:13.185 --> 00:19:15.750
If it's a regression problem,

414
00:19:15.750 --> 00:19:19.575
you can deal with outliers
or even missing data

415
00:19:19.575 --> 00:19:24.000
by just assigning a new
value using imputation.

416
00:19:24.000 --> 00:19:33.960
Now, imputation is going
to make a best guess,

417
00:19:33.960 --> 00:19:37.185
so to speak, as to what the
value actually should be.

418
00:19:37.185 --> 00:19:40.590
For instance, you might have
a set of data and you can

419
00:19:40.590 --> 00:19:45.685
take a mean, 45.

420
00:19:45.685 --> 00:19:48.920
This 45 is going to

421
00:19:48.920 --> 00:19:52.100
be what I would use in
the case of missing data.

422
00:19:52.100 --> 00:19:54.680
So in the salary
prediction example,

423
00:19:54.680 --> 00:19:56.630
let's say our data looks
something like this.

424
00:19:56.630 --> 00:20:00.070
So Employee 1 is going
to actually work,

425
00:20:00.070 --> 00:20:04.680
let's say 35 hours for a
week and another employee,

426
00:20:04.680 --> 00:20:09.135
E2, is going to work 44
hours for that week.

427
00:20:09.135 --> 00:20:12.195
Then you have Employee 3.

428
00:20:12.195 --> 00:20:16.140
No, there is no data
for Employee 3.

429
00:20:16.140 --> 00:20:19.500
Rather than just eliminating
it or worse putting a

430
00:20:19.500 --> 00:20:21.570
zero because Employee 3 did not

431
00:20:21.570 --> 00:20:24.030
work zero hours that'll
mess up your data,

432
00:20:24.030 --> 00:20:29.040
in this case, you can simply
take your mean which is 45,

433
00:20:29.040 --> 00:20:30.765
I embed drawing on this board.

434
00:20:30.765 --> 00:20:33.870
So we are going to just
put that in there and say,

435
00:20:33.870 --> 00:20:36.270
for Employee 3, it's going to

436
00:20:36.270 --> 00:20:39.255
take the mean which is 45 hours.

437
00:20:39.255 --> 00:20:40.890
Great. In place the missing data.

438
00:20:40.890 --> 00:20:43.560
It's not a zero, I'm not
ignoring Employee 3,

439
00:20:43.560 --> 00:20:44.640
I'm still going to have wait for

440
00:20:44.640 --> 00:20:45.690
it even though I don't know

441
00:20:45.690 --> 00:20:48.900
what it is, it's
going to be valuable.

442
00:20:48.900 --> 00:20:52.965
All right. Good. Now,
along with histograms,

443
00:20:52.965 --> 00:20:56.475
another visualization
tool, scatter plots.

444
00:20:56.475 --> 00:20:59.310
So in this case,

445
00:20:59.310 --> 00:21:02.130
the idea of scatter
plots is to visualize

446
00:21:02.130 --> 00:21:05.880
the relationship between the
features and the labels,

447
00:21:05.880 --> 00:21:07.800
where what you've
got are a whole lot

448
00:21:07.800 --> 00:21:11.205
of different unique points.

449
00:21:11.205 --> 00:21:14.070
It's important to
understand if there's

450
00:21:14.070 --> 00:21:17.460
a strong correlation between
features and labels.

451
00:21:17.460 --> 00:21:18.885
In this instance,

452
00:21:18.885 --> 00:21:21.899
a scatter plot might actually
help us see the correlation

453
00:21:21.899 --> 00:21:23.580
between the number of hours

454
00:21:23.580 --> 00:21:26.385
worked and their income levels.

455
00:21:26.385 --> 00:21:28.215
So in this case,

456
00:21:28.215 --> 00:21:32.190
yeah, it's looking like
a strong correlation.

457
00:21:32.190 --> 00:21:34.470
Now, on the flip side,

458
00:21:34.470 --> 00:21:38.280
we might see a weak correlation
if we were to use age and

459
00:21:38.280 --> 00:21:39.800
those elements being out

460
00:21:39.800 --> 00:21:43.190
here in that case,
nothing of value.

461
00:21:43.190 --> 00:21:46.445
When thinking about
data preparation,

462
00:21:46.445 --> 00:21:50.180
keep in mind that if you
don't address noisy data,

463
00:21:50.180 --> 00:21:52.960
it's just going to hurt
your model's performance.

464
00:21:52.960 --> 00:21:56.355
These types of
visualization techniques

465
00:21:56.355 --> 00:21:58.440
and approaches are critical.

466
00:21:58.440 --> 00:22:00.900
Your model will suffer because of

467
00:22:00.900 --> 00:22:04.980
noisy data points like
outliers or missing data.

468
00:22:04.980 --> 00:22:08.085
This results in less
accurate predictions.

469
00:22:08.085 --> 00:22:11.429
So we've been talking

470
00:22:11.429 --> 00:22:14.325
about in order to get
accurate predictions,

471
00:22:14.325 --> 00:22:18.440
you have to get clean data.
But there's more to that.

472
00:22:18.440 --> 00:22:20.615
You need an algorithm

473
00:22:20.615 --> 00:22:23.345
that makes sense for
your business problem.

474
00:22:23.345 --> 00:22:26.030
Choosing the right
algorithm for the job

475
00:22:26.030 --> 00:22:29.815
is another big step in this
part of the ML pipeline.

476
00:22:29.815 --> 00:22:31.635
It can be a challenge for

477
00:22:31.635 --> 00:22:33.450
any machine learning practitioner

478
00:22:33.450 --> 00:22:34.860
especially given that there are

479
00:22:34.860 --> 00:22:37.710
several 100 algorithms out there.

480
00:22:37.710 --> 00:22:42.870
Now, to help out,

481
00:22:42.870 --> 00:22:46.575
let's talk about these
four different categories

482
00:22:46.575 --> 00:22:48.480
of machine learning algorithms.

483
00:22:48.480 --> 00:22:51.285
We've got supervised,
unsupervised,

484
00:22:51.285 --> 00:22:53.985
reinforcement, and deep learning.

485
00:22:53.985 --> 00:22:56.775
Let's start here.
Supervised learning.

486
00:22:56.775 --> 00:22:58.740
It's a popular type of

487
00:22:58.740 --> 00:23:02.205
machine learning because
it's widely applicable,

488
00:23:02.205 --> 00:23:05.235
has several successful
applications out in the world.

489
00:23:05.235 --> 00:23:09.510
The focus of supervised
algorithms is on learning

490
00:23:09.510 --> 00:23:11.820
patterns by seeing the
relationship between

491
00:23:11.820 --> 00:23:14.700
variables and known outcomes.

492
00:23:14.700 --> 00:23:16.590
It's called supervised learning

493
00:23:16.590 --> 00:23:18.420
because there needs
to be a supervisor,

494
00:23:18.420 --> 00:23:20.430
a trainer, that can actually

495
00:23:20.430 --> 00:23:23.700
show the engine the right
answers, so to speak.

496
00:23:23.700 --> 00:23:25.500
In machine learning by the way,

497
00:23:25.500 --> 00:23:29.160
a trainer can be any
sort of complex systems,

498
00:23:29.160 --> 00:23:31.140
could be machine, could be human,

499
00:23:31.140 --> 00:23:33.600
or other natural processes.

500
00:23:33.600 --> 00:23:37.110
Imagine you're training a
Machine Learning model that's

501
00:23:37.110 --> 00:23:39.870
capable of predicting
future earthquakes.

502
00:23:39.870 --> 00:23:43.065
In this case, the teacher or

503
00:23:43.065 --> 00:23:47.370
the ultimate source of
truth is nature herself.

504
00:23:47.370 --> 00:23:49.709
Like any student,

505
00:23:49.709 --> 00:23:53.565
a supervised algorithm
needs to learn by example.

506
00:23:53.565 --> 00:23:55.770
Essentially, it
needs a teacher who

507
00:23:55.770 --> 00:23:58.110
uses training data to help it

508
00:23:58.110 --> 00:24:00.330
determine the patterns
and relationships

509
00:24:00.330 --> 00:24:03.615
between the inputs
and the outputs.

510
00:24:03.615 --> 00:24:07.635
This picture here for
example, it's a car.

511
00:24:07.635 --> 00:24:10.065
So you get a nice little car.

512
00:24:10.065 --> 00:24:11.760
I embed the cars.

513
00:24:11.760 --> 00:24:13.020
It's got two wheels,

514
00:24:13.020 --> 00:24:14.610
it's got a headlight,
you got to have

515
00:24:14.610 --> 00:24:17.025
a window font there,
that's a car, great.

516
00:24:17.025 --> 00:24:18.510
This one over here,

517
00:24:18.510 --> 00:24:19.815
well, that's a truck.

518
00:24:19.815 --> 00:24:24.180
Okay. Fine. After the
training is finished,

519
00:24:24.180 --> 00:24:26.294
a successful learning algorithm

520
00:24:26.294 --> 00:24:28.140
can make the
decisions on its own.

521
00:24:28.140 --> 00:24:30.480
You no longer need a
teacher to actually

522
00:24:30.480 --> 00:24:33.075
label things as car or truck.

523
00:24:33.075 --> 00:24:36.389
In the end, the output
knows that's a car,

524
00:24:36.389 --> 00:24:39.300
that's a truck, it
can do it by itself.

525
00:24:39.300 --> 00:24:42.240
The call center use case

526
00:24:42.240 --> 00:24:45.195
is an example of
supervised learning.

527
00:24:45.195 --> 00:24:47.370
We trained our model on a bunch

528
00:24:47.370 --> 00:24:49.680
of historical customer data

529
00:24:49.680 --> 00:24:51.930
that included the correct labels

530
00:24:51.930 --> 00:24:55.210
or the customer agent skills.

531
00:24:56.840 --> 00:25:00.060
That enabled the model to make

532
00:25:00.060 --> 00:25:01.980
its own prediction based

533
00:25:01.980 --> 00:25:03.990
on other similar
data moving forward.

534
00:25:03.990 --> 00:25:06.180
So for example, that I know that

535
00:25:06.180 --> 00:25:10.980
this particular call needs
someone with a Kindle skill.

536
00:25:10.980 --> 00:25:13.470
We'll talk more about
what it means for

537
00:25:13.470 --> 00:25:16.440
an algorithm to determine
relationship later,

538
00:25:16.440 --> 00:25:20.205
we talk about parameters and
hyperparameters for now.

539
00:25:20.205 --> 00:25:22.980
Let's go ahead and focus
right now though on

540
00:25:22.980 --> 00:25:26.325
the types of algorithms
rather than those elements.

541
00:25:26.325 --> 00:25:28.740
Supervised algorithms.

542
00:25:28.740 --> 00:25:31.830
They need good training datasets.

543
00:25:31.830 --> 00:25:35.040
In properly labeled observations,

544
00:25:35.040 --> 00:25:37.920
hang on, I need to
emphasize something.

545
00:25:37.920 --> 00:25:40.770
It's really important to
know that this type of

546
00:25:40.770 --> 00:25:43.785
machine learning
is only successful

547
00:25:43.785 --> 00:25:46.530
if the system we are
trying to model it after

548
00:25:46.530 --> 00:25:50.130
is already functioning
and easy to observe.

549
00:25:50.130 --> 00:25:52.590
If we want to train
a model that label

550
00:25:52.590 --> 00:25:55.245
cars or trucks or
buses or whatever,

551
00:25:55.245 --> 00:25:56.700
then we need to make sure that

552
00:25:56.700 --> 00:25:59.055
the training data is labeled.

553
00:25:59.055 --> 00:26:02.370
If not, then you
got to go through

554
00:26:02.370 --> 00:26:03.930
a large number of photos and

555
00:26:03.930 --> 00:26:06.060
actually label them manually.

556
00:26:06.060 --> 00:26:10.005
Now, if such a human process
was not already in place,

557
00:26:10.005 --> 00:26:12.915
then obtaining that
ideal training dataset,

558
00:26:12.915 --> 00:26:16.310
it could be problematic
and might ultimately be

559
00:26:16.310 --> 00:26:21.140
a reason to not pursue a
supervised learning algorithm.

560
00:26:21.140 --> 00:26:23.660
So let's talk about

561
00:26:23.660 --> 00:26:25.700
what happens when there's
no teacher in the room.

562
00:26:25.700 --> 00:26:29.025
Okay, I'm gone. Wait a minute,

563
00:26:29.025 --> 00:26:30.870
I still have to be here.
Sorry, I thought you'd quit.

564
00:26:30.870 --> 00:26:32.730
No. Hello, here we go.

565
00:26:32.730 --> 00:26:36.525
Sometimes all we've
got is just the data.

566
00:26:36.525 --> 00:26:38.250
No provided labels.

567
00:26:38.250 --> 00:26:41.880
There's nobody here telling
you what something is.

568
00:26:41.880 --> 00:26:44.595
Can something useful
still be learned?

569
00:26:44.595 --> 00:26:50.160
Well, yeah, that's
unsupervised learning.

570
00:26:50.160 --> 00:26:53.370
With unsupervised algorithms,

571
00:26:53.370 --> 00:26:55.800
we don't know all the variables.

572
00:26:55.800 --> 00:26:57.300
We don't know the patterns.

573
00:26:57.300 --> 00:27:00.450
So the machine itself
simply looks at

574
00:27:00.450 --> 00:27:04.230
the data and tries to create
labels all on its own.

575
00:27:04.230 --> 00:27:05.895
A common type of

576
00:27:05.895 --> 00:27:10.410
unsupervised learning,
it's called clustering.

577
00:27:13.950 --> 00:27:16.539
This algorithm,

578
00:27:16.539 --> 00:27:19.900
it groups data points
into different clusters

579
00:27:19.900 --> 00:27:22.420
based on similar features
in order to better

580
00:27:22.420 --> 00:27:26.260
understand the attributes of
a specific group or cluster.

581
00:27:26.260 --> 00:27:29.395
For instance, let's say you sell

582
00:27:29.395 --> 00:27:32.770
office supplies different
companies all over the world.

583
00:27:32.770 --> 00:27:36.595
Well, in analyzing customer
purchasing habits,

584
00:27:36.595 --> 00:27:39.865
an unsupervised model
might actually be

585
00:27:39.865 --> 00:27:44.480
able to identify two
different groups.

586
00:27:45.120 --> 00:27:47.770
Each groups, there's no need for

587
00:27:47.770 --> 00:27:51.250
a label but what it
finds out is that maybe

588
00:27:51.250 --> 00:27:52.960
this one group is just

589
00:27:52.960 --> 00:27:56.710
purchasing paper and
pencils or whatever,

590
00:27:56.710 --> 00:27:59.815
and this turns out to
be smaller companies.

591
00:27:59.815 --> 00:28:02.620
Whereas this other cluster
of groups is buying

592
00:28:02.620 --> 00:28:06.790
conference tables and chairs
and big furniture items,

593
00:28:06.790 --> 00:28:09.790
it turns out this happens to
be your larger companies.

594
00:28:09.790 --> 00:28:12.160
You may not have had
this label initially

595
00:28:12.160 --> 00:28:14.410
but just their purchasing
habits started

596
00:28:14.410 --> 00:28:15.730
dividing them up into buckets

597
00:28:15.730 --> 00:28:19.570
automatically or specifically
the engine did that.

598
00:28:19.570 --> 00:28:22.390
Clustering in this situation

599
00:28:22.390 --> 00:28:24.610
could help you
realize that you need

600
00:28:24.610 --> 00:28:27.220
to come up with different
marketing strategy

601
00:28:27.220 --> 00:28:29.335
for different types of companies.

602
00:28:29.335 --> 00:28:31.915
Consider fraud detection.

603
00:28:31.915 --> 00:28:35.110
A supervised algorithm
could predict

604
00:28:35.110 --> 00:28:39.220
a particular threat that's
already been classified.

605
00:28:39.220 --> 00:28:41.455
But the most dangerous attacks

606
00:28:41.455 --> 00:28:43.705
are the ones you
don't see coming.

607
00:28:43.705 --> 00:28:45.370
The ones you don't know about.

608
00:28:45.370 --> 00:28:49.420
That is the ones that haven't
already been labeled.

609
00:28:49.420 --> 00:28:52.270
To detect an
unclassified category

610
00:28:52.270 --> 00:28:54.160
of fraud in the early phases,

611
00:28:54.160 --> 00:28:56.485
like a sudden large order from an

612
00:28:56.485 --> 00:28:59.560
unknown user or a suspicious
shipping address,

613
00:28:59.560 --> 00:29:03.370
unsupervised algorithms
group malicious actors

614
00:29:03.370 --> 00:29:05.485
into a cluster and then

615
00:29:05.485 --> 00:29:08.380
analyze their connections
to other accounts without

616
00:29:08.380 --> 00:29:12.580
knowing the actual labels
of the attack originally.

617
00:29:12.580 --> 00:29:16.090
All right. Another algorithm,

618
00:29:16.090 --> 00:29:17.620
it's been gaining popularity

619
00:29:17.620 --> 00:29:21.895
a lot recently,
Reinforcement Learning.

620
00:29:21.895 --> 00:29:23.920
Let me put this up
on the board here.

621
00:29:23.920 --> 00:29:25.990
So if our example here
we're going to start

622
00:29:25.990 --> 00:29:32.110
with the agent, then an action.

623
00:29:33.180 --> 00:29:36.350
This is the environment.

624
00:29:38.790 --> 00:29:47.480
Finally, we come over here
to the state, the reward.

625
00:29:50.180 --> 00:29:52.560
This becomes the loop.

626
00:29:52.560 --> 00:29:57.015
Now unlike these
first two algorithms

627
00:29:57.015 --> 00:30:00.485
that both actually have an
endpoint and end state,

628
00:30:00.485 --> 00:30:03.910
this one, reinforcement,
continually

629
00:30:03.910 --> 00:30:08.935
improves by mining feedback
from previous iterations.

630
00:30:08.935 --> 00:30:13.060
In reinforcement learning,
this agent continually

631
00:30:13.060 --> 00:30:14.995
learns through trial and error

632
00:30:14.995 --> 00:30:17.650
as it interacts with
the environment.

633
00:30:17.650 --> 00:30:21.550
The reinforcement learning
is broadly useful when

634
00:30:21.550 --> 00:30:24.010
the reward of a
desired outcome is

635
00:30:24.010 --> 00:30:27.940
known but the path
to achieve it isn't.

636
00:30:27.940 --> 00:30:30.340
That path requires a lot

637
00:30:30.340 --> 00:30:32.755
of trial and error to
actually discover.

638
00:30:32.755 --> 00:30:35.710
Well, let's think
of Pac-Man here.

639
00:30:35.710 --> 00:30:37.585
So you've got Pac-Man.

640
00:30:37.585 --> 00:30:39.980
We get Pac-Man out there.

641
00:30:40.560 --> 00:30:44.275
Pac-Man, fine. In this case,

642
00:30:44.275 --> 00:30:45.760
maybe it's supply chain,

643
00:30:45.760 --> 00:30:47.080
but Pac-Man's more fun.

644
00:30:47.080 --> 00:30:49.030
So we're going to go
ahead and the action

645
00:30:49.030 --> 00:30:51.325
might be is it going to go left?

646
00:30:51.325 --> 00:30:54.160
Is it going to go
right, don't know?

647
00:30:54.160 --> 00:30:56.920
So depending on whether
he goes left or right,

648
00:30:56.920 --> 00:31:00.910
the reward or the state is
going to constantly change.

649
00:31:00.910 --> 00:31:02.590
The model is learning,

650
00:31:02.590 --> 00:31:04.705
and it's going to be graded

651
00:31:04.705 --> 00:31:08.140
as opposed to tagged or labeled.

652
00:31:08.140 --> 00:31:10.195
So if I go left,

653
00:31:10.195 --> 00:31:11.920
maybe no that's bad.

654
00:31:11.920 --> 00:31:13.990
So I'm going to have
a score of minus two.

655
00:31:13.990 --> 00:31:16.060
But if I go right,
okay that's good,

656
00:31:16.060 --> 00:31:18.160
that's going to be a
plus two whatever it is.

657
00:31:18.160 --> 00:31:21.025
Think about playing
a new board game

658
00:31:21.025 --> 00:31:23.034
but you don't know the rules,

659
00:31:23.034 --> 00:31:25.060
and you might not know the
intricacies of the game,

660
00:31:25.060 --> 00:31:26.620
but you just know you got to get

661
00:31:26.620 --> 00:31:28.450
to the other side of the board.

662
00:31:28.450 --> 00:31:30.760
So as you move through
the game and you learn

663
00:31:30.760 --> 00:31:33.190
the values of certain actions,

664
00:31:33.190 --> 00:31:35.590
you get more familiar
with the space,

665
00:31:35.590 --> 00:31:38.650
left, bad, right, good.

666
00:31:38.650 --> 00:31:40.390
"No, it's fire-breathing dragon.

667
00:31:40.390 --> 00:31:42.145
No, negative two whatever."

668
00:31:42.145 --> 00:31:44.230
Fine. These values you

669
00:31:44.230 --> 00:31:47.395
learn can influence
your future behavior.

670
00:31:47.395 --> 00:31:49.000
Well, I'm sure as heck
ain't going to do

671
00:31:49.000 --> 00:31:51.010
that move again that's a bad one.

672
00:31:51.010 --> 00:31:52.150
I'm not going to
keep moving towards

673
00:31:52.150 --> 00:31:54.550
the dragon or the ghost.

674
00:31:54.550 --> 00:31:56.185
So as a result,

675
00:31:56.185 --> 00:31:59.045
the performance
starts to improve.

676
00:31:59.045 --> 00:32:04.875
It gets better based on your
past experience. All right.

677
00:32:04.875 --> 00:32:07.230
That's reinforcement, fine.

678
00:32:07.230 --> 00:32:13.935
Now, let's talk about the
deep learning algorithms.

679
00:32:13.935 --> 00:32:16.785
Yeah, here's a buzzword
for you, right?

680
00:32:16.785 --> 00:32:20.625
Deep Learning. It's a reinvention

681
00:32:20.625 --> 00:32:23.645
of artificial neural networks.

682
00:32:23.645 --> 00:32:25.210
Now if you're thinking about

683
00:32:25.210 --> 00:32:27.160
the biological neural network,

684
00:32:27.160 --> 00:32:32.080
so here's a neuron and that's
connects and it's fine.

685
00:32:32.080 --> 00:32:33.865
I can't draw neuron,
pretend it's a neuron.

686
00:32:33.865 --> 00:32:36.355
You've got these in the brain.
If you think like that,

687
00:32:36.355 --> 00:32:39.970
you're actually on the right
track because just like

688
00:32:39.970 --> 00:32:42.190
a biological neural network

689
00:32:42.190 --> 00:32:44.530
where this connects to
another nerve, another nerve,

690
00:32:44.530 --> 00:32:48.910
and so on, each neuron is
activated when the sum of the

691
00:32:48.910 --> 00:32:51.234
input signals into one neuron

692
00:32:51.234 --> 00:32:53.890
exceeds a particular threshold.

693
00:32:53.890 --> 00:32:57.475
The thing is, a single neuron,

694
00:32:57.475 --> 00:32:59.110
it's not sufficient for

695
00:32:59.110 --> 00:33:01.660
any practical
classification needs.

696
00:33:01.660 --> 00:33:05.170
Instead, we combine them into
a fully connected set of

697
00:33:05.170 --> 00:33:09.745
layers to produce
artificial neural networks.

698
00:33:09.745 --> 00:33:14.065
We call these
Multilayer Perceptrons.

699
00:33:14.065 --> 00:33:17.364
So you might start
with some inputs,

700
00:33:17.364 --> 00:33:19.090
and each of these can
be considered a neuron

701
00:33:19.090 --> 00:33:20.830
but then you've
got a whole lot of

702
00:33:20.830 --> 00:33:23.110
different hidden layers and each

703
00:33:23.110 --> 00:33:25.630
one of these of their
own piece and so on.

704
00:33:25.630 --> 00:33:27.970
Eventually, you might
get to an output,

705
00:33:27.970 --> 00:33:29.245
but it's a collection of these.

706
00:33:29.245 --> 00:33:34.000
How deep is deep learning
in the real-world?

707
00:33:34.000 --> 00:33:36.280
Some networks can have

708
00:33:36.280 --> 00:33:40.970
thousands of layers
of these perceptrons.

709
00:33:41.010 --> 00:33:43.359
As you can imagine,

710
00:33:43.359 --> 00:33:45.670
the computational
power required to

711
00:33:45.670 --> 00:33:48.610
train such networks,
it's not cheap.

712
00:33:48.610 --> 00:33:50.650
One important breakthrough in

713
00:33:50.650 --> 00:33:53.260
deep learning was
the invention of

714
00:33:53.260 --> 00:33:58.180
Convolutional Neural
Networks or CNNs for short.

715
00:33:58.180 --> 00:34:01.885
These are especially useful
for image processing.

716
00:34:01.885 --> 00:34:05.440
Now the main idea of a CNN is,

717
00:34:05.440 --> 00:34:07.360
in this case for image
processing is I take

718
00:34:07.360 --> 00:34:09.100
nearby pixels in the image into

719
00:34:09.100 --> 00:34:10.990
account instead of treating them

720
00:34:10.990 --> 00:34:13.750
as entirely separate inputs.

721
00:34:13.750 --> 00:34:17.155
A special operation
called a convolution

722
00:34:17.155 --> 00:34:21.070
is applied to entire
subsections of the image.

723
00:34:21.070 --> 00:34:23.140
If several convolutional layers

724
00:34:23.140 --> 00:34:25.210
are stacked one after another,

725
00:34:25.210 --> 00:34:27.070
each convolutional layer learns

726
00:34:27.070 --> 00:34:28.825
to recognize patterns that

727
00:34:28.825 --> 00:34:33.355
increase in complexity as it
moves through the layers.

728
00:34:33.355 --> 00:34:39.130
Now, if we take the output
of a neuron and feed it

729
00:34:39.130 --> 00:34:44.380
as an input to itself or to
neurons of previous layers.

730
00:34:44.380 --> 00:34:47.604
So instead of everything
going in one direction,

731
00:34:47.604 --> 00:34:51.955
we actually feed backwards
or maybe into itself.

732
00:34:51.955 --> 00:34:56.305
This is what we call
Recurrent Neural Networks.

733
00:34:56.305 --> 00:34:58.659
It's as if the neuron remembers

734
00:34:58.659 --> 00:35:00.835
the output from a
previous iteration,

735
00:35:00.835 --> 00:35:03.760
thus creating some memory.

736
00:35:03.760 --> 00:35:08.185
A more complex network
is called LSTM,

737
00:35:08.185 --> 00:35:11.050
stands for long
short-term memory.

738
00:35:11.050 --> 00:35:14.590
It's commonly used for speech
recognition or translation.

739
00:35:14.590 --> 00:35:17.630
It's conversation
for another time.

740
00:35:18.930 --> 00:35:21.775
Feature Selection.

741
00:35:21.775 --> 00:35:24.760
This our next important
step, feature selection,

742
00:35:24.760 --> 00:35:26.650
where you get to select which

743
00:35:26.650 --> 00:35:29.500
features you want to
use with your model.

744
00:35:29.500 --> 00:35:31.900
What you want to have is

745
00:35:31.900 --> 00:35:34.825
a minimal correlation
among your features,

746
00:35:34.825 --> 00:35:37.330
but you want to have
the maximum correlation

747
00:35:37.330 --> 00:35:39.865
between the features
and the desired output.

748
00:35:39.865 --> 00:35:41.530
So you want to select
the features that

749
00:35:41.530 --> 00:35:43.825
correlate to your desired output.

750
00:35:43.825 --> 00:35:47.170
Now part of selecting the
best features includes

751
00:35:47.170 --> 00:35:51.185
recognizing when you've
got to engineer a feature.

752
00:35:51.185 --> 00:35:54.960
Feature engineering is the
process of manipulating

753
00:35:54.960 --> 00:35:56.700
your original data into

754
00:35:56.700 --> 00:36:00.500
new and potentially a lot
more useful features.

755
00:36:00.500 --> 00:36:02.860
Feature engineering is arguably

756
00:36:02.860 --> 00:36:05.575
the most critical and
time-consuming step

757
00:36:05.575 --> 00:36:07.135
of the ML Pipeline.

758
00:36:07.135 --> 00:36:09.130
It answers questions like,

759
00:36:09.130 --> 00:36:10.810
do the features I'm using

760
00:36:10.810 --> 00:36:12.955
makes sense for what
I want to predict?

761
00:36:12.955 --> 00:36:14.530
Or how can I

762
00:36:14.530 --> 00:36:17.080
systematically take what I've
learned about my features

763
00:36:17.080 --> 00:36:19.330
during the visualization
process and

764
00:36:19.330 --> 00:36:22.585
encode that information
into new features?

765
00:36:22.585 --> 00:36:25.330
For instance, in looking at

766
00:36:25.330 --> 00:36:27.940
the raw data of our
call center use case,

767
00:36:27.940 --> 00:36:29.500
you might have noticed already,

768
00:36:29.500 --> 00:36:31.270
50 percent of the customers were

769
00:36:31.270 --> 00:36:33.430
calling in about
tracking a package.

770
00:36:33.430 --> 00:36:36.175
However, after visualization,

771
00:36:36.175 --> 00:36:38.350
25 percent of those customers

772
00:36:38.350 --> 00:36:40.795
calling in about
tracking packages,

773
00:36:40.795 --> 00:36:43.645
they're actually located
in the exact same city.

774
00:36:43.645 --> 00:36:45.625
Now that's a large number.

775
00:36:45.625 --> 00:36:49.240
It's potentially
significant pattern.

776
00:36:49.240 --> 00:36:52.390
In this situation,
you could engineer

777
00:36:52.390 --> 00:36:53.470
a feature for

778
00:36:53.470 --> 00:36:57.220
customer's tracking packages
in specific cities.

779
00:36:57.220 --> 00:36:59.875
This information might
lead to same patterns,

780
00:36:59.875 --> 00:37:02.045
you otherwise wouldn't
have seen before.

781
00:37:02.045 --> 00:37:05.430
We've had some features that
answered questions like,

782
00:37:05.430 --> 00:37:08.745
what was the customer's
most recent order?

783
00:37:08.745 --> 00:37:12.510
What was the time of the
customer's most recent order?

784
00:37:12.510 --> 00:37:14.720
Does the customer own a kindle?

785
00:37:14.720 --> 00:37:16.765
When we feed these features

786
00:37:16.765 --> 00:37:19.015
into the model
training algorithm,

787
00:37:19.015 --> 00:37:23.350
it can only learn from
exactly what we show it.

788
00:37:23.350 --> 00:37:25.270
Here, for instance, we're

789
00:37:25.270 --> 00:37:27.010
showing the model
that this purchase

790
00:37:27.010 --> 00:37:30.550
was made at 1:00 PM
on Tuesday the 13th.

791
00:37:30.550 --> 00:37:33.280
Well, unless we really
want to predict something

792
00:37:33.280 --> 00:37:37.315
extremely specific or we're
doing a time series analysis,

793
00:37:37.315 --> 00:37:39.490
that's not really a
meaningful feature

794
00:37:39.490 --> 00:37:41.245
we want to feed into our model.

795
00:37:41.245 --> 00:37:43.540
It'd be much more
meaningful if we could

796
00:37:43.540 --> 00:37:45.640
transform that timestamp into

797
00:37:45.640 --> 00:37:48.070
a feature that represents

798
00:37:48.070 --> 00:37:51.025
maybe how long ago
that order took place.

799
00:37:51.025 --> 00:37:53.470
Knowing, for instance, that
your last purchase was

800
00:37:53.470 --> 00:37:56.260
months ago would
probably help the model

801
00:37:56.260 --> 00:37:58.870
realize that your
last purchase is

802
00:37:58.870 --> 00:38:02.020
probably not the reason
you're calling today.

803
00:38:02.020 --> 00:38:05.470
Now obviously, we can engine
those feature just by

804
00:38:05.470 --> 00:38:06.535
taking the diff between

805
00:38:06.535 --> 00:38:09.160
order date-time and
today's date-time.

806
00:38:09.160 --> 00:38:12.490
That's a much more
helpful feature.

807
00:38:12.490 --> 00:38:14.290
Here's another example we could

808
00:38:14.290 --> 00:38:17.845
use about image classification.

809
00:38:17.845 --> 00:38:20.050
Let's say you wanted
to train a model to

810
00:38:20.050 --> 00:38:21.805
identify cars in a picture.

811
00:38:21.805 --> 00:38:24.535
Fine. You can do this by feeding

812
00:38:24.535 --> 00:38:29.200
raw images of cars and training
it to identify the car.

813
00:38:29.200 --> 00:38:32.350
But it won't be that
helpful given that

814
00:38:32.350 --> 00:38:36.475
these images are very complex
combination of pixels.

815
00:38:36.475 --> 00:38:38.020
The raw data, that is

816
00:38:38.020 --> 00:38:39.970
the raw images you're
going to feed in,

817
00:38:39.970 --> 00:38:41.230
it doesn't include

818
00:38:41.230 --> 00:38:44.034
any higher-level
features such as edges,

819
00:38:44.034 --> 00:38:48.550
lines, circles, the patterns
that it can recognize.

820
00:38:48.550 --> 00:38:52.120
So during the feature
engineering stage,

821
00:38:52.120 --> 00:38:54.745
you can pre-process the data.

822
00:38:54.745 --> 00:38:56.440
This will classify it,

823
00:38:56.440 --> 00:38:58.885
possibly get to more
granular features,

824
00:38:58.885 --> 00:39:01.510
that way can feed
those features back

825
00:39:01.510 --> 00:39:04.600
into the model, get
better accuracy.

826
00:39:04.600 --> 00:39:07.150
We'll talk more
about accuracy and

827
00:39:07.150 --> 00:39:10.160
precision in a little
bit, but that's critical.

828
00:39:11.940 --> 00:39:15.200
Finally ready for training.

829
00:39:18.450 --> 00:39:20.485
First step you have to take

830
00:39:20.485 --> 00:39:21.730
when you're officially training

831
00:39:21.730 --> 00:39:24.460
your data is you
have to split it.

832
00:39:24.460 --> 00:39:26.980
Now, splitting the
data allows you

833
00:39:26.980 --> 00:39:29.470
to ensure that you've got
production data that's

834
00:39:29.470 --> 00:39:32.785
similar to your training
data that your model will as

835
00:39:32.785 --> 00:39:34.870
a result be more

836
00:39:34.870 --> 00:39:37.780
generalizable or
applicable outside

837
00:39:37.780 --> 00:39:39.355
of the training environment.

838
00:39:39.355 --> 00:39:41.080
Let's head over the board

839
00:39:41.080 --> 00:39:42.100
so we can investigate
this a little

840
00:39:42.100 --> 00:39:49.420
more closely. Here we go.

841
00:39:49.420 --> 00:39:52.540
Once again, thanks Tom for
doing the work for me.

842
00:39:52.540 --> 00:39:55.960
Typically, you want to split

843
00:39:55.960 --> 00:39:58.885
your data into three sections:

844
00:39:58.885 --> 00:40:00.730
you've got your training data,

845
00:40:00.730 --> 00:40:03.400
your dev data, and
your test data.

846
00:40:03.400 --> 00:40:05.740
Now, training data is going to

847
00:40:05.740 --> 00:40:08.515
include both the
features and the labels,

848
00:40:08.515 --> 00:40:10.150
this feeds into the
algorithm you've

849
00:40:10.150 --> 00:40:12.235
selected to help
produce your model.

850
00:40:12.235 --> 00:40:13.810
The model is then used to make

851
00:40:13.810 --> 00:40:17.125
predictions over a
developments dataset,

852
00:40:17.125 --> 00:40:18.550
which is where you'll likely

853
00:40:18.550 --> 00:40:20.260
notice things that
you'll want to tweak,

854
00:40:20.260 --> 00:40:21.865
and tune, and change.

855
00:40:21.865 --> 00:40:23.920
Then when you're ready,

856
00:40:23.920 --> 00:40:26.754
then you can actually
run the test dataset,

857
00:40:26.754 --> 00:40:29.655
which only includes
features since

858
00:40:29.655 --> 00:40:30.990
you want the labels to be

859
00:40:30.990 --> 00:40:32.940
what's predicted
through the model.

860
00:40:32.940 --> 00:40:35.985
The performance you get
here with a test dataset

861
00:40:35.985 --> 00:40:37.200
is then what you can

862
00:40:37.200 --> 00:40:40.295
reasonably expect to
see in production.

863
00:40:40.295 --> 00:40:43.210
The amount of data you will

864
00:40:43.210 --> 00:40:46.345
have determines how
ultimately you split it up.

865
00:40:46.345 --> 00:40:48.760
But regardless,
you'll want to train

866
00:40:48.760 --> 00:40:51.010
your model on as much data as

867
00:40:51.010 --> 00:40:53.590
possible knowing that
you're going to need to

868
00:40:53.590 --> 00:40:54.730
reserve some of it for

869
00:40:54.730 --> 00:40:56.965
the dev phase and
some for testing.

870
00:40:56.965 --> 00:40:59.020
So if you have a lot of data,

871
00:40:59.020 --> 00:41:02.110
then you can probably
split it up into let's

872
00:41:02.110 --> 00:41:06.040
say 70 percent here for training,

873
00:41:06.040 --> 00:41:08.650
and 15 percent for dev,

874
00:41:08.650 --> 00:41:12.230
and another 15 percent for test.

875
00:41:13.140 --> 00:41:16.390
If you have little data, well,

876
00:41:16.390 --> 00:41:19.810
maybe it's 80 percent 10 and 10,

877
00:41:19.810 --> 00:41:22.960
you'll end up working
it out the way you can.

878
00:41:22.960 --> 00:41:24.970
Another important
thing to note though

879
00:41:24.970 --> 00:41:27.260
as you start splitting
up your data,

880
00:41:27.260 --> 00:41:29.375
make sure you randomize it.

881
00:41:29.375 --> 00:41:30.970
This is critical.

882
00:41:30.970 --> 00:41:32.380
You've got to randomize it during

883
00:41:32.380 --> 00:41:35.590
your split to help
your model avoid bias.

884
00:41:35.590 --> 00:41:38.230
This is especially true
with structured data,

885
00:41:38.230 --> 00:41:40.390
if your data coming
in a specific order.

886
00:41:40.390 --> 00:41:42.460
So let's say for example

887
00:41:42.460 --> 00:41:44.590
that your data is
listed sequentially.

888
00:41:44.590 --> 00:41:47.380
Well, your model will
start to become used to

889
00:41:47.380 --> 00:41:48.850
that structure and
it will start to

890
00:41:48.850 --> 00:41:51.460
adapt to this pattern
as it learns.

891
00:41:51.460 --> 00:41:52.810
Then eventually when you run

892
00:41:52.810 --> 00:41:54.685
your model against test data,

893
00:41:54.685 --> 00:41:57.925
this pattern of sequential data

894
00:41:57.925 --> 00:42:00.670
will be applied and
that'll bias your model.

895
00:42:00.670 --> 00:42:05.230
So effectively, to make sure
your model isn't biased,

896
00:42:05.230 --> 00:42:07.660
you need to feed it
randomized data.

897
00:42:07.660 --> 00:42:11.290
Now, popular randomization is
simply shuffling your data.

898
00:42:11.290 --> 00:42:13.630
Now, if you aren't familiar
with that, no worries,

899
00:42:13.630 --> 00:42:15.160
there's a lot of great tools out

900
00:42:15.160 --> 00:42:17.095
there that will help
you shuffle your data.

901
00:42:17.095 --> 00:42:19.690
For example, Scikit-learn.

902
00:42:19.690 --> 00:42:23.095
Now, randomizing and
splitting your training data

903
00:42:23.095 --> 00:42:26.665
is a critical step in
the training process.

904
00:42:26.665 --> 00:42:29.440
Common mistake
people make is that

905
00:42:29.440 --> 00:42:32.185
they don't hold out testing data,

906
00:42:32.185 --> 00:42:34.630
and what they end
up doing is simply

907
00:42:34.630 --> 00:42:35.950
testing on part of

908
00:42:35.950 --> 00:42:38.485
the data they trained
with, the training data.

909
00:42:38.485 --> 00:42:41.305
Well, this doesn't
generalize your model,

910
00:42:41.305 --> 00:42:43.705
it actually will lead to
either over fitting or

911
00:42:43.705 --> 00:42:46.450
underfitting. Let's
talk about that.

912
00:42:46.450 --> 00:42:49.345
Overfitting it's where your model

913
00:42:49.345 --> 00:42:53.020
learns the particulars
of a dataset too well.

914
00:42:53.020 --> 00:42:56.440
It's essentially memorizing
your training data

915
00:42:56.440 --> 00:42:59.050
as opposed to actually
learning the relationship

916
00:42:59.050 --> 00:43:00.655
between the features
and the labels

917
00:43:00.655 --> 00:43:02.500
so the model can use
what it learns in

918
00:43:02.500 --> 00:43:04.210
those relationships to build

919
00:43:04.210 --> 00:43:07.210
patterns to apply to
new data in the future.

920
00:43:07.210 --> 00:43:10.450
Remember our stock
data from earlier.

921
00:43:10.450 --> 00:43:11.860
Well, the model learns

922
00:43:11.860 --> 00:43:13.540
the pattern here
as the stock price

923
00:43:13.540 --> 00:43:15.130
goes up the end of the month

924
00:43:15.130 --> 00:43:17.005
and then drops the
beginning of the month.

925
00:43:17.005 --> 00:43:21.325
For example here,
30th 425, 4, 1, 375.

926
00:43:21.325 --> 00:43:23.920
It might miss other
important data

927
00:43:23.920 --> 00:43:25.870
that's likely
impacting the price,

928
00:43:25.870 --> 00:43:30.235
such as April is tax
season in this example.

929
00:43:30.235 --> 00:43:34.030
It's clear here that mixing
up the rows is going to be

930
00:43:34.030 --> 00:43:35.470
necessary to give the model

931
00:43:35.470 --> 00:43:39.280
an opportunity to learn
other things from the data.

932
00:43:39.280 --> 00:43:43.074
It's pretty clear here that
we need to look at more,

933
00:43:43.074 --> 00:43:44.845
a lot more dates.

934
00:43:44.845 --> 00:43:47.575
In addition to simply
randomizing the data,

935
00:43:47.575 --> 00:43:49.720
it's also very
important to collect

936
00:43:49.720 --> 00:43:53.050
as much relevant data
as possible because

937
00:43:53.050 --> 00:43:56.140
underfitting on the other
hand can occur if you don't

938
00:43:56.140 --> 00:43:59.710
have enough features to
model the data properly.

939
00:43:59.710 --> 00:44:01.870
This can again prevent
the model from

940
00:44:01.870 --> 00:44:04.165
properly generalizing
the data because it

941
00:44:04.165 --> 00:44:06.400
doesn't have enough
information to

942
00:44:06.400 --> 00:44:10.495
predict a right answer,
to predict correct.

943
00:44:10.495 --> 00:44:14.365
To really understand overfitting

944
00:44:14.365 --> 00:44:17.230
and underfitting and
how to avoid it,

945
00:44:17.230 --> 00:44:22.075
we need to talk about two
things; bias and variance.

946
00:44:22.075 --> 00:44:24.460
Think about bias as the gap

947
00:44:24.460 --> 00:44:26.500
between your predicted value and

948
00:44:26.500 --> 00:44:29.485
the actual value where variance

949
00:44:29.485 --> 00:44:33.460
describes how dispersed
your predicted values are.

950
00:44:33.460 --> 00:44:35.530
Now, that's a lot of jargon.

951
00:44:35.530 --> 00:44:37.495
So let's actually take a moment

952
00:44:37.495 --> 00:44:40.015
and look at it
visually over here.

953
00:44:40.015 --> 00:44:42.580
So a bull's eye that's

954
00:44:42.580 --> 00:44:46.690
a nice analogy to use here
because generally speaking,

955
00:44:46.690 --> 00:44:50.065
the center of the bull's eye
is where you aim your darts,

956
00:44:50.065 --> 00:44:51.700
the center of the bull's eye in

957
00:44:51.700 --> 00:44:55.495
this analogy is the
label or your target,

958
00:44:55.495 --> 00:44:57.880
it predicts the
value of your model.

959
00:44:57.880 --> 00:45:00.865
Each dot is then going to

960
00:45:00.865 --> 00:45:02.470
be a result that you're

961
00:45:02.470 --> 00:45:04.180
model produced
during the training.

962
00:45:04.180 --> 00:45:05.740
So let me demonstrate.

963
00:45:05.740 --> 00:45:10.225
So we start with a low
bias, low variance model.

964
00:45:10.225 --> 00:45:12.850
Everything's clustered tight and

965
00:45:12.850 --> 00:45:14.530
it's right there
in the bull's eye.

966
00:45:14.530 --> 00:45:17.335
I'm getting everything
I predict in one area,

967
00:45:17.335 --> 00:45:19.345
there's not a lot of spread.

968
00:45:19.345 --> 00:45:21.820
Now, next, if we go over to

969
00:45:21.820 --> 00:45:25.610
a low variance but a high bias,

970
00:45:26.310 --> 00:45:29.050
so in this case,

971
00:45:29.050 --> 00:45:31.480
I'm not getting
everything that I want,

972
00:45:31.480 --> 00:45:32.950
but at least I'm getting

973
00:45:32.950 --> 00:45:34.765
a predictable series
of responses.

974
00:45:34.765 --> 00:45:36.175
It's a tight cluster,

975
00:45:36.175 --> 00:45:38.335
I'm just not on the bull's eye.

976
00:45:38.335 --> 00:45:40.255
Now, on the other hand,

977
00:45:40.255 --> 00:45:43.345
a high variance low bias.

978
00:45:43.345 --> 00:45:44.845
Well, in this case,

979
00:45:44.845 --> 00:45:47.950
it means I'm on

980
00:45:47.950 --> 00:45:51.505
target as far as the
center of the spread goes,

981
00:45:51.505 --> 00:45:53.230
but the spread is wide,

982
00:45:53.230 --> 00:45:55.375
it's all over the place.

983
00:45:55.375 --> 00:45:59.200
Then high variance, high bias.

984
00:45:59.200 --> 00:46:02.695
Yeah. This is the bad.

985
00:46:02.695 --> 00:46:04.645
So in this case,

986
00:46:04.645 --> 00:46:09.070
I'm all over the place
and I'm not on target.

987
00:46:09.070 --> 00:46:11.905
Ideal? What's the ideal case?

988
00:46:11.905 --> 00:46:13.780
Yeah. You guessed it. You want

989
00:46:13.780 --> 00:46:17.390
the low bias and low variance.

990
00:46:18.060 --> 00:46:20.395
Realistically though?

991
00:46:20.395 --> 00:46:23.095
Yeah. There's a balancing
act that's happening here.

992
00:46:23.095 --> 00:46:27.834
Bias and variance both
contribute to errors,

993
00:46:27.834 --> 00:46:30.400
but what's you're
ultimately going for here

994
00:46:30.400 --> 00:46:33.370
is to minimize your
prediction error,

995
00:46:33.370 --> 00:46:36.835
not bias or variance
specifically.

996
00:46:36.835 --> 00:46:40.090
That's the bias
variance trade-off.

997
00:46:40.090 --> 00:46:42.850
Bringing underfitting
and overfitting

998
00:46:42.850 --> 00:46:44.410
back into the picture.

999
00:46:44.410 --> 00:46:46.975
Underfitting is where you've got

1000
00:46:46.975 --> 00:46:50.515
low variance and high bias.

1001
00:46:50.515 --> 00:46:53.980
These models are
overly simple and they

1002
00:46:53.980 --> 00:46:57.265
can't really see the underlying
patterns in the data.

1003
00:46:57.265 --> 00:47:02.035
Overfitting. That's the
high-variance and low bias.

1004
00:47:02.035 --> 00:47:04.930
These models are overly complex,

1005
00:47:04.930 --> 00:47:08.515
and while they can detect
patterns in the training data,

1006
00:47:08.515 --> 00:47:12.250
they're not accurate outside
of the training data.

1007
00:47:12.250 --> 00:47:16.330
So let's consider our
use case as an example.

1008
00:47:16.330 --> 00:47:19.660
Say hypothetically that we
trained our model based

1009
00:47:19.660 --> 00:47:23.590
solely on data from customers
who already had a kindle,

1010
00:47:23.590 --> 00:47:26.020
a prime account, and there was

1011
00:47:26.020 --> 00:47:27.820
a package tracking question

1012
00:47:27.820 --> 00:47:30.220
at some point during
their membership.

1013
00:47:30.220 --> 00:47:33.775
So our model could detect
a pattern that showed that

1014
00:47:33.775 --> 00:47:35.230
say 70 percent of

1015
00:47:35.230 --> 00:47:38.260
prime members call in
about an Amazon device.

1016
00:47:38.260 --> 00:47:40.510
But should the model
used this pattern and

1017
00:47:40.510 --> 00:47:42.910
try to make any
future predictions?

1018
00:47:42.910 --> 00:47:44.920
Well, you'd probably say

1019
00:47:44.920 --> 00:47:47.485
no and if you did,
you'd be correct.

1020
00:47:47.485 --> 00:47:49.780
In this example, the model

1021
00:47:49.780 --> 00:47:52.550
didn't even consider
Alexa related data,

1022
00:47:52.550 --> 00:47:54.330
or what about deep lens,

1023
00:47:54.330 --> 00:47:56.130
or holiday data, or

1024
00:47:56.130 --> 00:47:58.755
any number of other
types of data points.

1025
00:47:58.755 --> 00:48:00.840
Therefore, the model's going to

1026
00:48:00.840 --> 00:48:02.730
be underfitted because it's

1027
00:48:02.730 --> 00:48:04.770
hardly sufficient information to

1028
00:48:04.770 --> 00:48:07.340
predict at a more granular level,

1029
00:48:07.340 --> 00:48:09.100
while prime members are actually

1030
00:48:09.100 --> 00:48:12.590
calling in about
an Amazon device.

1031
00:48:14.100 --> 00:48:18.655
Now, this is an
oversimplified example.

1032
00:48:18.655 --> 00:48:20.230
But the point remains,

1033
00:48:20.230 --> 00:48:22.405
in testing and production,

1034
00:48:22.405 --> 00:48:24.490
our model won't pay attention

1035
00:48:24.490 --> 00:48:26.410
to these other
missing categories,

1036
00:48:26.410 --> 00:48:28.885
it will skew the results towards

1037
00:48:28.885 --> 00:48:32.135
only the data that the model
was actually trained on.

1038
00:48:32.135 --> 00:48:36.104
One technique that
can be used to combat

1039
00:48:36.104 --> 00:48:38.100
underfitting and overfitting

1040
00:48:38.100 --> 00:48:41.205
is called hyperparameter tuning.

1041
00:48:41.205 --> 00:48:44.280
In machine learning,
there are parameters,

1042
00:48:44.280 --> 00:48:46.640
and there are hyperparameters.

1043
00:48:46.640 --> 00:48:49.900
Let's go back to the desk
and pull up the slides.

1044
00:48:49.900 --> 00:48:52.760
Let's talk about
parameters briefly.

1045
00:48:55.320 --> 00:48:59.980
Now, a parameter is internal
of the model and it's

1046
00:48:59.980 --> 00:49:01.795
something the model can learn

1047
00:49:01.795 --> 00:49:04.630
or estimate purely
off of the data.

1048
00:49:04.630 --> 00:49:08.410
An example of a parameter
could be the weight of

1049
00:49:08.410 --> 00:49:10.405
an artificial neural network

1050
00:49:10.405 --> 00:49:13.165
or the coefficients
in linear regression.

1051
00:49:13.165 --> 00:49:16.465
The model has to have
parameters to make predictions,

1052
00:49:16.465 --> 00:49:19.850
and most often, these
aren't set by humans.

1053
00:49:19.850 --> 00:49:22.545
Hyperparameters on
the other hand,

1054
00:49:22.545 --> 00:49:24.630
they're external of the model

1055
00:49:24.630 --> 00:49:27.570
and can't be estimated
from the data.

1056
00:49:27.570 --> 00:49:31.785
Hyperparameters set by
humans, and typically,

1057
00:49:31.785 --> 00:49:33.240
you can't really know

1058
00:49:33.240 --> 00:49:35.835
the best value of
the hyperparameter,

1059
00:49:35.835 --> 00:49:40.130
but you can trial and error
and use that to get there.

1060
00:49:40.130 --> 00:49:43.570
Yeah. Think about hyperparameter
as the knobs, the lever,

1061
00:49:43.570 --> 00:49:44.980
you're going to use those to tune

1062
00:49:44.980 --> 00:49:46.975
the machine learning algorithm,

1063
00:49:46.975 --> 00:49:50.065
and that'll improve
its performance.

1064
00:49:50.065 --> 00:49:53.070
The right hyperparameters
have to be

1065
00:49:53.070 --> 00:49:56.610
chosen for the right
type of problem.

1066
00:49:56.610 --> 00:49:58.965
Here's an example of
a hyperparameter.

1067
00:49:58.965 --> 00:50:00.765
It could be the learning rate

1068
00:50:00.765 --> 00:50:03.395
for training a neural network.

1069
00:50:03.395 --> 00:50:08.090
Let's take a look at different
types of hyperparameters.

1070
00:50:11.310 --> 00:50:14.830
Walking through this part
of the process is one of

1071
00:50:14.830 --> 00:50:16.735
the most effective ways

1072
00:50:16.735 --> 00:50:19.285
of improving your
model's performance.

1073
00:50:19.285 --> 00:50:21.370
So make sure you take the time to

1074
00:50:21.370 --> 00:50:24.470
conduct hyperparameter
tuning thoroughly.

1075
00:50:24.560 --> 00:50:30.105
Speaking of which, now it is
time to train your model.

1076
00:50:30.105 --> 00:50:33.000
The process of training
an ML model involves

1077
00:50:33.000 --> 00:50:34.290
providing your algorithm with

1078
00:50:34.290 --> 00:50:36.585
training data to learn from.

1079
00:50:36.585 --> 00:50:39.930
As mentioned earlier,
for supervised learning,

1080
00:50:39.930 --> 00:50:41.850
the training data must contain

1081
00:50:41.850 --> 00:50:45.045
both the features and
the correct prediction,

1082
00:50:45.045 --> 00:50:46.800
which again we call labels.

1083
00:50:46.800 --> 00:50:49.500
The learning algorithm
finds patterns in

1084
00:50:49.500 --> 00:50:53.190
the training data that maps
the features to the label.

1085
00:50:53.190 --> 00:50:56.805
So when you show the
trained model new inputs,

1086
00:50:56.805 --> 00:51:00.675
it'll return accurately
predicted labels.

1087
00:51:00.675 --> 00:51:03.630
Then you can use
the ML model to get

1088
00:51:03.630 --> 00:51:05.280
predictions on new data

1089
00:51:05.280 --> 00:51:07.290
for which you don't
know the label.

1090
00:51:07.290 --> 00:51:11.100
For example, let's say you
want to train an ML model to

1091
00:51:11.100 --> 00:51:15.525
predict if an email is spam
or not spam. All right.

1092
00:51:15.525 --> 00:51:17.715
You provide your algorithm
with training data,

1093
00:51:17.715 --> 00:51:20.625
It contains emails,
and the known label,

1094
00:51:20.625 --> 00:51:22.845
those labeled tells it whether
it's spam or not spam,

1095
00:51:22.845 --> 00:51:25.200
the algorithm then
trains the model

1096
00:51:25.200 --> 00:51:28.140
using that data resulting
in a model that

1097
00:51:28.140 --> 00:51:30.690
tries to predict whether
a new email which it

1098
00:51:30.690 --> 00:51:34.230
hasn't seen before
is spam or not spam.

1099
00:51:34.230 --> 00:51:37.005
All right. We did

1100
00:51:37.005 --> 00:51:39.870
the same process with
our call center example.

1101
00:51:39.870 --> 00:51:41.850
We passed along features such

1102
00:51:41.850 --> 00:51:43.920
as does the customer
own a kindle,

1103
00:51:43.920 --> 00:51:45.885
yes or no, along with
the appropriate label,

1104
00:51:45.885 --> 00:51:47.685
yes it owned it, no
they don't own it.

1105
00:51:47.685 --> 00:51:52.905
In this case, kindle skill
we put into our algorithm,

1106
00:51:52.905 --> 00:51:54.720
which then learn
the relationships

1107
00:51:54.720 --> 00:51:57.285
between these inputs
and outputs and

1108
00:51:57.285 --> 00:51:59.220
spit out a model that
could extrapolate

1109
00:51:59.220 --> 00:52:02.830
those patterns into
similar data sets.

1110
00:52:03.080 --> 00:52:06.330
As explained earlier, after

1111
00:52:06.330 --> 00:52:09.555
the initial phase of
training your model is done,

1112
00:52:09.555 --> 00:52:13.500
you'll need to evaluate how
accurate that model is by

1113
00:52:13.500 --> 00:52:15.150
using the development
data that you set

1114
00:52:15.150 --> 00:52:17.790
aside and run it
through the model,

1115
00:52:17.790 --> 00:52:19.200
and this is going to tell you how

1116
00:52:19.200 --> 00:52:20.580
well you generalize the models.

1117
00:52:20.580 --> 00:52:22.020
The test data may be fed

1118
00:52:22.020 --> 00:52:25.065
the model for the most
accurate predictions.

1119
00:52:25.065 --> 00:52:27.555
In fact let's circle
back this topic

1120
00:52:27.555 --> 00:52:30.765
of accuracy and precision.

1121
00:52:30.765 --> 00:52:34.335
While you're evaluating you
want to fit the data that

1122
00:52:34.335 --> 00:52:37.965
generalizes more towards
unseen problems.

1123
00:52:37.965 --> 00:52:40.860
Remember from our earlier
discussion about over-fitting,

1124
00:52:40.860 --> 00:52:44.300
you should not fit the
training data to obtain

1125
00:52:44.300 --> 00:52:46.730
the maximum accuracy which

1126
00:52:46.730 --> 00:52:49.010
is kind of weird, kind
of intuitive, right?

1127
00:52:49.010 --> 00:52:52.025
I mean, you want a model
that predicts accurately

1128
00:52:52.025 --> 00:52:55.570
on previously unseen
data, that's true.

1129
00:52:55.570 --> 00:52:59.145
But remember if you train your
model to be too accurate,

1130
00:52:59.145 --> 00:53:03.360
it will be over-fit to that
specific training data.

1131
00:53:03.360 --> 00:53:05.745
For classification problems like

1132
00:53:05.745 --> 00:53:08.160
the call center use case we've
been dealing with all day,

1133
00:53:08.160 --> 00:53:11.310
we're trying to predict if
a new observation will be

1134
00:53:11.310 --> 00:53:13.950
classified as this
customer agent skill

1135
00:53:13.950 --> 00:53:15.705
or that customer agent skill.

1136
00:53:15.705 --> 00:53:18.810
One of the most effective
ways to evaluate

1137
00:53:18.810 --> 00:53:21.765
your model's accuracy, precision,

1138
00:53:21.765 --> 00:53:24.930
and ability to recall
involves looking

1139
00:53:24.930 --> 00:53:28.485
at something called
a confusion matrix.

1140
00:53:28.485 --> 00:53:33.090
Now, the confusion matrix
analyzes the model and

1141
00:53:33.090 --> 00:53:35.310
shows how many of
the data points were

1142
00:53:35.310 --> 00:53:38.055
predicted correctly
and incorrectly.

1143
00:53:38.055 --> 00:53:40.425
So let's take a look here,

1144
00:53:40.425 --> 00:53:42.300
in the bottom right,

1145
00:53:42.300 --> 00:53:47.430
this is the class one box.

1146
00:53:47.430 --> 00:53:50.565
Meaning this represents
all of the true positives,

1147
00:53:50.565 --> 00:53:53.520
you predicted a one
and you got a one.

1148
00:53:53.520 --> 00:53:56.175
So great, for our
call center case

1149
00:53:56.175 --> 00:53:57.690
this could mean of all the things

1150
00:53:57.690 --> 00:53:58.905
you thought you'd predict,

1151
00:53:58.905 --> 00:54:00.330
and your model did predict,

1152
00:54:00.330 --> 00:54:02.520
for example needing
to route customer

1153
00:54:02.520 --> 00:54:04.920
to a specific agent with
strong elected skills,

1154
00:54:04.920 --> 00:54:08.415
your model did this 1,800 times.

1155
00:54:08.415 --> 00:54:10.635
In the top left box,

1156
00:54:10.635 --> 00:54:14.100
this is the class
zero class zero box,

1157
00:54:14.100 --> 00:54:16.050
this is your true negative.

1158
00:54:16.050 --> 00:54:18.615
For instance, with our use case

1159
00:54:18.615 --> 00:54:20.265
you might predict the model

1160
00:54:20.265 --> 00:54:24.585
will not route calls to
the fresh department,

1161
00:54:24.585 --> 00:54:26.400
and the model in fact did not

1162
00:54:26.400 --> 00:54:29.340
route any calls to Amazon Fresh.

1163
00:54:29.340 --> 00:54:31.605
The top right box,

1164
00:54:31.605 --> 00:54:35.385
now this is your class
one class zero box

1165
00:54:35.385 --> 00:54:39.885
where you predicted
in this case a one,

1166
00:54:39.885 --> 00:54:42.420
but you ended up getting a zero,

1167
00:54:42.420 --> 00:54:45.600
and finally last is
this bottom box where

1168
00:54:45.600 --> 00:54:49.320
you predict a zero but
end up getting a one.

1169
00:54:49.320 --> 00:54:55.340
To summarize, accuracy is
the degree of deviation from

1170
00:54:55.340 --> 00:54:57.950
the truth or the total number of

1171
00:54:57.950 --> 00:55:00.650
right predictions divided by

1172
00:55:00.650 --> 00:55:03.155
the total number of predictions.

1173
00:55:03.155 --> 00:55:07.990
Precision is the ability to
reproduce similar results,

1174
00:55:07.990 --> 00:55:10.230
and it's defined by all of

1175
00:55:10.230 --> 00:55:13.050
your true positive
numbers divided

1176
00:55:13.050 --> 00:55:17.110
by true positive
and false positive.

1177
00:55:17.180 --> 00:55:20.805
All right. At this point

1178
00:55:20.805 --> 00:55:22.380
after you've trained
your model and you're

1179
00:55:22.380 --> 00:55:24.450
satisfied with the accuracy

1180
00:55:24.450 --> 00:55:27.510
based on some of the
techniques we've talked about,

1181
00:55:27.510 --> 00:55:30.570
it's best practice
to evaluate how it's

1182
00:55:30.570 --> 00:55:34.555
doing by running it against
a few different algorithms.

1183
00:55:34.555 --> 00:55:37.940
Now, you should consider
running it through

1184
00:55:37.940 --> 00:55:39.290
a couple different algorithms

1185
00:55:39.290 --> 00:55:41.765
within the chosen
algorithm category.

1186
00:55:41.765 --> 00:55:44.540
So if you're working say
with a supervised algorithm

1187
00:55:44.540 --> 00:55:47.390
like our classification
algorithm for the call center,

1188
00:55:47.390 --> 00:55:49.355
you should try the model against

1189
00:55:49.355 --> 00:55:51.965
a different
classification algorithm,

1190
00:55:51.965 --> 00:55:54.665
for example decision
tree algorithm

1191
00:55:54.665 --> 00:55:57.590
or the K nearest
neighbors algorithm.

1192
00:55:57.590 --> 00:56:00.440
But this will give you
a better idea of how

1193
00:56:00.440 --> 00:56:04.200
to get the best fit and the
best results for your model.

1194
00:56:04.970 --> 00:56:09.915
Okay. Deployments,
monitoring, here we are.

1195
00:56:09.915 --> 00:56:11.460
You've prepared your data,

1196
00:56:11.460 --> 00:56:14.205
you've cleaned your data,
you've visualized it.

1197
00:56:14.205 --> 00:56:15.630
You've selected your features,

1198
00:56:15.630 --> 00:56:17.160
you've split your
data test your model,

1199
00:56:17.160 --> 00:56:18.600
you've tuned it several times,

1200
00:56:18.600 --> 00:56:21.150
lets be honest, and after
you've done all that,

1201
00:56:21.150 --> 00:56:22.380
and you're satisfied with

1202
00:56:22.380 --> 00:56:25.080
the model's predictions
on unseen data,

1203
00:56:25.080 --> 00:56:27.060
it's time to deploy
your model into

1204
00:56:27.060 --> 00:56:31.635
production so it can begin
making your predictions.

1205
00:56:31.635 --> 00:56:35.220
One of the primary ML
tools for building,

1206
00:56:35.220 --> 00:56:39.405
training and deploying
models Amazon SageMaker.

1207
00:56:39.405 --> 00:56:43.020
Amazon SageMaker
is fully managed,

1208
00:56:43.020 --> 00:56:45.540
it covers the entire
end-to-end pipeline

1209
00:56:45.540 --> 00:56:46.920
that we've just discussed.

1210
00:56:46.920 --> 00:56:49.320
The build module,
SageMaker provides

1211
00:56:49.320 --> 00:56:52.695
a hosted environment for
you to work with your data,

1212
00:56:52.695 --> 00:56:54.690
you can experiment
with your algorithms,

1213
00:56:54.690 --> 00:56:56.670
you can visualize the output.

1214
00:56:56.670 --> 00:56:59.730
Then the train module
actually takes care

1215
00:56:59.730 --> 00:57:03.060
of the model training and
tuning at high scale.

1216
00:57:03.060 --> 00:57:05.385
Then it has the deploy module,

1217
00:57:05.385 --> 00:57:07.260
designed to provide you

1218
00:57:07.260 --> 00:57:09.585
a managed environment
for you to host,

1219
00:57:09.585 --> 00:57:11.520
test models for inference,

1220
00:57:11.520 --> 00:57:15.405
secure low latency,
the tools are there.

1221
00:57:15.405 --> 00:57:16.980
Now, this additional tool is and

1222
00:57:16.980 --> 00:57:19.980
SageMaker that are going
to help you label data,

1223
00:57:19.980 --> 00:57:21.660
manager your compute costs,

1224
00:57:21.660 --> 00:57:23.325
take care of forecasting,

1225
00:57:23.325 --> 00:57:26.310
and much more that has battle of

1226
00:57:26.310 --> 00:57:29.820
one of our Machine Learning
tech leaders here at Amazon,

1227
00:57:29.820 --> 00:57:31.050
they're going to
discuss in a different

1228
00:57:31.050 --> 00:57:32.355
session later on today,

1229
00:57:32.355 --> 00:57:33.180
you don't want to miss

1230
00:57:33.180 --> 00:57:35.130
that one that one's
going to be important.

1231
00:57:35.130 --> 00:57:38.265
All right. Getting back to
deploying and monitoring.

1232
00:57:38.265 --> 00:57:40.380
You'll want to
remember to monitor

1233
00:57:40.380 --> 00:57:42.240
your production data and

1234
00:57:42.240 --> 00:57:44.535
retrain your model
if it's necessary,

1235
00:57:44.535 --> 00:57:46.785
because a newly deployed model

1236
00:57:46.785 --> 00:57:48.735
needs to reflect current
production date,

1237
00:57:48.735 --> 00:57:50.715
you don't want to
get out of date.

1238
00:57:50.715 --> 00:57:55.215
Since data distributions
can drift over time,

1239
00:57:55.215 --> 00:57:58.395
deploying a model it's
not a onetime exercise,

1240
00:57:58.395 --> 00:58:00.375
it's a continuous process.

1241
00:58:00.375 --> 00:58:01.680
You're not going to
be out of a job.

1242
00:58:01.680 --> 00:58:03.060
It's a good practice,

1243
00:58:03.060 --> 00:58:05.685
you continually monitor
the production data,

1244
00:58:05.685 --> 00:58:07.530
and retrain if you find

1245
00:58:07.530 --> 00:58:09.435
that the production
data distribution

1246
00:58:09.435 --> 00:58:11.850
has deviated significantly from

1247
00:58:11.850 --> 00:58:13.635
the training data distribution,

1248
00:58:13.635 --> 00:58:16.980
no deviation, isn't a change.

1249
00:58:16.980 --> 00:58:20.175
Evaluating in a
production setting

1250
00:58:20.175 --> 00:58:22.050
is a little bit different.

1251
00:58:22.050 --> 00:58:25.350
Now you've got to have a
very concrete success metric

1252
00:58:25.350 --> 00:58:27.825
that you can use to
measure success.

1253
00:58:27.825 --> 00:58:30.225
In our call center use case,

1254
00:58:30.225 --> 00:58:32.490
our routing experiments
were predicated

1255
00:58:32.490 --> 00:58:34.740
on the assumption
that the ability

1256
00:58:34.740 --> 00:58:36.270
to more accurately predict

1257
00:58:36.270 --> 00:58:39.510
skills would reduce the
number of transfers.

1258
00:58:39.510 --> 00:58:41.920
Now, in production we

1259
00:58:41.920 --> 00:58:45.040
can actually put that
assumption to test.

1260
00:58:45.350 --> 00:58:48.000
Well, okay so that takes us to

1261
00:58:48.000 --> 00:58:50.595
the end of them ML pipeline.

1262
00:58:50.595 --> 00:58:52.920
If that felt like a little
bit of a whirlwind,

1263
00:58:52.920 --> 00:58:55.080
that's because it was.

1264
00:58:55.080 --> 00:58:56.940
It has like a tone that goes into

1265
00:58:56.940 --> 00:58:58.695
implementing an ML solution.

1266
00:58:58.695 --> 00:59:00.750
It's a process that
most often takes

1267
00:59:00.750 --> 00:59:02.910
several weeks or months,

1268
00:59:02.910 --> 00:59:04.410
don't be scared of it, we've

1269
00:59:04.410 --> 00:59:06.510
really just skim the surface.

1270
00:59:06.510 --> 00:59:08.160
But now hopefully, you've

1271
00:59:08.160 --> 00:59:10.365
enough of foundation
of the process.

1272
00:59:10.365 --> 00:59:13.410
We talked about the key
terms and processes,

1273
00:59:13.410 --> 00:59:15.480
key terms and concepts to use.

1274
00:59:15.480 --> 00:59:17.190
So the rest of today's events,

1275
00:59:17.190 --> 00:59:19.905
you'll be able to really dive
deeper into the content,

1276
00:59:19.905 --> 00:59:23.925
services, especially tools
that most interest you.

1277
00:59:23.925 --> 00:59:26.580
So with that I'm Blaine Sundrud,

1278
00:59:26.580 --> 00:59:28.545
and I hope you got something
good out of today,

1279
00:59:28.545 --> 00:59:30.270
and have an excellent
rest of the day,

1280
00:59:30.270 --> 00:59:31.380
and I'm going go and
throw it back to you

1281
00:59:31.380 --> 00:59:33.610
all. Have a great day.