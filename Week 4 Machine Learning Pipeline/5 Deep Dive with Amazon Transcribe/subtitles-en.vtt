WEBVTT

1
00:00:04.800 --> 00:00:08.700
In this session, we're going to talk
about the Transcribe service, and

2
00:00:08.700 --> 00:00:12.900
how you can actually improve the
confidence level with the feedback loop.

3
00:00:12.900 --> 00:00:14.200
My name is Ken Shek.

4
00:00:14.200 --> 00:00:19.500
I'm the specialist SA, and
I also have my coworker Emmanuel Etheve.

5
00:00:19.500 --> 00:00:22.600
He is a specialized solution architect.

6
00:00:22.600 --> 00:00:26.600
So before we go into the details,
I wanted to show you

7
00:00:26.600 --> 00:00:30.300
a couple of common-use cases in
the media-and-entertainment field.

8
00:00:30.300 --> 00:00:32.349
If you look at the architecture here,

9
00:00:32.349 --> 00:00:36.900
there are couple use cases that you
could use with our AI/ML services.

10
00:00:36.900 --> 00:00:41.200
For example, you can use
recognition celebrity detections

11
00:00:41.200 --> 00:00:45.900
face match to generate a sequence
of the market information and

12
00:00:45.900 --> 00:00:49.487
you could use that for the video editing
to help the video-editing software.

13
00:00:50.500 --> 00:00:53.600
And also you could use the moderation API

14
00:00:53.600 --> 00:00:57.900
to apply a reduction on any
kind of unsafe content.

15
00:00:57.900 --> 00:01:02.100
And of course you also have
a transcript service that could do

16
00:01:02.100 --> 00:01:06.912
a speech-to-text to generate a subtitle
and which is what we're focusing on today.

17
00:01:06.912 --> 00:01:12.800
And then last, you could also use the
comprehends to extract the sentiments of

18
00:01:12.800 --> 00:01:18.200
the subtitle or you could use translate to
do a multi-language translation as well.

19
00:01:18.200 --> 00:01:21.688
So you can have multiple languages,
different subtitles.

20
00:01:21.688 --> 00:01:25.823
And also at the end you could have
all this metadata that goes into

21
00:01:25.823 --> 00:01:30.575
the Elasticsearch engine and [INAUDIBLE]
you could generate a hot-search

22
00:01:30.575 --> 00:01:32.887
capability for all this metadata.

23
00:01:32.887 --> 00:01:35.511
For example, you could do a search for

24
00:01:35.511 --> 00:01:40.686
flowers that will show all the video
that contains the metadata of flower.

25
00:01:40.686 --> 00:01:45.791
While there are so
many different use cases in the AI/ML

26
00:01:45.791 --> 00:01:51.655
area there are some use cases
more tolerant to a low-confidence

27
00:01:51.655 --> 00:01:55.799
level of result, for example, hot search.

28
00:01:55.799 --> 00:02:00.920
You could do a search for spaceship,
and in return you have a list of videos

29
00:02:00.920 --> 00:02:05.875
that there might be a miss detection,
but that shouldn't be too much of

30
00:02:05.875 --> 00:02:10.856
a problem because it's just a search
in recommendation of the result.

31
00:02:10.856 --> 00:02:15.866
But there are also use cases that
actually require much higher accuracy and

32
00:02:15.866 --> 00:02:18.500
much higher confidence of the result.

33
00:02:18.500 --> 00:02:22.700
For example, if you're generating
a subtitle of the Transcribe,

34
00:02:22.700 --> 00:02:27.100
what you really want it to do is you want
it to have a much higher accuracy and

35
00:02:27.100 --> 00:02:29.900
much higher confidence of the subtitle.

36
00:02:29.900 --> 00:02:33.583
And this is why we're doing the talk,
to see,

37
00:02:33.583 --> 00:02:38.778
to help you to train the Transcribe
service with the feedback loop

38
00:02:38.778 --> 00:02:43.991
to get the confidence level and
accuracy better with the result.

39
00:02:43.991 --> 00:02:49.051
So with that I wanted to show you
one of the case studies that we did

40
00:02:49.051 --> 00:02:53.834
which is actually how we started
with this prototype thing is

41
00:02:53.834 --> 00:02:58.898
that there is a company they
are the In-home Fitness company and

42
00:02:58.898 --> 00:03:04.094
what they do is they live stream
their instructor-led classes.

43
00:03:04.094 --> 00:03:08.291
They live stream it to the customer and
what they needed to do is they needed to

44
00:03:08.291 --> 00:03:12.297
automate the process of generating
subtitles and closed captioning for

45
00:03:12.297 --> 00:03:13.105
two reasons.

46
00:03:13.105 --> 00:03:17.793
One is, in the US there is a governance
reason that they need to provide

47
00:03:17.793 --> 00:03:22.640
subtitles and closed caption, and
then the second reason is they wanted

48
00:03:22.640 --> 00:03:27.499
to automate to minimize all these
operation costs and overhead as well.

49
00:03:27.499 --> 00:03:30.350
So with that I'm going to
introduce Emmanuel, and

50
00:03:30.350 --> 00:03:34.260
he is going to walk through the demo
with you, and then after that I will

51
00:03:34.260 --> 00:03:39.000
come back to talk about how the
architecture looks like behind the scene.

52
00:03:39.000 --> 00:03:39.500
Thank you.

53
00:03:41.200 --> 00:03:43.894
Hi everybody, and thank you Ken.

54
00:03:43.894 --> 00:03:49.110
Before we dive into the subtitle
use case that was mentioned

55
00:03:49.110 --> 00:03:54.134
before by Ken,
I'd like to give you a quick disclaimer.

56
00:03:56.097 --> 00:04:01.953
AI/ML tools that we have in the AWS
platform are no magic trick,

57
00:04:01.953 --> 00:04:06.183
so remember that you need
to customize them and

58
00:04:06.183 --> 00:04:12.047
to adapt them to your customer use
case for them to be efficient.

59
00:04:12.047 --> 00:04:16.042
With that said let's have
a look at what we have here, so

60
00:04:16.042 --> 00:04:20.122
this is a platform that can
developed in order to generate

61
00:04:20.122 --> 00:04:23.700
subtitles automatically
from a transcription.

62
00:04:25.200 --> 00:04:27.128
We already have a video here.

63
00:04:27.128 --> 00:04:29.263
I'm just going to upload a second one.

64
00:04:31.754 --> 00:04:34.384
So I'm going to upload this video, so

65
00:04:34.384 --> 00:04:39.000
there's a few steps that we're
going to do when we upload the video.

66
00:04:39.000 --> 00:04:44.023
So we are going to transcode this
video in order to have a proxy format.

67
00:04:44.023 --> 00:04:48.194
Basically, we are just
extracting the audio track

68
00:04:48.194 --> 00:04:52.947
because the video content is of no use for
Transcribe, and

69
00:04:52.947 --> 00:04:58.676
we are also generating the transcription
from from this audio track.

70
00:05:01.952 --> 00:05:05.802
So we can see here
the different steps that we

71
00:05:05.802 --> 00:05:10.497
are going through in order
to generate that content.

72
00:05:10.497 --> 00:05:15.329
Before I look at this specific content,
I'm going to show you a short

73
00:05:15.329 --> 00:05:19.472
video that has already been
processed in our platform and

74
00:05:19.472 --> 00:05:24.062
I'll come back to it afterward
showing you what has been done.

75
00:05:24.062 --> 00:05:28.100
I just want you to pay attention to
the capitalized word in the subtitles.

76
00:05:29.400 --> 00:05:33.900
>> Just wanted to run through
a quick proof-of-concept demo.

77
00:05:33.900 --> 00:05:35.961
So we have Amazon connect.

78
00:05:35.961 --> 00:05:39.998
We have a call coming in that's going to
hit Amazon connect, and the first thing

79
00:05:39.998 --> 00:05:44.176
we're going to do is quickly play a prompt
that says press one to start streaming.

80
00:05:44.176 --> 00:05:46.202
That's going to be done by Polly.

81
00:05:46.202 --> 00:05:50.762
Once the customer presses one, we're
going to associate the call, the customer

82
00:05:50.762 --> 00:05:55.130
audio with a Kinesis video stream and
start actually streaming that audio.

83
00:05:55.130 --> 00:06:00.271
The next I'm going to do is invoke
a Lambda function to take the KBS

84
00:06:00.271 --> 00:06:06.382
details like the ARN, the start fragment
and the timestamp and so forth and

85
00:06:06.382 --> 00:06:12.600
put that in a Dynamo table with the
customer's phone number and contact ID.

86
00:06:12.600 --> 00:06:16.245
I'm going to have a job application on
my computer that I will then start and

87
00:06:16.245 --> 00:06:19.771
the first thing that that Java
application will do is talk to Dynamo and

88
00:06:19.771 --> 00:06:24.017
retrieve the Kinesis information based on
the phone number that I have programmed in

89
00:06:24.017 --> 00:06:25.073
that application.

90
00:06:25.073 --> 00:06:29.145
And then it will actually go out
to Kinesis video streams and

91
00:06:29.145 --> 00:06:33.870
start consuming that stream and
feed it over to Amazon Transcribe and

92
00:06:33.870 --> 00:06:38.024
take the transcription in real
time of the customer audio and

93
00:06:38.024 --> 00:06:41.378
put it out on my console,
so let's give it a try.

94
00:06:41.378 --> 00:06:46.517
>> All right, so what you've seen
here in this particular video also

95
00:06:46.517 --> 00:06:53.400
capitalized word are what we've been
providing through the feedback loop.

96
00:06:53.400 --> 00:06:58.700
So we've built our own dictionary
adding some extra word to

97
00:06:58.700 --> 00:07:04.200
help Transcribe to better
recognize a certain set of words.

98
00:07:04.200 --> 00:07:11.000
If I go back into the content for
instance here, we have Java application.

99
00:07:11.000 --> 00:07:16.506
If we have a look at what
was originally identified

100
00:07:16.506 --> 00:07:20.833
we would have found a job application,

101
00:07:20.833 --> 00:07:26.746
which is quite different
from what we are expecting.

102
00:07:26.746 --> 00:07:29.890
So there are also other
interesting detections here.

103
00:07:32.027 --> 00:07:39.000
If I browse my content, and I would like
you to pay attention to the acronym.

104
00:07:40.400 --> 00:07:44.700
Recognizing an acronym
is a very hard task for

105
00:07:44.700 --> 00:07:48.222
Transcribe because acronyms
are not actual words.

106
00:07:48.222 --> 00:07:52.962
They're abbreviations, and by default

107
00:07:52.962 --> 00:07:58.100
Transcribe we would not be
able to recognize an acronym.

108
00:08:00.100 --> 00:08:05.870
So, we have a way to work
around that cave it by telling

109
00:08:05.870 --> 00:08:11.663
to transcribe to pay attention
to sequence of words.

110
00:08:11.663 --> 00:08:17.221
So when we are going to
create our custom dictionary,

111
00:08:17.221 --> 00:08:21.050
we are going to tell transcribe to pay

112
00:08:21.050 --> 00:08:25.884
attention to the sequence
of K space V space S.

113
00:08:25.884 --> 00:08:30.157
And we are going to spell it
that way into the dictionary,

114
00:08:30.157 --> 00:08:33.284
this way when transcribe
is going to pause.

115
00:08:33.284 --> 00:08:38.133
The audio file is going to look and
wait for this sequence,

116
00:08:38.133 --> 00:08:42.775
and if it matches that sequence,
then it's going to use

117
00:08:42.775 --> 00:08:48.154
the acronym that has been provided
by the custom dictionary.

118
00:08:48.154 --> 00:08:55.098
Before I move to the next video, I'd like
to show you how we achieve this result.

119
00:08:55.098 --> 00:09:00.152
We did it by running several
iteration of the transcribe

120
00:09:00.152 --> 00:09:05.744
service with a different set of
vocabulary in order to improve

121
00:09:05.744 --> 00:09:10.597
the average confidence
level of the transcription.

122
00:09:10.597 --> 00:09:14.200
If you have a look at the result here,

123
00:09:14.200 --> 00:09:19.315
we started with %94.3
of average confidence,

124
00:09:19.315 --> 00:09:24.911
and we ended up with a %95.3
of average confidence.

125
00:09:24.911 --> 00:09:30.646
So we've been able by several iteration.

126
00:09:30.646 --> 00:09:35.523
We've custom set of vocabulary
to improve the average

127
00:09:35.523 --> 00:09:40.514
confidence level of
the transcription by one percent.

128
00:09:40.514 --> 00:09:42.852
So I'm going to move to the next video and

129
00:09:42.852 --> 00:09:45.785
while I explain to you
why this video is useful.

130
00:09:45.785 --> 00:09:50.416
I'm also going to paste here
our custom dictionary that

131
00:09:50.416 --> 00:09:54.166
we're going to use for
this particular video.

132
00:09:54.166 --> 00:09:59.565
And here you can have some
example of the words that we've

133
00:09:59.565 --> 00:10:06.670
identified that's going to help us to
improve the subtitles into the video.

134
00:10:06.670 --> 00:10:10.535
I'm just going to submit it,
and why this is processed?

135
00:10:10.535 --> 00:10:15.669
I would like you to pay attention to
what is being said in this video.

136
00:10:15.669 --> 00:10:18.040
I'm just going to restart it.

137
00:10:23.123 --> 00:10:26.914
>> Okay,
tell us which ws service [CROSSTALK

138
00:10:26.914 --> 00:10:29.146
>> Absolutely, I love young lumber yard,

139
00:10:29.146 --> 00:10:31.846
which is the game engine,
that's really good.

140
00:10:31.846 --> 00:10:34.786
Most like history because
storage is fantastic.

141
00:10:34.786 --> 00:10:37.005
And of course you should
really lam the whole things.

142
00:10:37.005 --> 00:10:39.829
>> Cognito,
cognito [INAUDIBLE] technical way.

143
00:10:39.829 --> 00:10:44.189
Yes Cognito is fantastic, API Gateway,
>> Media convert,

144
00:10:44.189 --> 00:10:46.630
>> Media converters quite nice, but

145
00:10:46.630 --> 00:10:51.183
it's not my favorite obviously
like elastic mapreduce or EMR.

146
00:10:51.183 --> 00:10:54.251
No doubt database services
a good red shift.

147
00:10:54.251 --> 00:10:58.669
I love Aurora and really like all
the different flavors of RDS.

148
00:10:58.669 --> 00:10:59.963
>> Thank you David.

149
00:10:59.963 --> 00:11:02.253
>> You're welcome.

150
00:11:02.253 --> 00:11:08.926
>> So you can see we have a lot
of acronym in this video and

151
00:11:08.926 --> 00:11:16.826
I'd like to go back to the beginning
of the video and to pause.

152
00:11:16.826 --> 00:11:21.535
>> Absolutely, I love young lumber yard,
which is the game engine, that's really

153
00:11:21.535 --> 00:11:26.391
good, most like history because,
>> So here were talking about S3 and

154
00:11:26.391 --> 00:11:29.491
another limitation that we have with

155
00:11:29.491 --> 00:11:33.901
transcribe service is
the recognition of numbers.

156
00:11:33.901 --> 00:11:37.932
It just simply doesn't work currently,
we have transcribed.

157
00:11:37.932 --> 00:11:44.681
So the way around this is
to spell out in a word.

158
00:11:44.681 --> 00:11:47.696
So instead of using the number,

159
00:11:47.696 --> 00:11:52.946
you just going to spell out
the number three in writing so

160
00:11:52.946 --> 00:11:59.777
that transcribe is going to be able
to recognize or identify the number.

161
00:11:59.777 --> 00:12:03.156
So in the particular use case of S3,

162
00:12:03.156 --> 00:12:09.042
because it's an acronym and
a number you need to spell the S, and

163
00:12:09.042 --> 00:12:14.056
spell out the word three and
tell transcribe to look for

164
00:12:14.056 --> 00:12:19.511
this specific sequence in order
to recognize the acronym.

165
00:12:19.511 --> 00:12:27.098
So if I look at the result of
the transcription that has been done here.

166
00:12:27.098 --> 00:12:32.098
You'll be able to see that

167
00:12:32.098 --> 00:12:37.788
S3 has now been identified.

168
00:12:37.788 --> 00:12:40.916
Compared to what has been detected before.

169
00:12:40.916 --> 00:12:46.275
You can also see that
the average confidence level

170
00:12:46.275 --> 00:12:54.321
of the recognition is a lot higher once
we've used as a custom dictionary.

171
00:12:54.321 --> 00:12:59.941
So remember, when you are providing
a demo to your customer,

172
00:12:59.941 --> 00:13:05.902
you need to train your transcribe
in order to get a better result.

173
00:13:05.902 --> 00:13:10.937
You need to adapt it to the use
case of your customer, and

174
00:13:10.937 --> 00:13:18.136
training the transcribe service doesn't
need to happen at the top at all time.

175
00:13:18.136 --> 00:13:23.046
You will only trend the transcribe
service at the beginning, and

176
00:13:23.046 --> 00:13:27.687
once you reach an acceptable
confidence level you will be able

177
00:13:27.687 --> 00:13:32.343
to roll the service into production and
let it work by itself.

178
00:13:32.343 --> 00:13:37.548
You will always need human
interaction to complete

179
00:13:37.548 --> 00:13:42.632
some of the words that
identified by transcribe,

180
00:13:42.632 --> 00:13:47.351
but this tool will be able
to help you to speed up

181
00:13:47.351 --> 00:13:51.120
the transcription of your content.

182
00:13:51.120 --> 00:13:53.330
So I'm going to give
back the stand to Ken and

183
00:13:53.330 --> 00:13:56.500
he will dive deep into what's
happening behind the scene.

184
00:13:58.100 --> 00:14:00.990
>> Thank you in a Emmanuel.

185
00:14:00.990 --> 00:14:07.150
So let us now look at what is
happening behind the scene.

186
00:14:07.150 --> 00:14:11.733
So, as you see in the architecture bill.

187
00:14:11.733 --> 00:14:16.244
When Emmanuel upload the video, this
is the path that is actually happening.

188
00:14:16.244 --> 00:14:18.521
It uploads the video to the S3 buckets.

189
00:14:18.521 --> 00:14:22.587
And then what it does is actually
trigger a state machine, and

190
00:14:22.587 --> 00:14:27.764
then the state machine will actually call
out a Transcoder with me to convert.

191
00:14:27.764 --> 00:14:31.903
And what it does is extract
the audio stream from the video and

192
00:14:31.903 --> 00:14:37.031
then send the audio be streamed to
the transcribe to do the transcription.

193
00:14:37.031 --> 00:14:41.320
So once it was done, the second pass is
where that in a while actually provide

194
00:14:41.320 --> 00:14:43.868
the actual custom
vocabulary dictionary and

195
00:14:43.868 --> 00:14:46.296
then to train the transcribe a little bit.

196
00:14:46.296 --> 00:14:50.785
And this is actually the path that is
going through is it's calling out another

197
00:14:50.785 --> 00:14:53.723
step function state machine and
then what it does.

198
00:14:53.723 --> 00:14:54.971
The next thing is,

199
00:14:54.971 --> 00:14:59.817
it will try to create a vocabulary
dictionary with transcribe surfaces.

200
00:14:59.817 --> 00:15:03.828
And then once this is done,
then it will rerun the transcription

201
00:15:03.828 --> 00:15:08.600
by starting transcription and wait for
the transcription to be completed.

202
00:15:08.600 --> 00:15:13.137
And then at the end we collect
the result from the transcribe and

203
00:15:13.137 --> 00:15:18.369
do our conversion from the transcribe
result into the webvtt subtitle.

204
00:15:18.369 --> 00:15:23.539
Also, there is one thing that I wanted to
mention is that, in this particular demo.

205
00:15:23.539 --> 00:15:27.076
We also uses another service called IoT,
and

206
00:15:27.076 --> 00:15:31.365
the way that we use IoT,
we use IoT as a message broker.

207
00:15:31.365 --> 00:15:36.353
And what it does is a publish/subscribe
surface that will actually

208
00:15:36.353 --> 00:15:41.163
post all the messages from the backend
to IoT and then you will have

209
00:15:41.163 --> 00:15:46.171
all these web connected clients
that subscribe to the iot surface.

210
00:15:46.171 --> 00:15:49.855
We'll be able to get all these
messages asynchronously and

211
00:15:49.855 --> 00:15:51.409
at the same time as well.

212
00:15:51.409 --> 00:15:56.830
Let's move on to look into the actual
implementation of the state machine.

213
00:15:56.830 --> 00:15:58.300
What you're looking at right now
is the transcoding state machine,

214
00:16:00.042 --> 00:16:03.200
As I mentioned earlier, the transcoding
state machine does two things.

215
00:16:03.200 --> 00:16:09.093
One, is it go through the media converter
to extract the audio bit stream.

216
00:16:09.093 --> 00:16:12.145
And then second,
once the audio bit stream is extracted,

217
00:16:12.145 --> 00:16:14.600
it will send audio to transcribe surface.

218
00:16:14.600 --> 00:16:20.945
So transcribe will generate
that transcription result.

219
00:16:20.945 --> 00:16:22.656
This is exactly what is happening.

220
00:16:22.656 --> 00:16:27.162
There is a start transcode job, and
then we'll wait for the job to complete.

221
00:16:27.162 --> 00:16:31.545
So once the transcode is completed,
what we actually do is we are actually

222
00:16:31.545 --> 00:16:34.514
calling out invoke the next
sub- state machine,

223
00:16:34.514 --> 00:16:37.995
which is the state machine
that doing the transcription.

224
00:16:37.995 --> 00:16:42.518
And why we are doing that is because
then you could actually reuse

225
00:16:42.518 --> 00:16:46.808
the same state machine and
build other use cases on top of it.

226
00:16:46.808 --> 00:16:50.095
So let's look at
the transcribe state machine.

227
00:16:50.095 --> 00:16:54.748
What you're seeing here is when we
start to transcribe state machine,

228
00:16:54.748 --> 00:16:58.547
the very first thing that it
check is to see whether there is

229
00:16:58.547 --> 00:17:01.973
a custom vocabulary or
dictionary being provided.

230
00:17:01.973 --> 00:17:06.052
If it's no, then it will go through just
starting the transcribe and wait for

231
00:17:06.052 --> 00:17:07.826
the transcription to complete.

232
00:17:07.826 --> 00:17:12.150
And if there is a custom dictionary or
vocabulary provided,

233
00:17:12.150 --> 00:17:16.897
what it does is actually it's going
to create the vocabulary set in

234
00:17:16.897 --> 00:17:21.504
the transcribe surface, wait for
the vocabulary to complete.

235
00:17:21.504 --> 00:17:22.805
And once this is done,

236
00:17:22.805 --> 00:17:26.986
then it will go back to where it will
start to transcribe surface as well.

237
00:17:26.986 --> 00:17:30.794
With that, I kind of wanted to show
you how easy it is in the backend with

238
00:17:30.794 --> 00:17:33.300
the Lambda function and step function.

239
00:17:33.300 --> 00:17:39.517
This is how you create
the vocabulary with node.js.

240
00:17:39.517 --> 00:17:42.883
Couple things that I wanted
to highlight is, of course,

241
00:17:42.883 --> 00:17:47.663
you have to give a vocabulary name, and
that vocabulary name needs to be unique.

242
00:17:47.663 --> 00:17:52.946
And if it's not unique, then what you
do is actually instead of creating

243
00:17:52.946 --> 00:17:58.159
that vocabulary in transcribe,
you'll be updating the vocabulary.

244
00:17:58.159 --> 00:18:00.495
And then the way that you do
that is in the parameter.

245
00:18:00.495 --> 00:18:04.347
You will see you set the vocabulary
name equal to the base name.

246
00:18:04.347 --> 00:18:08.589
And then the base name will be
your unique vocabulary name for

247
00:18:08.589 --> 00:18:12.259
that particular video, or
maybe a set of the video.

248
00:18:13.826 --> 00:18:17.988
And then the next is how you actually
start the transcribe service.

249
00:18:17.988 --> 00:18:20.430
Same thing,
there is only few lines of code.

250
00:18:20.430 --> 00:18:25.575
The very first one is you need the audio
file name, and then what you do

251
00:18:25.575 --> 00:18:31.728
is actually you provided the transcription
job name equal to the audio file name.

252
00:18:31.728 --> 00:18:35.600
And at the same time,
if you see here in this particular code,

253
00:18:35.600 --> 00:18:38.653
you also wanted to generate
a unique file name for

254
00:18:38.653 --> 00:18:42.160
every transcription that you
create a transcribe job.

255
00:18:42.160 --> 00:18:47.901
And then after that, you have an option
is if you have custom vocabulary or

256
00:18:47.901 --> 00:18:53.284
dictionary, then you provide that
through the setting parameter.

257
00:18:53.284 --> 00:18:55.242
And then you start the transcription.

258
00:18:55.242 --> 00:19:00.331
And then the transcribe surface will
look into this parameter to see

259
00:19:00.331 --> 00:19:06.133
whether that you have a custom vocabulary
to use to make it to pay attention and

260
00:19:06.133 --> 00:19:08.470
pay focus on those word or not.

261
00:19:08.470 --> 00:19:12.885
So the next one that I wanted to show
you is the transcription result from

262
00:19:12.885 --> 00:19:13.996
the transcribe.

263
00:19:13.996 --> 00:19:18.901
A couple things that I wanted to
highlight is that you will get a list of

264
00:19:18.901 --> 00:19:23.722
items in an array, and then there
it will contains the start time and

265
00:19:23.722 --> 00:19:25.853
end times of a specific word.

266
00:19:25.853 --> 00:19:29.240
And you also get a confidence
level in the content.

267
00:19:29.240 --> 00:19:35.090
For example, in this case, tell have
the confidence level of 100%, 1.0.

268
00:19:35.090 --> 00:19:41.101
And then the start time is 0 and
end time is 0.25.

269
00:19:41.101 --> 00:19:47.466
And how we actually take this result and
convert it into the webptt subtitle.

270
00:19:47.466 --> 00:19:53.773
If you looked at the webptt subtitle
format, you also have a start time and

271
00:19:53.773 --> 00:19:59.587
end time, then you'll have to tell
us which surface the sentence.

272
00:19:59.587 --> 00:20:03.123
So what we do is we actually taking
the list of items returned from

273
00:20:03.123 --> 00:20:06.592
the transcribe, and
then we'll stitch the words together and

274
00:20:06.592 --> 00:20:10.800
then split it with the start time and
end time of the sentence as well.

275
00:20:10.800 --> 00:20:18.166
So this is how we create a creative
webptt track for the subtitle.

276
00:20:19.686 --> 00:20:25.509
So the next thing I wanted to show
you is the IoT, the message broker.

277
00:20:25.509 --> 00:20:31.560
And as I mentioned earlier, it's
a publish/subscribe surface, and we use it

278
00:20:31.560 --> 00:20:37.266
to actually send a synchronous message
from the backend to the web client.

279
00:20:37.266 --> 00:20:39.876
So the way that you
create is quite simple.

280
00:20:39.876 --> 00:20:41.890
The first thing you do,

281
00:20:41.890 --> 00:20:47.340
create a thing by calling AWS IoT
create things and given a name.

282
00:20:48.762 --> 00:20:51.939
The next thing is you create a IoT policy.

283
00:20:51.939 --> 00:20:54.182
And in this particular policy,

284
00:20:54.182 --> 00:20:59.092
what we do is we're giving it an action
as IoT and resources asterisk.

285
00:20:59.092 --> 00:21:02.038
The third thing is you would
define a message topic.

286
00:21:02.038 --> 00:21:05.182
This is the message topic
that your web clients or

287
00:21:05.182 --> 00:21:08.019
your connected clients will subscribe to,

288
00:21:08.019 --> 00:21:12.940
as well as the backend will be posting
the message to this particular topic.

289
00:21:12.940 --> 00:21:15.373
It could be any name of your choice.

290
00:21:15.373 --> 00:21:20.643
And then, the very last thing is also
very important, is that in order for

291
00:21:20.643 --> 00:21:25.148
the web client, the kinetic
client to subscribe to the topic,

292
00:21:25.148 --> 00:21:29.159
you need to give a permission
to that connected client.

293
00:21:29.159 --> 00:21:33.532
So you attach a policy
to the connected client?

294
00:21:33.532 --> 00:21:35.125
And this is how you do it.

295
00:21:35.125 --> 00:21:39.966
So the way that you attach a policy to
a Kanedo user is to running the command

296
00:21:39.966 --> 00:21:45.200
aws iot attached policy giving it
the policy name that we created earlier.

297
00:21:45.200 --> 00:21:51.186
And then, the target is actually
the unique identifier of the Kanedo user.

298
00:21:53.490 --> 00:21:59.352
So let's look at how the backend is
publishing the message to the IoT.

299
00:21:59.352 --> 00:22:01.527
Here's the little code cipher.

300
00:22:01.527 --> 00:22:05.104
If you look at it,
what you need is you need a topic,

301
00:22:05.104 --> 00:22:07.612
the topic that we defined earlier.

302
00:22:07.612 --> 00:22:09.684
And then, you'll have the payload.

303
00:22:09.684 --> 00:22:11.412
And then, you also have the endpoint.

304
00:22:11.412 --> 00:22:13.923
The endpoint is the IoT endpoint.

305
00:22:13.923 --> 00:22:16.493
And then,
what you do is you just call publish.

306
00:22:16.493 --> 00:22:19.356
And then, providing that topic and
the payload.

307
00:22:19.356 --> 00:22:21.876
In this case,
is the text message is the payload.

308
00:22:23.308 --> 00:22:27.907
Next, then, we will look into how
you're connected web client will

309
00:22:27.907 --> 00:22:30.224
actually subscribe to the topic.

310
00:22:30.224 --> 00:22:35.810
Before we do that, I highly recommend
that you download the IoT JavaScript SDK.

311
00:22:35.810 --> 00:22:37.986
Here's the link from the GitHub.

312
00:22:37.986 --> 00:22:42.019
And then what it does is actually it
does all this heavy lifting for you.

313
00:22:42.019 --> 00:22:47.818
And then the way that we use it is we'll
construct an instance of the device,

314
00:22:47.818 --> 00:22:51.547
providing the host,
which is the IoT endpoint.

315
00:22:51.547 --> 00:22:54.235
And then, providing the client's ID.

316
00:22:54.235 --> 00:22:55.163
It could be any name.

317
00:22:55.163 --> 00:22:57.436
In this case, it's kanedouser.

318
00:22:57.436 --> 00:23:01.092
And then, the protocol is WSS,
which is web socket.

319
00:23:01.092 --> 00:23:06.614
And then, providing your usual
credential access key ID secret key and

320
00:23:06.614 --> 00:23:08.093
sections tokens.

321
00:23:09.297 --> 00:23:14.650
Then, what you're going to do is you will
have to listen to two of the messages.

322
00:23:14.650 --> 00:23:16.754
The first message is connect.

323
00:23:16.754 --> 00:23:21.659
So when it's on connect, what you're going
to do is you're going to call a subscribe.

324
00:23:21.659 --> 00:23:24.897
And then, that's where that you
subscribe to a particular topic.

325
00:23:24.897 --> 00:23:27.785
And the topic that we defined earlier.

326
00:23:27.785 --> 00:23:32.010
And when you get the message,
when you subscribe to the message event,

327
00:23:32.010 --> 00:23:36.742
then you will get the message when the
backend is posting the message to the IoT.

328
00:23:36.742 --> 00:23:40.701
And that's the payload is where that you
could actually process the rest of your

329
00:23:40.701 --> 00:23:41.302
workflow.

330
00:23:44.176 --> 00:23:49.681
Now, let's take a look at the other
modules that we use media convert for

331
00:23:49.681 --> 00:23:51.134
the transcoding.

332
00:23:51.134 --> 00:23:52.501
So to create a job,

333
00:23:52.501 --> 00:23:57.387
what you need to do is very first
thing is to create a IM service role.

334
00:23:57.387 --> 00:23:59.434
And we'll talk about why we need that.

335
00:23:59.434 --> 00:24:01.994
And then we'll define a job template.

336
00:24:01.994 --> 00:24:06.404
In our case we are doing
a extracting the audio bit stream,

337
00:24:06.404 --> 00:24:10.098
so the job templates will
be a audio output only.

338
00:24:10.098 --> 00:24:12.637
And then we'll submit the job.

339
00:24:12.637 --> 00:24:15.721
So the first step is to
create the IAM service role.

340
00:24:15.721 --> 00:24:20.374
And why is it important is because you
need to give permission to the media

341
00:24:20.374 --> 00:24:25.045
convert surface to have access to our
S3 buckets as well as API Gateway.

342
00:24:25.045 --> 00:24:31.529
And then the way you do it is through the
command line is, aws iam create-role, and

343
00:24:31.529 --> 00:24:36.636
then giving it the assume role
permission to the middle convert.

344
00:24:36.636 --> 00:24:41.112
And then you attach the S3 policy to
that role as well as the API Gateway.

345
00:24:41.112 --> 00:24:47.039
So what you actually do is actually
you allowing me to convert, to assume,

346
00:24:47.039 --> 00:24:52.793
a role that have access to the S3
buckets as well as calling API Gateway.

347
00:24:54.811 --> 00:24:57.785
Then you define a job template.

348
00:24:57.785 --> 00:25:02.703
So in our demo what we actually
do is we have two outputs.

349
00:25:02.703 --> 00:25:07.126
The very first output called m4a
is the audio only output, and

350
00:25:07.126 --> 00:25:12.976
we do that to subscribe is to actually use
that audio output to transcribe service.

351
00:25:12.976 --> 00:25:17.017
And we also create the second set of
the output, which is the impactful.

352
00:25:17.017 --> 00:25:20.676
And why are we doing it is because
we generate the proxy file.

353
00:25:20.676 --> 00:25:25.455
So you could upload any kind of
video file format, for example,

354
00:25:25.455 --> 00:25:29.701
mlv, mp4, mxf, or wmv file,
and it will process it and

355
00:25:29.701 --> 00:25:35.478
being able to generate a proxy file that
you could play back on the web client.

356
00:25:35.478 --> 00:25:40.365
And here is a simple JSON file that you
can actually scan this QR code to download

357
00:25:40.365 --> 00:25:42.010
the code of the template.

358
00:25:42.010 --> 00:25:44.416
So the last step is to submit a job.

359
00:25:44.416 --> 00:25:50.127
Media convert service actually has a
concept of Parisian per account endpoint.

360
00:25:50.127 --> 00:25:54.312
So what you actually need to do is to
find out the exact endpoint that you can

361
00:25:54.312 --> 00:25:55.066
submit job.

362
00:25:55.066 --> 00:25:59.011
And how you do that,
you will run a command, aws mediaconvert,

363
00:25:59.011 --> 00:26:03.542
describe endpoint, and then providing
the regions, because as I said,

364
00:26:03.542 --> 00:26:06.412
it's region specific and account specific.

365
00:26:06.412 --> 00:26:09.341
And what it does is it will
return the endpoint to you and

366
00:26:09.341 --> 00:26:12.661
that's where the endpoint that
you will have to submit a job.

367
00:26:12.661 --> 00:26:15.635
So to actually submit a job or
create a job,

368
00:26:15.635 --> 00:26:21.177
then you'll run aws mediaconvert
create-job, and then providing the role,

369
00:26:21.177 --> 00:26:26.574
the service role that we created earlier,
to provide access to S3 buckets.

370
00:26:26.574 --> 00:26:28.672
And then we'll provide the endpoint.

371
00:26:28.672 --> 00:26:33.711
The endpoint is from the described
endpoint that we run the command earlier,

372
00:26:33.711 --> 00:26:35.816
as far as providing the region.

373
00:26:35.816 --> 00:26:39.530
So here is the equivalent way
of doing it in the Node.js.

374
00:26:39.530 --> 00:26:41.939
First of all, you create a instance and

375
00:26:41.939 --> 00:26:45.008
call the described endpoint
to get the endpoint.

376
00:26:45.008 --> 00:26:49.111
And then once this is done,
you will use the endpoint to create a new

377
00:26:49.111 --> 00:26:52.935
instance of the media convert,
and then you'll create job.

378
00:26:52.935 --> 00:26:57.510
By doing that what we are actually telling
the media convert instance is that

379
00:26:57.510 --> 00:26:59.941
instead of using the default endpoint,

380
00:26:59.941 --> 00:27:03.674
use the endpoint that we got
from the described endpoint API.

381
00:27:03.674 --> 00:27:09.073
Okay, so there are couple few gotchas
that I wanted to share with you guys.

382
00:27:09.073 --> 00:27:13.285
So transcribe actually support mp4 and
other file format, but

383
00:27:13.285 --> 00:27:16.720
it does have a limitation
of one gigabyte file size.

384
00:27:16.720 --> 00:27:21.551
So if you have an mp4 file over one
gigabyte transcribe with actually

385
00:27:21.551 --> 00:27:23.062
will not process it.

386
00:27:23.062 --> 00:27:28.279
So the way to do it is you actually use
media convert or elastic transcode or

387
00:27:28.279 --> 00:27:33.515
even ffmpeg to simply extract the audio
bit stream from your source file.

388
00:27:33.515 --> 00:27:39.163
Also, transcribe has another limitation
is two hours duration limitation.

389
00:27:39.163 --> 00:27:43.878
Demo doesn't support it today, but
then what you could actually do is you

390
00:27:43.878 --> 00:27:47.377
could actually split a long
file into smaller chunks,

391
00:27:47.377 --> 00:27:52.342
like maybe an hour each, and then
you'll parallel process the transcribe.

392
00:27:52.342 --> 00:27:56.993
Once you get the results back you can
actually stitch the results back together

393
00:27:56.993 --> 00:27:59.828
with some post-processing lambda function.

394
00:27:59.828 --> 00:28:02.463
The third example is working with acronym.

395
00:28:02.463 --> 00:28:07.156
You could actually train transcribe
to recognize the acronyms.

396
00:28:07.156 --> 00:28:11.685
But the way to do it is you actually
providing transcribe a sequence of

397
00:28:11.685 --> 00:28:16.540
letters and asking transcribe to pay
attention to a sequence of letters.

398
00:28:16.540 --> 00:28:23.229
For example, AWS,
then you will say a space w space and s.

399
00:28:23.229 --> 00:28:27.590
In that case then transcribe will actually
pay much more attention in a sequence of

400
00:28:27.590 --> 00:28:29.115
these particular letters.

401
00:28:29.115 --> 00:28:33.511
And then as the result when it when you
get the results back from the transcribe,

402
00:28:33.511 --> 00:28:36.136
you could actually remove
the space yourself or

403
00:28:36.136 --> 00:28:37.924
you could just leave it as it is.

404
00:28:37.924 --> 00:28:41.956
In addition to that you can also
train transcribe to work with number.

405
00:28:41.956 --> 00:28:46.737
For example, if you have S3 or EZ2,
what you really want it to do is to

406
00:28:46.737 --> 00:28:50.141
actually spell out S3,
for example, s space and

407
00:28:50.141 --> 00:28:55.917
then T_H-R-E-E to spell out numbers and
then to create that custom vocabulary.

408
00:28:55.917 --> 00:28:58.958
And then once you get the result,

409
00:28:58.958 --> 00:29:04.170
then you actually have a post
processing lambda function

410
00:29:04.170 --> 00:29:09.598
to do a mapping of mapping S3
back into the s number three and

411
00:29:09.598 --> 00:29:13.307
provide that into the subtitle tracks.

412
00:29:13.307 --> 00:29:19.806
So if accuracy is the top priority,
consider to use Amazon Mechanical Turk

413
00:29:19.806 --> 00:29:24.357
to outsource the air check
the final edits process.

414
00:29:24.357 --> 00:29:29.096
Last but not least, also create
a dictionary or a collections of

415
00:29:29.096 --> 00:29:34.470
vocabulary per instructors or speakers or
could be a sequence of movie.

416
00:29:34.470 --> 00:29:38.886
For example, if you have Star Wars,
Lord of the Ring, you could actually

417
00:29:38.886 --> 00:29:43.448
pre-create those vocabularies and
dictionary, then you can fit that into

418
00:29:43.448 --> 00:29:48.598
the transcript surface to allow it to have
a much better result of the transcription.

419
00:29:48.598 --> 00:29:50.603
This concludes the presentation.

420
00:29:50.603 --> 00:29:54.766
My name is Kenny Shaq and
my colleague Amo El Edith.

421
00:29:54.766 --> 00:29:56.096
Thank you for watching.