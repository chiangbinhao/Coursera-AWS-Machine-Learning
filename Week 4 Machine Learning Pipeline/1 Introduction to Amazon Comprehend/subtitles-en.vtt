WEBVTT

1
00:00:11.590 --> 00:00:13.034
Hi, everybody, thanks for joining.

2
00:00:13.034 --> 00:00:15.453
We're really excited to
introduce you today and

3
00:00:15.453 --> 00:00:19.053
walk you through a new service called
AWS natural language processing.

4
00:00:19.053 --> 00:00:23.565
My name is Nino Bice,
a little bit about myself.

5
00:00:23.565 --> 00:00:27.410
I'm a product manager on the team,
I work with the team of engineers.

6
00:00:27.410 --> 00:00:30.460
And together, we've gone out,
thought extensively around,

7
00:00:30.460 --> 00:00:32.161
what are the problems in the space?

8
00:00:32.161 --> 00:00:35.547
How could we bring a solution to
you that solves those problems?

9
00:00:35.547 --> 00:00:36.680
And we work day in and

10
00:00:36.680 --> 00:00:40.366
day out with our customers to
continue in evolution of a roadmap.

11
00:00:40.366 --> 00:00:44.562
But that's a little bit on myself,
I've been at the AWS about a year now.

12
00:00:44.562 --> 00:00:48.779
The majority of my background is in
data systems and cloud in general, and

13
00:00:48.779 --> 00:00:50.488
semantic data experiences.

14
00:00:50.488 --> 00:00:55.016
So it's why I'll be talking to
you about NLP specifically today.

15
00:00:55.016 --> 00:00:57.701
The course will take you through
service introduction, of course,

16
00:00:57.701 --> 00:00:59.565
we'll talk about some overview and
use cases.

17
00:00:59.565 --> 00:01:03.954
So you can understand not only what it is,
but what it can do for you and

18
00:01:03.954 --> 00:01:05.177
how you can use it.

19
00:01:05.177 --> 00:01:08.658
And I'll take you through a brief
demonstration of our console.

20
00:01:08.658 --> 00:01:11.776
Which is really helpful in
understanding the service, and

21
00:01:11.776 --> 00:01:15.220
even allows you to play with it
with your own information as well.

22
00:01:15.220 --> 00:01:19.674
So before we dive in to introducing
the features of the service,

23
00:01:19.674 --> 00:01:21.494
let's set the stage for.

24
00:01:21.494 --> 00:01:25.771
Why are we here, and why are we
talking about this service in general?

25
00:01:25.771 --> 00:01:30.153
It's really important to understand
that the unstructured text, so

26
00:01:30.153 --> 00:01:33.874
text that is not in a schema and
is not in a relational table.

27
00:01:33.874 --> 00:01:37.043
It's frankly exploding,
It's growing exponentially.

28
00:01:37.043 --> 00:01:40.486
So if you think about the '70s,
'80s, '90s,

29
00:01:40.486 --> 00:01:45.566
a lot of us were inputting information
into computer systems in a structured way.

30
00:01:45.566 --> 00:01:51.401
Forms, we were writing them through
data input, a lot of things like Excel.

31
00:01:51.401 --> 00:01:53.669
This information was
coming in structured and

32
00:01:53.669 --> 00:01:56.179
it was therefore being
stored in a structured way.

33
00:01:56.179 --> 00:01:59.159
And that means that there's a set
of technology that we've built

34
00:01:59.159 --> 00:02:01.933
to allow you to store and
query that at that structured data.

35
00:02:01.933 --> 00:02:05.996
So now we're entering an era where
a lot of information being generated is

36
00:02:05.996 --> 00:02:06.922
unstructured.

37
00:02:06.922 --> 00:02:08.985
So you can think of
things like social media,

38
00:02:08.985 --> 00:02:10.731
you can think of things like Twitter.

39
00:02:10.731 --> 00:02:13.341
You can think of the the way
that your brand company or

40
00:02:13.341 --> 00:02:15.668
service is interacting
with your customers.

41
00:02:15.668 --> 00:02:18.672
So those customers are feeling
like with with Chatbots,

42
00:02:18.672 --> 00:02:21.686
they're interacting with
you in conversational ways.

43
00:02:21.686 --> 00:02:25.430
They're interacting with your brand or
service in comments and reviews.

44
00:02:25.430 --> 00:02:28.825
This is all data that's important and
it's growing exponentially,

45
00:02:28.825 --> 00:02:31.134
because it's easier to
communicate that way.

46
00:02:31.134 --> 00:02:36.173
And more people will continue to do that
value is locked inside of this text.

47
00:02:36.173 --> 00:02:40.294
So to a machine it looks like
a string of unstructured text.

48
00:02:40.294 --> 00:02:44.359
To a brand manager, it looks like what
somebody is saying about their price.

49
00:02:44.359 --> 00:02:47.790
Or the experience they had
staying at a specific hotel.

50
00:02:47.790 --> 00:02:50.039
Or the fact that when
they stayed somewhere,

51
00:02:50.039 --> 00:02:52.714
they really enjoyed the coffee
shop down the street.

52
00:02:52.714 --> 00:02:55.805
These are all these are all
elements of information that

53
00:02:55.805 --> 00:02:58.644
are important to any business,
or really anyone.

54
00:02:58.644 --> 00:03:04.091
So the reason why we're able to bring
something of high value like this today.

55
00:03:04.091 --> 00:03:08.147
Is because of that machine learning and
artificial intelligence.

56
00:03:08.147 --> 00:03:12.548
Text analytics and NLP specifically
has been around for a while.

57
00:03:12.548 --> 00:03:14.349
But it's really been rules-based,

58
00:03:14.349 --> 00:03:16.999
allowing you allowing you
to parse unstructured data.

59
00:03:16.999 --> 00:03:20.153
So you can do things like
keyword counting and sorting.

60
00:03:20.153 --> 00:03:22.191
Now with deep learning models,

61
00:03:22.191 --> 00:03:27.329
were able to train this technology to
bring human-like context and awareness.

62
00:03:27.329 --> 00:03:30.727
To that text extraction,
to that NLP experience.

63
00:03:30.727 --> 00:03:33.583
And the last thing we want to mention,
that's really important.

64
00:03:33.583 --> 00:03:38.360
Is that we've thought deeply around how
to bring this technology to market, so

65
00:03:38.360 --> 00:03:39.944
that it's for everyone.

66
00:03:39.944 --> 00:03:45.176
It doesn't require an advanced skill-set,
or maybe a three-month exercise.

67
00:03:45.176 --> 00:03:49.593
Where you learn about models deeply,
you learn about training models.

68
00:03:49.593 --> 00:03:54.413
This service specifically is brought so
that everyone that works with data today.

69
00:03:54.413 --> 00:03:55.748
With the skills you have today,

70
00:03:55.748 --> 00:03:58.231
can now look at approaching
natural language processing.

71
00:03:58.231 --> 00:04:04.504
That's AI-based, using the skills that
you have, so let's introduce the service.

72
00:04:04.504 --> 00:04:07.593
The service itself offers
five main capabilities and

73
00:04:07.593 --> 00:04:09.383
we'll talk about it this way.

74
00:04:09.383 --> 00:04:13.403
And it's important to remember that all
of these capabilities are based on deep

75
00:04:13.403 --> 00:04:14.007
learning.

76
00:04:14.007 --> 00:04:15.684
The first one is sentiment.

77
00:04:15.684 --> 00:04:21.619
Sentiment allows you to understand whether
what the user is saying is positive or

78
00:04:21.619 --> 00:04:22.500
negative.

79
00:04:22.500 --> 00:04:24.894
Or even neutral,
sometimes that's important as well.

80
00:04:24.894 --> 00:04:28.604
You want to know if there's not sentiment,
that might be a signal.

81
00:04:28.604 --> 00:04:30.204
The next one is entities.

82
00:04:30.204 --> 00:04:34.035
This feature goes through the unstructured
text and extracts entities and

83
00:04:34.035 --> 00:04:35.927
actually categorizes them for you.

84
00:04:35.927 --> 00:04:40.896
So things like people, or things like
organizations will be given a category.

85
00:04:40.896 --> 00:04:43.793
And we'll walk through more
detail what that means.

86
00:04:43.793 --> 00:04:46.519
The third capability
is language detection.

87
00:04:46.519 --> 00:04:50.412
So for a company that has
a multilingual application,

88
00:04:50.412 --> 00:04:53.043
with a multilingual customer base.

89
00:04:53.043 --> 00:04:55.950
You can actually determine
what language the text is in.

90
00:04:55.950 --> 00:04:58.962
So you know if you have to
translate the text itself, or

91
00:04:58.962 --> 00:05:01.856
take some other kind of
business action on the text.

92
00:05:01.856 --> 00:05:05.720
The fourth capability is key phrase,
think of this as noun phrases.

93
00:05:05.720 --> 00:05:09.404
So where entities are extracted,
is maybe proper nouns.

94
00:05:09.404 --> 00:05:12.970
The key phrase will catch everything
else from the unstructured text, so

95
00:05:12.970 --> 00:05:15.170
you actually can go
deeper into the meaning.

96
00:05:15.170 --> 00:05:17.019
What were they saying about the person?

97
00:05:17.019 --> 00:05:20.261
What were they saying about
the organization for example?

98
00:05:20.261 --> 00:05:23.084
And then the fifth capability
is topic modeling.

99
00:05:23.084 --> 00:05:26.250
Topic modeling works over
a large corpus of documents.

100
00:05:26.250 --> 00:05:30.792
And helps you do things like organize them
into the topics contained within those

101
00:05:30.792 --> 00:05:31.538
documents.

102
00:05:31.538 --> 00:05:35.522
So it's really nice for organization and
information management.

103
00:05:35.522 --> 00:05:40.085
So let's talk a little bit deeper around
the APIs that help you text analysis.

104
00:05:40.085 --> 00:05:41.604
In the example on the left,

105
00:05:41.604 --> 00:05:44.716
you can see that we have
a snippet of unstructured text.

106
00:05:44.716 --> 00:05:48.949
This may have come in through a comment,
or it was maybe mentioned somewhere.

107
00:05:48.949 --> 00:05:52.016
And you can see what the four
APIs are doing here.

108
00:05:52.016 --> 00:05:55.150
The first one is extracting
the named entity, so

109
00:05:55.150 --> 00:05:58.215
amazon.com is extracted
as an organization.

110
00:05:58.215 --> 00:06:01.639
Seattle, of course,
is extracted as a location.

111
00:06:01.639 --> 00:06:06.192
You can see that we extract noun-based
phrases, or things like everyone,

112
00:06:06.192 --> 00:06:07.981
great customer experience.

113
00:06:07.981 --> 00:06:11.265
We know that the sentiment on
the last sentence is positive.

114
00:06:11.265 --> 00:06:14.816
because of course everybody loves
the great customer experience,

115
00:06:14.816 --> 00:06:16.830
it's generally a positive thing.

116
00:06:16.830 --> 00:06:20.829
And of course we've determined this
snippet of text is English, of course,

117
00:06:20.829 --> 00:06:22.129
because it's English.

118
00:06:22.129 --> 00:06:25.742
The fifth capability that we've
talked about is topic modeling.

119
00:06:25.742 --> 00:06:27.772
So topic modeling, what we've done is,

120
00:06:27.772 --> 00:06:30.450
we've actually brought topic
modeling as a service.

121
00:06:30.450 --> 00:06:34.166
So for those that aren't familiar,
topic modeling is doable today.

122
00:06:34.166 --> 00:06:37.855
It's based on an algorithm called
Latent Dirichlet Allocation, LDA, and

123
00:06:37.855 --> 00:06:39.592
it's been kind of hard to go set up.

124
00:06:39.592 --> 00:06:44.163
You have to go find an environment,
there's a lot of parameters to tune.

125
00:06:44.163 --> 00:06:48.490
You have to obviously deploy and operate
that environment, to run that algorithm.

126
00:06:48.490 --> 00:06:52.751
Our team has done a lot of heavy lifting
to make that algorithm available to you as

127
00:06:52.751 --> 00:06:53.526
a simple API.

128
00:06:53.526 --> 00:06:56.119
So we can think about topic
modeling as a service.

129
00:06:56.119 --> 00:06:59.859
So you can just walk up,
bring your documents and start using it.

130
00:06:59.859 --> 00:07:02.954
This service works by
extracting up to 100 topics.

131
00:07:02.954 --> 00:07:05.008
A topic is a keyword bucket, so

132
00:07:05.008 --> 00:07:09.442
you can see what's in the actual
corpus of documents themselves.

133
00:07:09.442 --> 00:07:13.793
And then the service also returns
to you an automatic view which maps

134
00:07:13.793 --> 00:07:15.485
documents to the topics.

135
00:07:15.485 --> 00:07:20.027
So to give you a really basic use case,
you can take a thousand blog posts.

136
00:07:20.027 --> 00:07:24.751
Understand what's in the blog post,
from a top 100 topic perspective.

137
00:07:24.751 --> 00:07:28.627
And then actually map all the blog
posts into those topic buckets.

138
00:07:28.627 --> 00:07:33.065
So if you wanted to give your users
a really easy way to to explore or

139
00:07:33.065 --> 00:07:34.771
browse your blog posts.

140
00:07:34.771 --> 00:07:36.493
Based on the topics they're interested in.

141
00:07:36.493 --> 00:07:42.489
You could do this with a simple call to
this job, and the job service itself.

142
00:07:42.489 --> 00:07:46.031
The next thing we'll talk about
is what gets us really excited,

143
00:07:46.031 --> 00:07:47.812
is why the service is valuable?

144
00:07:47.812 --> 00:07:50.654
So like I said,
NLP has been around for a while.

145
00:07:50.654 --> 00:07:54.750
There's a lot of folks
doing NLP that's AI-based.

146
00:07:54.750 --> 00:07:58.755
What we've built here today is
a service that's truly accurate.

147
00:07:58.755 --> 00:08:02.981
We have an engineering team, and
a data science team behind this service.

148
00:08:02.981 --> 00:08:06.951
Continually working nonstop
to make the service accurate.

149
00:08:06.951 --> 00:08:10.795
On day one you'll notice that this
service is accurate out of the box, and

150
00:08:10.795 --> 00:08:12.231
it's in its competitive.

151
00:08:12.231 --> 00:08:14.925
And it's useful for
the accuracy that you need for

152
00:08:14.925 --> 00:08:17.245
your use cases that you're dependent on.

153
00:08:17.245 --> 00:08:23.060
It's continuously trained, so as we've
said before, we have folks behind there.

154
00:08:23.060 --> 00:08:27.151
Collecting data, annotating,
training the model, looking for

155
00:08:27.151 --> 00:08:29.350
accuracy problems, fixing them.

156
00:08:29.350 --> 00:08:31.546
We're doing this continuously non-stop.

157
00:08:31.546 --> 00:08:34.829
So the more you use this service,
the more that you'll be able

158
00:08:34.829 --> 00:08:38.506
to have the service become accurate for
you, based on your own data.

159
00:08:38.506 --> 00:08:42.254
And then based on the fact that the team
is training it on your behalf, so

160
00:08:42.254 --> 00:08:44.132
the service gets better over time.

161
00:08:44.132 --> 00:08:48.998
And the service is easy to use, so as
opposed to understanding what a model is.

162
00:08:48.998 --> 00:08:51.786
Or how to think about training a model,
or invoking a model.

163
00:08:51.786 --> 00:08:55.743
You can simply walk up, and
it's included in the AWS SDK.

164
00:08:55.743 --> 00:08:59.208
And you can simply invoke the service,
it's a REST API.

165
00:08:59.208 --> 00:09:03.386
And you could build the service in
conjunction with an AWS analytic

166
00:09:03.386 --> 00:09:04.886
service quite easily.

167
00:09:04.886 --> 00:09:06.481
So now let's dive into a demo show,

168
00:09:06.481 --> 00:09:08.939
you a little bit about what
the service actually does.

169
00:09:08.939 --> 00:09:12.276
And how it works and
we'll show you the console itself.

170
00:09:12.276 --> 00:09:16.860
So let's take a moment to look at the
service, and look at some real examples.

171
00:09:16.860 --> 00:09:21.068
So if you log in to the AWS console,
you'll notice that the database NLP

172
00:09:21.068 --> 00:09:23.893
service comes with a really
nice API explorer.

173
00:09:23.893 --> 00:09:29.839
Where you can enter your own text, or
use example text that we provided for you.

174
00:09:29.839 --> 00:09:35.920
In this particular case, this is the text
that comes with the console itself.

175
00:09:35.920 --> 00:09:40.652
And you can see over here,
the entities that we've extracted.

176
00:09:40.652 --> 00:09:45.134
So you can see amazon.com is
an organization, you can see Seattle,

177
00:09:45.134 --> 00:09:46.921
Washington is a location.

178
00:09:46.921 --> 00:09:51.771
You can even see other organizations
like Starbucks and Boeing.

179
00:09:51.771 --> 00:09:54.666
The next thing that you'll see is
that we've extracted key phrases.

180
00:09:54.666 --> 00:09:58.764
So these are noun-based phrases that
we're extracting from this text, so

181
00:09:58.764 --> 00:10:01.398
some of them are the entities
we've extracted.

182
00:10:01.398 --> 00:10:05.639
But there are also other things, more like
common nouns, like customers, books, and

183
00:10:05.639 --> 00:10:06.225
blenders.

184
00:10:06.225 --> 00:10:11.666
As I mentioned earlier, combining named
entities with their key phrase output.

185
00:10:11.666 --> 00:10:15.229
Really helps you understand
what's in the text, and

186
00:10:15.229 --> 00:10:17.852
what's being referred to in the text.

187
00:10:17.852 --> 00:10:20.451
The next API that we've
mentioned is language detection.

188
00:10:20.451 --> 00:10:26.237
So for this text, you can obviously
see that we're very confident

189
00:10:26.237 --> 00:10:31.936
that the input text is English,
and we've marked it as English.

190
00:10:31.936 --> 00:10:34.577
The fourth API is the sentiment API, so

191
00:10:34.577 --> 00:10:39.948
it sees that this statement that we've
entered here, is relatively neutral.

192
00:10:39.948 --> 00:10:46.533
But if I erase this and I said something
like, I love my Amazon deliveries.

193
00:10:46.533 --> 00:10:48.225
And then I analyze that text,

194
00:10:48.225 --> 00:10:52.366
you can now see that we're very
confident this is a positive statement.

195
00:10:52.366 --> 00:10:57.182
This is a great example of how you can
use that to understand what customers

196
00:10:57.182 --> 00:10:58.044
are saying.

197
00:10:58.044 --> 00:10:59.754
And of course if I went back up here,

198
00:10:59.754 --> 00:11:02.383
I'd see that Amazon
the organization was mentioned.

199
00:11:02.383 --> 00:11:07.812
So you can quite literally understand that
customers mentioning your organization.

200
00:11:07.812 --> 00:11:11.832
They're mentioning it in
a positive sentiment way,

201
00:11:11.832 --> 00:11:16.579
which allows you to really take action,
dive in, learn more.

202
00:11:16.579 --> 00:11:19.475
The fifth API that we've talked about,
is topic modeling.

203
00:11:19.475 --> 00:11:24.275
So as I've mentioned, we've taken
a fairly complex algorithm like LDA.

204
00:11:24.275 --> 00:11:27.452
And made it available as
a pretty easy-to-use service.

205
00:11:27.452 --> 00:11:31.428
In this case what you can see here,
is that all we require as input,

206
00:11:31.428 --> 00:11:33.706
to run the topic modeling job for you.

207
00:11:33.706 --> 00:11:38.299
Is an S3 bucket that contains a corpus
of your documents and input format.

208
00:11:38.299 --> 00:11:41.991
Which literally just says,
tell us if you're delimiting by line.

209
00:11:41.991 --> 00:11:44.722
Or if each document is its own file.

210
00:11:44.722 --> 00:11:46.510
You can specify the number of topics.

211
00:11:46.510 --> 00:11:50.137
So you might want to take 1,000 documents,
and put them into ten topics.

212
00:11:50.137 --> 00:11:53.394
Or you might want to put them
into the top 100 topics.

213
00:11:53.394 --> 00:11:56.112
The next thing is to provide
a security permission to

214
00:11:56.112 --> 00:11:57.919
access the bucket on your behalf.

215
00:11:57.919 --> 00:12:01.495
Give it a name, so this is just simply so
you could track the job.

216
00:12:01.495 --> 00:12:04.120
And then a location of
where to put the output.

217
00:12:04.120 --> 00:12:07.859
And as I've mentioned before
you'll get to CSV files as output.

218
00:12:07.859 --> 00:12:11.256
One file will show you
what are the topics.

219
00:12:11.256 --> 00:12:15.544
So if you've said show me 100 topics,
we'll show you those 100 topics, and

220
00:12:15.544 --> 00:12:17.452
the keywords associated with them.

221
00:12:17.452 --> 00:12:21.476
And the next output is going to be what
documents are mapping to those topics?

222
00:12:21.476 --> 00:12:25.475
And you can go act on that output,
however you'd like.

223
00:12:25.475 --> 00:12:28.475
So that completes the demo,
this is the console.

224
00:12:28.475 --> 00:12:33.263
We urge you to go in, plug in your
own data, try out the service,

225
00:12:33.263 --> 00:12:35.056
see if it works for you.

226
00:12:35.056 --> 00:12:38.696
So now we're done with the demo,
let's talk about some common patterns.

227
00:12:38.696 --> 00:12:40.773
What are we hearing from our customers,

228
00:12:40.773 --> 00:12:44.145
around where do they want to get
started with their NLP solutions?

229
00:12:44.145 --> 00:12:47.885
And we've really ultimately seen it
that the patterns boil down to these

230
00:12:47.885 --> 00:12:48.631
three areas.

231
00:12:48.631 --> 00:12:51.022
It's really Voice of Customer Analytics.

232
00:12:51.022 --> 00:12:52.392
What are your customers,

233
00:12:52.392 --> 00:12:56.445
what is anyone really generally saying
about your brand product or service?

234
00:12:56.445 --> 00:13:00.668
These are really important in
understanding if the new product you've

235
00:13:00.668 --> 00:13:01.660
just launched.

236
00:13:01.660 --> 00:13:03.170
How are people perceiving it?

237
00:13:03.170 --> 00:13:05.761
Do they like the price?

238
00:13:05.761 --> 00:13:08.094
Do they do they think
that the color is off?

239
00:13:08.094 --> 00:13:10.760
These are really important
things that you want to know,

240
00:13:10.760 --> 00:13:13.093
that you can capture from
the Voice of Customer.

241
00:13:13.093 --> 00:13:14.951
This can be from social media,

242
00:13:14.951 --> 00:13:19.103
this can be from comments that
they're leaving on a site somewhere.

243
00:13:19.103 --> 00:13:22.871
This can this can be from emails that
they're sending to your company directly.

244
00:13:22.871 --> 00:13:26.902
It could even be support conversations
that your agents are noting within

245
00:13:26.902 --> 00:13:28.080
support call notes.

246
00:13:28.080 --> 00:13:31.359
The next general pattern that
we see is Semantic Search.

247
00:13:31.359 --> 00:13:32.900
So for example,
if you're an elastic search customer.

248
00:13:32.900 --> 00:13:37.200
And you're currently indexing a corpus of
documents to make them available to users.

249
00:13:37.200 --> 00:13:42.300
You can actually use the NLP service
to extract things like topics,

250
00:13:42.300 --> 00:13:43.500
key phrases and entities.

251
00:13:43.500 --> 00:13:47.003
And also index on those as well.

252
00:13:47.003 --> 00:13:50.163
So your customers can get a better
natural search experience.

253
00:13:50.163 --> 00:13:54.152
You could suggest other documents
from the search experience based on

254
00:13:54.152 --> 00:13:56.659
topic contained within the search result.

255
00:13:56.659 --> 00:14:01.298
It just makes search better, understanding
what's in the documents themselves outside

256
00:14:01.298 --> 00:14:02.711
of just a keyword context.

257
00:14:02.711 --> 00:14:05.307
The third pattern is
Knowledge Management Discovery.

258
00:14:05.307 --> 00:14:08.747
So we hear a lot of customers say,
I want to take a big corpus and

259
00:14:08.747 --> 00:14:09.703
organize them.

260
00:14:09.703 --> 00:14:11.654
I want to understand
what's in these documents.

261
00:14:11.654 --> 00:14:15.837
I've got a variety of use
cases from making this

262
00:14:15.837 --> 00:14:19.191
document corpus easier to navigate.

263
00:14:19.191 --> 00:14:22.281
All the way to, we're really looking for
what's contained in these documents.

264
00:14:22.281 --> 00:14:26.214
To make sure that we're meeting certain
standards around what information can be

265
00:14:26.214 --> 00:14:27.298
stored in documents.

266
00:14:27.298 --> 00:14:33.303
So we see a lot of customers using NLP
in these these three general patterns.

267
00:14:33.303 --> 00:14:37.789
Let's now take a look at an example
of how you would use this NLP

268
00:14:37.789 --> 00:14:41.426
service in context of
an AWS analytics solution.

269
00:14:41.426 --> 00:14:45.783
In this case, we're going to talk
about a social analytics application.

270
00:14:45.783 --> 00:14:49.599
So on the very left of
the diagram we'll have tweets.

271
00:14:49.599 --> 00:14:53.679
Let's pretend that we have a bunch of
customers tweeting about our brand service

272
00:14:53.679 --> 00:14:54.342
or product.

273
00:14:54.342 --> 00:14:59.803
We've set up a Kinesis Firehose which is
calling the the the Twitter search API.

274
00:14:59.803 --> 00:15:02.547
And it's pulling in tweets
that we've set to filter out,

275
00:15:02.547 --> 00:15:04.119
that we think is pertinent to us.

276
00:15:04.119 --> 00:15:09.021
We're then running those tweets through
the NLP service to extract things

277
00:15:09.021 --> 00:15:11.213
like the entities in the tweets.

278
00:15:11.213 --> 00:15:15.030
Or the sentiment of the tweets, or
even the key phrases in the tweets.

279
00:15:15.030 --> 00:15:18.508
We might even be determining what is
the language that the tweets are in.

280
00:15:18.508 --> 00:15:22.227
So we really understand more about where
our customer base is in the world and

281
00:15:22.227 --> 00:15:23.400
what they're saying.

282
00:15:23.400 --> 00:15:26.317
So we'll run all those tweets
through the NLP service and

283
00:15:26.317 --> 00:15:27.971
we'll store them into a store.

284
00:15:27.971 --> 00:15:32.734
We could use a relational service in
this case, or we could use Amazon S3.

285
00:15:32.734 --> 00:15:36.439
We've written all the output from
the NLP service into S3, and

286
00:15:36.439 --> 00:15:39.674
now we can just take a query
analytics tool like Athena.

287
00:15:39.674 --> 00:15:42.458
And start to query and
analyze the NLP output.

288
00:15:42.458 --> 00:15:45.301
So for example, once we query that data,

289
00:15:45.301 --> 00:15:48.970
we can then build views
inside of Amazon QuickSight.

290
00:15:48.970 --> 00:15:50.827
That shows us things like,

291
00:15:50.827 --> 00:15:56.246
who is mentioning other organizations
when they're tweeting about my brand?

292
00:15:56.246 --> 00:16:00.639
Who is mentioning my brand or my service
or my product in a negative context,

293
00:16:00.639 --> 00:16:01.199
and why?

294
00:16:01.199 --> 00:16:04.787
What are the what are the keywords that
they're using are the key phrases they're

295
00:16:04.787 --> 00:16:06.404
using when they talk about my brand?

296
00:16:06.404 --> 00:16:09.372
This could allow us to do
a variety of things, like, hey,

297
00:16:09.372 --> 00:16:10.717
in this part of the world.

298
00:16:10.717 --> 00:16:14.465
Customers are interpreting the product
we've just launched as maybe too

299
00:16:14.465 --> 00:16:15.134
expensive.

300
00:16:15.134 --> 00:16:20.500
So bringing the NLP service together
with AWS analytics capabilities.

301
00:16:20.500 --> 00:16:24.186
Allows you to really do
text analytics at scale for

302
00:16:24.186 --> 00:16:28.762
a wide variety of scenarios,
in this case social analytics.

303
00:16:28.762 --> 00:16:32.680
So thanks for attending the course
on it the new AWS NLP service.

304
00:16:32.680 --> 00:16:35.396
We're so excited to see what
you can do with the service and

305
00:16:35.396 --> 00:16:37.012
the solutions that you can build.

306
00:16:37.012 --> 00:16:40.174
It's really easy to get started,
we've offered a free tier.

307
00:16:40.174 --> 00:16:42.640
So there's no cost to you
to use your own data,

308
00:16:42.640 --> 00:16:45.562
we've even provided some
sample data in the console.

309
00:16:45.562 --> 00:16:47.944
So once again, I'm Nino Bice,
on behalf of the team.

310
00:16:47.944 --> 00:16:52.862
Thanks for considering the AWS NLP,
thanks for watching.

311
00:16:52.862 --> 00:16:53.362
[MUSIC]