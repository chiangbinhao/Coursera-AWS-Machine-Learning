WEBVTT

1
00:00:05.270 --> 00:00:08.430
Hi. My name is
[inaudible] and I'm

2
00:00:08.430 --> 00:00:11.100
a senior solutions
architect at AWS.

3
00:00:11.100 --> 00:00:14.985
Welcome to the course on
Amazon Rekognition Deep Dive.

4
00:00:14.985 --> 00:00:17.220
So in this course,
we will talk about

5
00:00:17.220 --> 00:00:19.870
some of the core features
of Amazon Rekognition,

6
00:00:19.870 --> 00:00:22.320
then we'll cover the API and how

7
00:00:22.320 --> 00:00:24.810
you can use those to
build applications,

8
00:00:24.810 --> 00:00:27.870
we will then cover some
of the use cases along

9
00:00:27.870 --> 00:00:29.370
with reference architectures on

10
00:00:29.370 --> 00:00:30.980
how to build those use cases,

11
00:00:30.980 --> 00:00:32.150
and we will then wrap up

12
00:00:32.150 --> 00:00:34.760
the course of its on
the best practices.

13
00:00:34.760 --> 00:00:37.310
Before we talk about
Amazon Rekognition,

14
00:00:37.310 --> 00:00:38.900
let's quickly talk about what is

15
00:00:38.900 --> 00:00:41.635
computer vision and
why is it important.

16
00:00:41.635 --> 00:00:44.690
So here's an example
image but as a human,

17
00:00:44.690 --> 00:00:46.655
we can easily see that
there is a person,

18
00:00:46.655 --> 00:00:48.695
the person is on a mountain bike,

19
00:00:48.695 --> 00:00:51.200
and we see there are
rocks there and so on.

20
00:00:51.200 --> 00:00:53.795
But if you think about
it behind the scene,

21
00:00:53.795 --> 00:00:57.200
only what computer see
is a bunch of numbers.

22
00:00:57.200 --> 00:00:59.640
So if you take any
pixel of this image,

23
00:00:59.640 --> 00:01:02.165
all we have is behind
the seeing a bunch of

24
00:01:02.165 --> 00:01:04.550
numbers which computers have

25
00:01:04.550 --> 00:01:06.215
no idea what is in that image.

26
00:01:06.215 --> 00:01:08.330
Whereas as human, it is very

27
00:01:08.330 --> 00:01:10.560
easy for us to see
different objects,

28
00:01:10.560 --> 00:01:13.360
what is in the scene, and
all activities and so on.

29
00:01:13.360 --> 00:01:16.175
So the goal here is using

30
00:01:16.175 --> 00:01:18.050
a machine learning models to be

31
00:01:18.050 --> 00:01:20.180
able to detect what
are the objects,

32
00:01:20.180 --> 00:01:21.530
what are the scene,
what are some of

33
00:01:21.530 --> 00:01:23.555
the activities in these images,

34
00:01:23.555 --> 00:01:26.635
and be able to make
sense of this content.

35
00:01:26.635 --> 00:01:29.790
To detect different
objects, different scene,

36
00:01:29.790 --> 00:01:31.530
and different activities,

37
00:01:31.530 --> 00:01:34.785
generally you have to build
a machine learning models.

38
00:01:34.785 --> 00:01:37.610
To do that, you need
lots and lots of data.

39
00:01:37.610 --> 00:01:39.230
So for example, if you want to

40
00:01:39.230 --> 00:01:42.800
detect cats in an image or if
there are dogs in an image,

41
00:01:42.800 --> 00:01:44.480
you need lots and
lots of pictures of

42
00:01:44.480 --> 00:01:46.310
cats and dogs and that's where

43
00:01:46.310 --> 00:01:48.170
application services more

44
00:01:48.170 --> 00:01:50.435
specifically
recognition comes in.

45
00:01:50.435 --> 00:01:52.310
So before we talk
about Rekognition,

46
00:01:52.310 --> 00:01:53.480
let's quickly talk about

47
00:01:53.480 --> 00:01:55.345
the Amazons and Machine
Learning stack.

48
00:01:55.345 --> 00:01:57.500
These are the
services that Amazon

49
00:01:57.500 --> 00:01:59.915
has to make Machine Learning and

50
00:01:59.915 --> 00:02:02.090
AI really easy and basically

51
00:02:02.090 --> 00:02:04.775
democratize AI and ML
for organizations.

52
00:02:04.775 --> 00:02:06.440
At the bottom layer after stack,

53
00:02:06.440 --> 00:02:08.885
we have frameworks
and interfaces.

54
00:02:08.885 --> 00:02:10.220
So this is where you can use

55
00:02:10.220 --> 00:02:12.140
things like AWS deep learning

56
00:02:12.140 --> 00:02:16.100
AMI and run any of the
frameworks that you want,

57
00:02:16.100 --> 00:02:17.120
things like whether it's

58
00:02:17.120 --> 00:02:20.375
TensorFlow and whether
it's MXNet, Caffe2,

59
00:02:20.375 --> 00:02:22.130
you name it, you can run any of

60
00:02:22.130 --> 00:02:25.685
those frameworks and build
your machine learning models.

61
00:02:25.685 --> 00:02:27.185
At the mid-level of the stack,

62
00:02:27.185 --> 00:02:29.120
is platform services and

63
00:02:29.120 --> 00:02:31.310
that's where we have
services like SageMaker.

64
00:02:31.310 --> 00:02:35.030
SageMaker makes it very easy
for developers and data

65
00:02:35.030 --> 00:02:37.530
scientists to build train

66
00:02:37.530 --> 00:02:39.825
and then host their
machine learning models.

67
00:02:39.825 --> 00:02:42.045
At the top layer of the stack,

68
00:02:42.045 --> 00:02:43.640
we have application services.

69
00:02:43.640 --> 00:02:47.450
So this is where we at
Amazon built and train

70
00:02:47.450 --> 00:02:49.280
machine-learning models
whether this is for

71
00:02:49.280 --> 00:02:52.340
computer vision or language
services and so on,

72
00:02:52.340 --> 00:02:54.995
things like transcribe and
translate and we deploy

73
00:02:54.995 --> 00:02:56.570
those machine learning models in

74
00:02:56.570 --> 00:02:59.420
highly available and highly
scalable environment,

75
00:02:59.420 --> 00:03:02.725
and wrap those models with

76
00:03:02.725 --> 00:03:06.075
APIs and that's how we
expose those models to you.

77
00:03:06.075 --> 00:03:07.329
So as a developer,

78
00:03:07.329 --> 00:03:10.310
without knowing anything about
AI and machine learning,

79
00:03:10.310 --> 00:03:12.620
you can take advantage
of these services and

80
00:03:12.620 --> 00:03:15.110
build pretty powerful
smart applications.

81
00:03:15.110 --> 00:03:17.270
Before we look at some
of the core features,

82
00:03:17.270 --> 00:03:18.605
here are some of the customers

83
00:03:18.605 --> 00:03:20.150
of Amazon recognition who are

84
00:03:20.150 --> 00:03:21.695
leveraging the service today

85
00:03:21.695 --> 00:03:23.480
for a variety of use cases and,

86
00:03:23.480 --> 00:03:25.070
we'll talk a little
bit more about some of

87
00:03:25.070 --> 00:03:27.065
these use cases farther
down in the course.

88
00:03:27.065 --> 00:03:28.885
Let's now dig into

89
00:03:28.885 --> 00:03:31.400
Rekognition as this
course is focused on

90
00:03:31.400 --> 00:03:33.590
computer vision use cases
and see what are some

91
00:03:33.590 --> 00:03:35.890
of the core features that
Rekognition provide.

92
00:03:35.890 --> 00:03:39.000
So Amazon Rekognition is
a deep learning base,

93
00:03:39.000 --> 00:03:41.300
image, and video
analysis service.

94
00:03:41.300 --> 00:03:43.580
When we talk about
image analysis,

95
00:03:43.580 --> 00:03:46.310
it can do things like
object and scene detection,

96
00:03:46.310 --> 00:03:49.760
it can do facial analysis,
facial recognition,

97
00:03:49.760 --> 00:03:53.000
it can do things like
celebrity Rekognition as

98
00:03:53.000 --> 00:03:56.320
well as detect unsafe
content and images,

99
00:03:56.320 --> 00:04:00.065
it also has the ability
to detect text in images.

100
00:04:00.065 --> 00:04:02.765
When we talk about video,

101
00:04:02.765 --> 00:04:04.970
again Amazon Rekognition can not

102
00:04:04.970 --> 00:04:06.920
only detect objects and seen like

103
00:04:06.920 --> 00:04:10.800
in images but it can also have
the time axis information.

104
00:04:10.800 --> 00:04:13.070
So it can detect
activity for which

105
00:04:13.070 --> 00:04:16.030
usually you need more
than just one frame and

106
00:04:16.030 --> 00:04:17.330
usually you have to
look at a couple of

107
00:04:17.330 --> 00:04:18.770
frames to see what is going

108
00:04:18.770 --> 00:04:21.895
on at that time in that
video for example.

109
00:04:21.895 --> 00:04:23.775
Along the same lines as image,

110
00:04:23.775 --> 00:04:25.370
a Rekognition video can do

111
00:04:25.370 --> 00:04:27.290
things like celebrity
Rekognition,

112
00:04:27.290 --> 00:04:29.885
it can do content
moderation along with

113
00:04:29.885 --> 00:04:32.585
face detection as
well as pathing,

114
00:04:32.585 --> 00:04:35.930
where it can track the
path that different people

115
00:04:35.930 --> 00:04:37.700
took in that video and there's

116
00:04:37.700 --> 00:04:39.650
very interesting use
cases that you can build.

117
00:04:39.650 --> 00:04:42.260
For example, looking at
sounder sports videos

118
00:04:42.260 --> 00:04:45.050
and we'll talk about some of
those down in the use cases.

119
00:04:45.050 --> 00:04:46.250
Let's now dig through each of

120
00:04:46.250 --> 00:04:47.960
these features and talk a

121
00:04:47.960 --> 00:04:49.880
little bit more about
how these work.

122
00:04:49.880 --> 00:04:52.225
So the first one is object
and scene detection.

123
00:04:52.225 --> 00:04:53.840
In this case, we take

124
00:04:53.840 --> 00:04:56.345
an image and we send it
to Amazon Rekognition.

125
00:04:56.345 --> 00:04:58.130
Rekognition then
look at the image,

126
00:04:58.130 --> 00:04:59.870
detect different
objects, what is in

127
00:04:59.870 --> 00:05:02.255
the scene and return
us a list of labels.

128
00:05:02.255 --> 00:05:03.650
So for example in this case,

129
00:05:03.650 --> 00:05:05.210
you see image on the left,

130
00:05:05.210 --> 00:05:08.660
we get different labels like
chair or a living room,

131
00:05:08.660 --> 00:05:10.400
coffee table, and so on.

132
00:05:10.400 --> 00:05:12.740
Whereas on the image
on the right it's able

133
00:05:12.740 --> 00:05:15.230
to detect that there
is a swimming pool,

134
00:05:15.230 --> 00:05:16.955
there is water, and so on.

135
00:05:16.955 --> 00:05:18.905
Generally, when you're building

136
00:05:18.905 --> 00:05:20.765
media library which has a lot of

137
00:05:20.765 --> 00:05:22.250
unstructured content thing like

138
00:05:22.250 --> 00:05:24.435
images and videos in the past,

139
00:05:24.435 --> 00:05:26.090
usually you had to

140
00:05:26.090 --> 00:05:28.970
tag every single image
or video that gets into

141
00:05:28.970 --> 00:05:31.205
the media library to
make library more

142
00:05:31.205 --> 00:05:32.750
searchable and also to

143
00:05:32.750 --> 00:05:34.765
be able to easily
find the content.

144
00:05:34.765 --> 00:05:37.530
By using Amazon
Rekognition scenes,

145
00:05:37.530 --> 00:05:39.065
now Rekognition returns us

146
00:05:39.065 --> 00:05:41.050
all the information
about that scene,

147
00:05:41.050 --> 00:05:42.755
it's very easy to then

148
00:05:42.755 --> 00:05:45.500
automatically tag those images
and video and make your

149
00:05:45.500 --> 00:05:48.080
media library easily
accessible instead of

150
00:05:48.080 --> 00:05:50.120
manually tagging
every single object

151
00:05:50.120 --> 00:05:51.670
that comes into
the Media Library.

152
00:05:51.670 --> 00:05:54.290
Rekognition provides
facial analysis and

153
00:05:54.290 --> 00:05:57.200
so that's very you can
send for example an image.

154
00:05:57.200 --> 00:06:00.020
It can analyze the different
facial characteristics

155
00:06:00.020 --> 00:06:01.145
along different dimensions.

156
00:06:01.145 --> 00:06:03.275
So things like demographic data,

157
00:06:03.275 --> 00:06:06.215
where it can tell you the
age range of the person,

158
00:06:06.215 --> 00:06:08.345
it can detect gender
of the person,

159
00:06:08.345 --> 00:06:11.375
it can provide information
about facial landmarks.

160
00:06:11.375 --> 00:06:13.745
So for example, where
the person's left eye,

161
00:06:13.745 --> 00:06:16.900
right eye is information
about nose and so on.

162
00:06:16.900 --> 00:06:18.950
Rekognition return
you information

163
00:06:18.950 --> 00:06:20.810
about the brightness
or the sharpness of

164
00:06:20.810 --> 00:06:22.580
the image as well as

165
00:06:22.580 --> 00:06:25.160
other attributes including
how the emotions are,

166
00:06:25.160 --> 00:06:27.170
whether the person is
happy, they're smiling,

167
00:06:27.170 --> 00:06:30.050
they're surprised, whether
their eyes are open,

168
00:06:30.050 --> 00:06:32.215
whether they have beard
or not, and so on.

169
00:06:32.215 --> 00:06:34.670
Other information that
Rekognition provides you

170
00:06:34.670 --> 00:06:37.385
as part of the facial analysis
is the official pores

171
00:06:37.385 --> 00:06:39.050
which can help you understand

172
00:06:39.050 --> 00:06:40.700
and see which direction
the person is

173
00:06:40.700 --> 00:06:42.200
looking at and so on by

174
00:06:42.200 --> 00:06:44.255
taking advantage of the
attributes like pitch,

175
00:06:44.255 --> 00:06:45.880
roll, and yaw for example.

176
00:06:45.880 --> 00:06:48.020
Facial detection and analysis is

177
00:06:48.020 --> 00:06:50.150
able to detect up to a 100 faces.

178
00:06:50.150 --> 00:06:52.070
So for example in this case,

179
00:06:52.070 --> 00:06:54.980
you see that is an
image of a crowd and

180
00:06:54.980 --> 00:06:56.885
Rekognition can easily detect

181
00:06:56.885 --> 00:06:58.725
up to a 100 faces in that crowd.

182
00:06:58.725 --> 00:07:01.230
Another feature that
Amazon Rekognition support

183
00:07:01.230 --> 00:07:02.690
is called face comparison.

184
00:07:02.690 --> 00:07:05.540
So in this case, you can
take two images and ask

185
00:07:05.540 --> 00:07:07.580
Rekognition to compare if those

186
00:07:07.580 --> 00:07:09.860
are images of the
same person or not.

187
00:07:09.860 --> 00:07:12.525
So for example, you can
see image on the left,

188
00:07:12.525 --> 00:07:15.470
the individual is with some
other people and the image on

189
00:07:15.470 --> 00:07:18.470
the right is another
picture of the same person.

190
00:07:18.470 --> 00:07:20.690
But if you see the
image on the right,

191
00:07:20.690 --> 00:07:22.570
the person doesn't have
beard for example.

192
00:07:22.570 --> 00:07:25.160
Where he has more facial hair
on the image on the left,

193
00:07:25.160 --> 00:07:27.110
my Rekognition is able to compare

194
00:07:27.110 --> 00:07:30.245
those and tell you a
confidence level saying,

195
00:07:30.245 --> 00:07:33.155
it looks like there's a
93 percent similarity

196
00:07:33.155 --> 00:07:35.180
in these two images
for these two faces.

197
00:07:35.180 --> 00:07:37.700
Whereas the other two
faces in the images don't

198
00:07:37.700 --> 00:07:39.410
necessarily have any similarity

199
00:07:39.410 --> 00:07:41.170
and the score is zero percent.

200
00:07:41.170 --> 00:07:43.470
Another feature that Rekognition

201
00:07:43.470 --> 00:07:45.745
provides is called
facial Rekognition.

202
00:07:45.745 --> 00:07:48.725
So in this case you're not
just comparing two faces.

203
00:07:48.725 --> 00:07:50.690
In this case what
we're saying is,

204
00:07:50.690 --> 00:07:52.955
we want to build a
collection of faces.

205
00:07:52.955 --> 00:07:56.195
So there could be millions
of faces that you can

206
00:07:56.195 --> 00:07:59.975
index in that collection and
then for any incoming face,

207
00:07:59.975 --> 00:08:01.640
if you want to
compare it and see if

208
00:08:01.640 --> 00:08:03.475
there is a match in
that collection.

209
00:08:03.475 --> 00:08:06.470
That's something Rekognition
can easily do even when

210
00:08:06.470 --> 00:08:07.790
you have collection
of millions of

211
00:08:07.790 --> 00:08:09.670
faces in under a second.

212
00:08:09.670 --> 00:08:11.030
Another core feature that

213
00:08:11.030 --> 00:08:13.070
I condition provide is
content moderation.

214
00:08:13.070 --> 00:08:15.950
So in this case, it can
easily detect if there is

215
00:08:15.950 --> 00:08:18.200
any explicit or
suggestive content

216
00:08:18.200 --> 00:08:20.390
for example in the
image or video.

217
00:08:20.390 --> 00:08:23.719
We don't tell you whether
it's explicit or suggestive,

218
00:08:23.719 --> 00:08:26.540
data are basically two
levels of hierarchy.

219
00:08:26.540 --> 00:08:28.535
So you got a top level category,

220
00:08:28.535 --> 00:08:30.740
where you get to know
whether the content it

221
00:08:30.740 --> 00:08:33.800
has explicit or
suggestive content.

222
00:08:33.800 --> 00:08:35.420
Then it can further give you

223
00:08:35.420 --> 00:08:38.120
more details at the second
level whether for example,

224
00:08:38.120 --> 00:08:39.560
there is nudity and so on.

225
00:08:39.560 --> 00:08:41.405
The reason we give you
these two categories

226
00:08:41.405 --> 00:08:42.920
is because depending on

227
00:08:42.920 --> 00:08:47.135
the countries or laws of
our country or region,

228
00:08:47.135 --> 00:08:49.040
one type of content
might be okay,

229
00:08:49.040 --> 00:08:50.240
if we added in another region

230
00:08:50.240 --> 00:08:51.785
the same content may not be okay.

231
00:08:51.785 --> 00:08:52.940
So for example, by

232
00:08:52.940 --> 00:08:54.905
detecting that there
is revealing clothes,

233
00:08:54.905 --> 00:08:57.860
maybe that content is okay
for certain type of website

234
00:08:57.860 --> 00:08:59.170
and certain type
of region and not

235
00:08:59.170 --> 00:09:00.875
maybe in another type
of region and so on.

236
00:09:00.875 --> 00:09:04.045
So you get to control based
on what is in that content.

237
00:09:04.045 --> 00:09:06.575
Another core feature that
Rekognition provides,

238
00:09:06.575 --> 00:09:08.350
is celebrity air recognition.

239
00:09:08.350 --> 00:09:09.405
So in this case again,

240
00:09:09.405 --> 00:09:13.265
you can send an image or you
can send a video and we have

241
00:09:13.265 --> 00:09:15.635
hundreds and thousands
of famous individuals

242
00:09:15.635 --> 00:09:17.955
whether they are politicians,

243
00:09:17.955 --> 00:09:20.490
sportsman, actors,
actresses, and so on,

244
00:09:20.490 --> 00:09:23.390
and you can easily get those
recognize the inner content.

245
00:09:23.390 --> 00:09:26.885
Another core feature of
Rekognition is detect text.

246
00:09:26.885 --> 00:09:28.390
So in this case for example,

247
00:09:28.390 --> 00:09:29.830
you can see an image with

248
00:09:29.830 --> 00:09:31.370
the piece of paper
but some texts on

249
00:09:31.370 --> 00:09:32.810
our table and Rekognition is

250
00:09:32.810 --> 00:09:34.820
able to easily then
detect that text.

251
00:09:34.820 --> 00:09:36.080
In fact, it returned to

252
00:09:36.080 --> 00:09:37.670
all those different
lines then you

253
00:09:37.670 --> 00:09:40.730
can build in a very interesting
applications using that.

254
00:09:40.730 --> 00:09:42.590
Another feature is the pathing.

255
00:09:42.590 --> 00:09:44.555
This is where you can detect

256
00:09:44.555 --> 00:09:47.660
different path that people
took in a video for example.

257
00:09:47.660 --> 00:09:49.790
So in this screen, you
can see that it is

258
00:09:49.790 --> 00:09:52.355
soccer players and so by
looking at that video,

259
00:09:52.355 --> 00:09:54.800
you can detect there are
different players in the game

260
00:09:54.800 --> 00:09:57.515
and these are the different
paths that people took.

261
00:09:57.515 --> 00:09:59.870
Some of that information
can easily be used

262
00:09:59.870 --> 00:10:02.340
then for interesting use
cases like building,

263
00:10:02.340 --> 00:10:04.040
highlighting, real or many

264
00:10:04.040 --> 00:10:05.750
of the other similar use cases.

265
00:10:05.750 --> 00:10:07.070
These are some of

266
00:10:07.070 --> 00:10:08.965
the core features that
Rekognition provide.

267
00:10:08.965 --> 00:10:12.560
Now, let's quickly look at the
Rekognition API and see as

268
00:10:12.560 --> 00:10:14.210
a developer what does it

269
00:10:14.210 --> 00:10:16.420
take to take advantage
of these features.

270
00:10:16.420 --> 00:10:18.575
So we'll look at the first API

271
00:10:18.575 --> 00:10:20.240
which is called the tech labels.

272
00:10:20.240 --> 00:10:23.270
This is to get different
labels for an image.

273
00:10:23.270 --> 00:10:25.775
So you can see it's
pretty straight forward.

274
00:10:25.775 --> 00:10:27.110
The way it works is you call

275
00:10:27.110 --> 00:10:29.510
DetectLabels and you pass it's

276
00:10:29.510 --> 00:10:31.640
request which has information

277
00:10:31.640 --> 00:10:34.250
about the image that you
want to do the analysis for.

278
00:10:34.250 --> 00:10:37.340
You can either send that
raw bytes or you can send

279
00:10:37.340 --> 00:10:39.415
the information
about the S3 bucket

280
00:10:39.415 --> 00:10:41.110
where that image resides.

281
00:10:41.110 --> 00:10:43.220
In addition, you
can ask how many of

282
00:10:43.220 --> 00:10:45.725
the labels that you want to
get returned in that result,

283
00:10:45.725 --> 00:10:48.200
as well as what is the
minimum confidence level.

284
00:10:48.200 --> 00:10:49.640
So if you set the
confidence level

285
00:10:49.640 --> 00:10:51.495
for example to 80 percent,

286
00:10:51.495 --> 00:10:53.090
then we will only return you

287
00:10:53.090 --> 00:10:54.680
the labels which we are certain

288
00:10:54.680 --> 00:10:57.140
that that level has

289
00:10:57.140 --> 00:10:59.390
a confidence level of
80 percent or above.

290
00:10:59.390 --> 00:11:01.170
Once you call the tech labels,

291
00:11:01.170 --> 00:11:04.220
you get the response
back where we return you

292
00:11:04.220 --> 00:11:05.810
list of labels along with

293
00:11:05.810 --> 00:11:07.850
the confidence level for
each of those labels.

294
00:11:07.850 --> 00:11:10.895
Now let's see this was
the request for image,

295
00:11:10.895 --> 00:11:14.245
how would you do the same
thing for the video API?

296
00:11:14.245 --> 00:11:15.915
So if you're sending the video,

297
00:11:15.915 --> 00:11:19.065
in this case again the
request is quite similar,

298
00:11:19.065 --> 00:11:20.960
they use sender information about

299
00:11:20.960 --> 00:11:23.150
the S3 bucket where the video is.

300
00:11:23.150 --> 00:11:25.490
One thing that additional
that I want to highlight here

301
00:11:25.490 --> 00:11:28.025
is that we would
call SNSTopicArn.

302
00:11:28.025 --> 00:11:30.350
The reason we have
that is because

303
00:11:30.350 --> 00:11:34.025
the Video API is asynchronous.

304
00:11:34.025 --> 00:11:37.400
So in this case, instead of
calling DetectLabel you call

305
00:11:37.400 --> 00:11:40.145
StartLabelDetection
which basically starts

306
00:11:40.145 --> 00:11:42.715
a job or to analyze the video.

307
00:11:42.715 --> 00:11:45.220
As part of that
StartLabelDetection,

308
00:11:45.220 --> 00:11:48.200
you can pass us information
about an SNS topic.

309
00:11:48.200 --> 00:11:50.030
So as the job completes,

310
00:11:50.030 --> 00:11:52.400
we can then call
that SNS topic to

311
00:11:52.400 --> 00:11:54.650
know let you know that
the job is complete.

312
00:11:54.650 --> 00:11:56.270
So instead of manually pooling to

313
00:11:56.270 --> 00:11:58.370
know whether the job
is complete or not,

314
00:11:58.370 --> 00:12:02.585
you can just take advantage
of the SNSTopicArn feature.

315
00:12:02.585 --> 00:12:05.040
Menu called StartLabelDetection
the response

316
00:12:05.040 --> 00:12:07.220
returns you a job
ID and then based

317
00:12:07.220 --> 00:12:09.320
on that you can call
GetLabelDetection

318
00:12:09.320 --> 00:12:11.780
which returns you obviously
a couple of attributes.

319
00:12:11.780 --> 00:12:14.120
One of those is the
status message that

320
00:12:14.120 --> 00:12:16.645
can help you see whether
the job is complete or not.

321
00:12:16.645 --> 00:12:20.075
In addition, the core
information that is labels.

322
00:12:20.075 --> 00:12:21.695
For the image API,

323
00:12:21.695 --> 00:12:24.340
you saw we pretty much got
the list of labels back.

324
00:12:24.340 --> 00:12:25.480
In case of video,

325
00:12:25.480 --> 00:12:27.845
there is an additional
attribute called Timestamp,

326
00:12:27.845 --> 00:12:31.640
so it can tell you time
one millisecond or

327
00:12:31.640 --> 00:12:34.670
five millisecond in
the frame these are

328
00:12:34.670 --> 00:12:36.499
the objects and or activities

329
00:12:36.499 --> 00:12:38.720
and things that
recognition found.

330
00:12:38.720 --> 00:12:42.005
Most of the other APIs they
all follow the same pattern.

331
00:12:42.005 --> 00:12:43.410
So pretty much for image,

332
00:12:43.410 --> 00:12:45.380
you will pass the
image information

333
00:12:45.380 --> 00:12:47.255
and get the results
back, for the video,

334
00:12:47.255 --> 00:12:48.515
you will start a job,

335
00:12:48.515 --> 00:12:49.910
get a job ID back,

336
00:12:49.910 --> 00:12:51.170
and then based on that you can

337
00:12:51.170 --> 00:12:52.805
get the rest of the information.

338
00:12:52.805 --> 00:12:55.010
So the API calls usually we

339
00:12:55.010 --> 00:12:57.410
categorize them into
different buckets: one is

340
00:12:57.410 --> 00:12:59.720
the non-storage API
operations and the

341
00:12:59.720 --> 00:13:02.375
other are storage-based
API operations.

342
00:13:02.375 --> 00:13:05.960
So all the things like
DetectLabel, compare faces,

343
00:13:05.960 --> 00:13:07.610
and so on those are all done

344
00:13:07.610 --> 00:13:11.160
using non-storage API operations.

345
00:13:11.160 --> 00:13:13.265
Whereas when you
build a collection

346
00:13:13.265 --> 00:13:15.740
and you do things
like index faces,

347
00:13:15.740 --> 00:13:17.165
you want to list all the faces.

348
00:13:17.165 --> 00:13:20.600
Those are the ones that you
are doing on that storage

349
00:13:20.600 --> 00:13:22.100
which means the
collection that you

350
00:13:22.100 --> 00:13:24.490
created for all the
faces you have.

351
00:13:24.490 --> 00:13:26.990
Now with the understanding
of core features

352
00:13:26.990 --> 00:13:29.690
as well as how does the API work,

353
00:13:29.690 --> 00:13:32.060
let's look at some of the
use cases that we have

354
00:13:32.060 --> 00:13:35.000
seen customers are using
Amazon Rekognition.

355
00:13:35.000 --> 00:13:36.470
So the first one I'll talk about

356
00:13:36.470 --> 00:13:38.435
the royal wedding
coverage of Sky News.

357
00:13:38.435 --> 00:13:41.060
So in this case, when the
royal wedding happen,

358
00:13:41.060 --> 00:13:44.645
the Sky News built a feature
called Who is Who Live.

359
00:13:44.645 --> 00:13:47.165
They identify the guests

360
00:13:47.165 --> 00:13:49.910
as they were arriving
at the event.

361
00:13:49.910 --> 00:13:52.040
Then you can see the UI in

362
00:13:52.040 --> 00:13:53.900
the screen where it was showing

363
00:13:53.900 --> 00:13:56.210
the screen captions and basically

364
00:13:56.210 --> 00:13:58.775
showed the relation of the
guests to the royal couple.

365
00:13:58.775 --> 00:14:01.190
Here are a few other
screenshots of

366
00:14:01.190 --> 00:14:04.010
the UI how it looked like
as the person is arriving,

367
00:14:04.010 --> 00:14:06.170
so they were able to take
advantage of the things

368
00:14:06.170 --> 00:14:08.570
we mentioned like
Rekognition collection,

369
00:14:08.570 --> 00:14:11.720
indexing faces, and then as
the guest arrives comparing

370
00:14:11.720 --> 00:14:13.415
that face against
their collection

371
00:14:13.415 --> 00:14:15.230
and find out who that person is.

372
00:14:15.230 --> 00:14:16.370
So you can see it's pretty

373
00:14:16.370 --> 00:14:18.860
easy to build a use
cases like these.

374
00:14:18.860 --> 00:14:21.170
Another use case we have seen is

375
00:14:21.170 --> 00:14:23.510
for example sports
and media tagging.

376
00:14:23.510 --> 00:14:26.480
So instead of manually
going through a game,

377
00:14:26.480 --> 00:14:28.190
a recording of a
game and manually

378
00:14:28.190 --> 00:14:31.070
tagging a bunch of
frames across the game,

379
00:14:31.070 --> 00:14:32.990
you can pretty much take a video,

380
00:14:32.990 --> 00:14:34.700
use different
Rekognition features

381
00:14:34.700 --> 00:14:36.440
for example use pathing to

382
00:14:36.440 --> 00:14:38.150
detect different players and

383
00:14:38.150 --> 00:14:40.500
the path that it took
during the game,

384
00:14:40.500 --> 00:14:42.950
and maybe use detect
text for example

385
00:14:42.950 --> 00:14:44.989
to look at the scoreboard

386
00:14:44.989 --> 00:14:47.090
and find out when
the score change.

387
00:14:47.090 --> 00:14:48.530
That way by detecting

388
00:14:48.530 --> 00:14:50.615
different objects,
different activities,

389
00:14:50.615 --> 00:14:54.455
you can build for example
reel of highlights and so on.

390
00:14:54.455 --> 00:14:56.520
Instead of doing it manually,

391
00:14:56.520 --> 00:14:59.000
just easier to do that
by using a Rekognition

392
00:14:59.000 --> 00:15:00.260
and detecting all these objects

393
00:15:00.260 --> 00:15:01.850
and activities in the game.

394
00:15:01.850 --> 00:15:04.250
Another use case we have seen

395
00:15:04.250 --> 00:15:06.950
across in public safety
and they're companies like

396
00:15:06.950 --> 00:15:08.480
Marinus Analytics have

397
00:15:08.480 --> 00:15:11.270
Build Machine Learning
Analytics platform

398
00:15:11.270 --> 00:15:13.280
for law enforcement to

399
00:15:13.280 --> 00:15:16.370
combat human trafficking
and really reduce

400
00:15:16.370 --> 00:15:19.835
the time and effort to
identify and rescue victims.

401
00:15:19.835 --> 00:15:21.410
Another common use case is

402
00:15:21.410 --> 00:15:23.870
across many websites
or social networks,

403
00:15:23.870 --> 00:15:26.750
where they want to to analyze
the user-generated content.

404
00:15:26.750 --> 00:15:29.330
So whether it's an e-commerce
website where people

405
00:15:29.330 --> 00:15:32.030
are uploading product
reviews or this

406
00:15:32.030 --> 00:15:33.320
is a social networking site

407
00:15:33.320 --> 00:15:34.460
where people are uploading for

408
00:15:34.460 --> 00:15:37.460
example their pictures as
their profile picture,

409
00:15:37.460 --> 00:15:39.740
and you want to make sure
that the picture is of

410
00:15:39.740 --> 00:15:43.290
a person as well as what
is the activity going on,

411
00:15:43.290 --> 00:15:44.570
if there is any explicit or

412
00:15:44.570 --> 00:15:46.640
suggestive content in that image.

413
00:15:46.640 --> 00:15:50.630
So just by taking advantage
of the recognition APIs,

414
00:15:50.630 --> 00:15:52.280
you can easily validate that

415
00:15:52.280 --> 00:15:55.115
a user-generated content
before it gets published.

416
00:15:55.115 --> 00:15:57.530
The really cool thing here
is that you can easily

417
00:15:57.530 --> 00:15:59.930
do that by automat the process

418
00:15:59.930 --> 00:16:02.885
by using Rekognition
API instead of manually

419
00:16:02.885 --> 00:16:04.520
validating all the images

420
00:16:04.520 --> 00:16:06.080
or the video that gets uploaded.

421
00:16:06.080 --> 00:16:09.545
Let's see how you can build
a solution like this on AWS.

422
00:16:09.545 --> 00:16:10.775
So it's pretty straightforward.

423
00:16:10.775 --> 00:16:12.470
The image gets uploaded to

424
00:16:12.470 --> 00:16:15.485
S3 which can then trigger
a Lambda function.

425
00:16:15.485 --> 00:16:18.200
Lambda function can then
call stuff functions which

426
00:16:18.200 --> 00:16:21.455
is AWS service to build
really powerful workflows.

427
00:16:21.455 --> 00:16:23.090
Stuff function can then execute

428
00:16:23.090 --> 00:16:24.440
different Lambda
functions for you.

429
00:16:24.440 --> 00:16:27.170
So for example, the first
Lambda function can call detect

430
00:16:27.170 --> 00:16:30.650
faces and detect all the
faces that are in that image.

431
00:16:30.650 --> 00:16:32.480
That's how you can find out if

432
00:16:32.480 --> 00:16:34.460
there are any faces
in the image or

433
00:16:34.460 --> 00:16:36.470
if the uploaded image is maybe

434
00:16:36.470 --> 00:16:38.510
for dog or another
animal or so on.

435
00:16:38.510 --> 00:16:40.130
Another Lambda
function for example

436
00:16:40.130 --> 00:16:42.290
can call recognized celebrities.

437
00:16:42.290 --> 00:16:43.954
That's how you can identify

438
00:16:43.954 --> 00:16:46.010
if the profile picture
that for example is

439
00:16:46.010 --> 00:16:48.155
somebody uploading a
celebrities picture

440
00:16:48.155 --> 00:16:49.880
or is this some other person.

441
00:16:49.880 --> 00:16:52.310
Another Lambda
function can then call

442
00:16:52.310 --> 00:16:55.745
moderation API for example,
detect moderation labels.

443
00:16:55.745 --> 00:16:57.530
That's how you can
find out if there is

444
00:16:57.530 --> 00:17:00.520
any explicit or suggestive
content in there.

445
00:17:00.520 --> 00:17:03.590
Then you can take advantage
of the other APIs as

446
00:17:03.590 --> 00:17:07.610
well depending on if any of
the images will get uploaded,

447
00:17:07.610 --> 00:17:09.500
if any of those images
are duplicates,

448
00:17:09.500 --> 00:17:11.980
and so on and then you can
filter those based on that.

449
00:17:11.980 --> 00:17:16.070
All of that metadata can
then be stored in DynamoDB

450
00:17:16.070 --> 00:17:17.510
as well as something like

451
00:17:17.510 --> 00:17:20.270
Elasticsearch to make it
searchable and so on.

452
00:17:20.270 --> 00:17:22.040
Another common use case is

453
00:17:22.040 --> 00:17:24.335
where you can do chat moderation.

454
00:17:24.335 --> 00:17:25.640
So for example, if during

455
00:17:25.640 --> 00:17:28.115
the chat any of the
images get uploaded,

456
00:17:28.115 --> 00:17:30.980
you can then call API Gateway,

457
00:17:30.980 --> 00:17:32.540
pass that image through API get

458
00:17:32.540 --> 00:17:34.670
webpage then goes to AWS lambda.

459
00:17:34.670 --> 00:17:36.750
Lambda can then call Rekognition,

460
00:17:36.750 --> 00:17:38.505
ask for a modulation labels,

461
00:17:38.505 --> 00:17:41.595
if it detect based
on your criteria,

462
00:17:41.595 --> 00:17:44.540
if it detect any explicit
or suggestive content,

463
00:17:44.540 --> 00:17:47.090
it can then get the
response back and then

464
00:17:47.090 --> 00:17:50.225
the chat client can
then delete that image,

465
00:17:50.225 --> 00:17:52.310
that's part of that chat message.

466
00:17:52.310 --> 00:17:55.580
Another use case is we're
maybe a lot of times

467
00:17:55.580 --> 00:17:56.870
we want to find out how long

468
00:17:56.870 --> 00:17:58.625
people have to wait in a line.

469
00:17:58.625 --> 00:18:01.265
That's where again,
you can have a camera

470
00:18:01.265 --> 00:18:04.190
pointed at the lane
and using Rekognition,

471
00:18:04.190 --> 00:18:05.450
you can easily detect

472
00:18:05.450 --> 00:18:06.950
different people who
are in the line.

473
00:18:06.950 --> 00:18:08.690
So in this case, you
are not necessarily

474
00:18:08.690 --> 00:18:11.435
recognizing like who
these individuals are,

475
00:18:11.435 --> 00:18:12.710
all we're seeing
is that there are

476
00:18:12.710 --> 00:18:14.950
different people
standing in the line.

477
00:18:14.950 --> 00:18:18.190
Then based on their
position in the frame,

478
00:18:18.190 --> 00:18:20.540
you can easily identify
how many people

479
00:18:20.540 --> 00:18:22.910
are in the line and
then based on that

480
00:18:22.910 --> 00:18:25.130
do some analysis to find
out how long does it

481
00:18:25.130 --> 00:18:28.025
usually take for people
to be in that line.

482
00:18:28.025 --> 00:18:29.705
To build a solution like that,

483
00:18:29.705 --> 00:18:31.760
again you have a
camera which is taking

484
00:18:31.760 --> 00:18:33.740
the live picture of the people in

485
00:18:33.740 --> 00:18:36.580
the line, called API Gateway.

486
00:18:36.580 --> 00:18:40.275
API Gateway can then pass
that image to Lambda.

487
00:18:40.275 --> 00:18:43.700
Lambda can store that
image in S3 and then call

488
00:18:43.700 --> 00:18:46.700
Amazon recognition to
analyze that image to

489
00:18:46.700 --> 00:18:50.140
find out how many persons
are there in that image.

490
00:18:50.140 --> 00:18:53.180
That information can then
be stored in something

491
00:18:53.180 --> 00:18:54.560
like DynamoDB and then

492
00:18:54.560 --> 00:18:56.300
you can do analysis
on top of that.

493
00:18:56.300 --> 00:18:58.550
Another use case is where you can

494
00:18:58.550 --> 00:19:01.430
use Amazon Rekognition
is for example,

495
00:19:01.430 --> 00:19:03.185
for live demographic analysis.

496
00:19:03.185 --> 00:19:06.415
So in this case, imagine
you have these stores,

497
00:19:06.415 --> 00:19:08.530
they're different people walking

498
00:19:08.530 --> 00:19:10.700
as they are in different
parts of the store.

499
00:19:10.700 --> 00:19:12.980
Again you can take the
image from the store,

500
00:19:12.980 --> 00:19:15.910
call Rekognition API to find out

501
00:19:15.910 --> 00:19:17.930
the sentiment analysis of

502
00:19:17.930 --> 00:19:20.620
the people as well as the
gender of the people,

503
00:19:20.620 --> 00:19:22.100
age, range, and so on.

504
00:19:22.100 --> 00:19:24.980
So we have seen customers
talking about where maybe they

505
00:19:24.980 --> 00:19:28.480
have a menu in the store for
example, at a restaurant.

506
00:19:28.480 --> 00:19:30.380
Depending on if there are

507
00:19:30.380 --> 00:19:32.180
more kids into store
or if there are

508
00:19:32.180 --> 00:19:33.740
more one of the genders in

509
00:19:33.740 --> 00:19:35.730
the store and depending on that,

510
00:19:35.730 --> 00:19:38.510
they can then customize
their digital menu board and

511
00:19:38.510 --> 00:19:41.870
maybe highlight certain
things for kids and so on.

512
00:19:41.870 --> 00:19:43.430
The same architecture can be

513
00:19:43.430 --> 00:19:46.005
used for real-time store heatmap.

514
00:19:46.005 --> 00:19:48.680
So depending on different
cameras in the store,

515
00:19:48.680 --> 00:19:50.360
then you can find
out where there are

516
00:19:50.360 --> 00:19:52.790
more people or which
section of the store has

517
00:19:52.790 --> 00:19:54.950
more people as well as

518
00:19:54.950 --> 00:19:57.535
by their gender and their
age, range, and so on.

519
00:19:57.535 --> 00:19:58.780
To build a solution like that,

520
00:19:58.780 --> 00:20:00.290
again it's pretty
straightforward.

521
00:20:00.290 --> 00:20:02.120
So you have those
live images coming

522
00:20:02.120 --> 00:20:04.190
from cameras in the store.

523
00:20:04.190 --> 00:20:05.510
They get passed on to

524
00:20:05.510 --> 00:20:08.290
API Gateway which then
calls Lambda function,

525
00:20:08.290 --> 00:20:10.280
and then it gets stored on S3.

526
00:20:10.280 --> 00:20:13.355
You can call Rekognition
to detect faces

527
00:20:13.355 --> 00:20:16.430
and that's how you find
out sentiment analysis,

528
00:20:16.430 --> 00:20:18.170
all that information which

529
00:20:18.170 --> 00:20:19.790
you can then store
in something like

530
00:20:19.790 --> 00:20:22.130
DynamoDB or store in

531
00:20:22.130 --> 00:20:23.870
Amazon Redshift and use

532
00:20:23.870 --> 00:20:26.585
quick site to do
analysis and so on.

533
00:20:26.585 --> 00:20:29.210
So those were some
of the use cases,

534
00:20:29.210 --> 00:20:31.670
there are large
number of use cases.

535
00:20:31.670 --> 00:20:33.050
But given the time constraint,

536
00:20:33.050 --> 00:20:36.230
I only picked just a few to
talk about in this course.

537
00:20:36.230 --> 00:20:38.030
Now, let's talk about some of

538
00:20:38.030 --> 00:20:39.950
the best practices
that you should

539
00:20:39.950 --> 00:20:41.450
follow as you're building any of

540
00:20:41.450 --> 00:20:44.590
those solutions using
Amazon Rekognition.

541
00:20:44.590 --> 00:20:46.805
So first of all,
Rekognition support

542
00:20:46.805 --> 00:20:50.660
images in both PNG
and JPEG format.

543
00:20:50.660 --> 00:20:52.820
The maximum image size,

544
00:20:52.820 --> 00:20:56.105
if the image is stored
in S3 then it's 15 Meg.

545
00:20:56.105 --> 00:20:59.720
If you're calling the image
directly as you call API,

546
00:20:59.720 --> 00:21:01.535
then that is 5 Meg.

547
00:21:01.535 --> 00:21:05.120
If you're using video
analysis in that case,

548
00:21:05.120 --> 00:21:07.430
the video format is mp4 and

549
00:21:07.430 --> 00:21:12.075
MOV and the video codec
is H264 that we support.

550
00:21:12.075 --> 00:21:15.290
The maximum video
size should be 8 Gig.

551
00:21:15.290 --> 00:21:17.745
If you have video which
is larger than 8 Gig,

552
00:21:17.745 --> 00:21:19.970
you can easily use
other AWS services

553
00:21:19.970 --> 00:21:22.460
like Elemental for
example and some of

554
00:21:22.460 --> 00:21:25.250
the other services to then
break those videos into

555
00:21:25.250 --> 00:21:26.885
smaller pieces and then

556
00:21:26.885 --> 00:21:29.285
call Rekognition to
analyze those videos.

557
00:21:29.285 --> 00:21:31.430
Some of the other
best practices is

558
00:21:31.430 --> 00:21:33.820
when you use collections
for example,

559
00:21:33.820 --> 00:21:36.005
those collections are for faces.

560
00:21:36.005 --> 00:21:37.910
So do not try to index for

561
00:21:37.910 --> 00:21:40.370
example pictures of
cats or dogs and so on,

562
00:21:40.370 --> 00:21:41.785
that's not going to work.

563
00:21:41.785 --> 00:21:44.150
The maximum number of faces in

564
00:21:44.150 --> 00:21:47.000
a single collection today
is up to 20 million,

565
00:21:47.000 --> 00:21:49.310
that's a number that we
continue to work on.

566
00:21:49.310 --> 00:21:51.350
So as you're building
your application,

567
00:21:51.350 --> 00:21:53.165
definitely look at
the documentation

568
00:21:53.165 --> 00:21:54.470
to see the current number,

569
00:21:54.470 --> 00:21:57.320
even when you have 20 million
faces in a collection,

570
00:21:57.320 --> 00:21:59.840
the latency that you get
is still under a second,

571
00:21:59.840 --> 00:22:01.025
which is pretty awesome.

572
00:22:01.025 --> 00:22:04.400
The maximum number of faces
that get returned as a result

573
00:22:04.400 --> 00:22:08.315
and the search API
are up to 4,096.

574
00:22:08.315 --> 00:22:11.295
Another best practice
is when you index

575
00:22:11.295 --> 00:22:14.600
your images in
Rekognition collection,

576
00:22:14.600 --> 00:22:16.700
you should keep those images if

577
00:22:16.700 --> 00:22:18.875
you ever need to reindex those.

578
00:22:18.875 --> 00:22:22.310
Generally, you might need
to reindex those when

579
00:22:22.310 --> 00:22:25.745
we launch for example
maybe next major version.

580
00:22:25.745 --> 00:22:27.140
In that case, you might have to

581
00:22:27.140 --> 00:22:29.690
reindex those images
because we don't keep

582
00:22:29.690 --> 00:22:31.400
the images that you

583
00:22:31.400 --> 00:22:35.120
call Rekognition API for
index faces for example.

584
00:22:35.120 --> 00:22:37.640
We only extract the
facial features and

585
00:22:37.640 --> 00:22:40.430
store those vectors
in our system,

586
00:22:40.430 --> 00:22:42.725
and we do not keep the
original image that

587
00:22:42.725 --> 00:22:47.000
you send as part of
the index face API.

588
00:22:47.000 --> 00:22:51.065
So keeping those images
around will come handy when

589
00:22:51.065 --> 00:22:53.180
the face models get
updated and you want to

590
00:22:53.180 --> 00:22:55.630
take advantage of those
in your next collection,

591
00:22:55.630 --> 00:22:57.500
then you can use
those images and just

592
00:22:57.500 --> 00:23:00.225
reindex them in your
Rekognition collection.

593
00:23:00.225 --> 00:23:02.775
When you use index faces API,

594
00:23:02.775 --> 00:23:05.585
so understand that
detects largest

595
00:23:05.585 --> 00:23:07.430
a 100 faces and input

596
00:23:07.430 --> 00:23:10.460
those images and adds them
to the specific collection.

597
00:23:10.460 --> 00:23:13.310
I've seen sometimes customer
who use that index face,

598
00:23:13.310 --> 00:23:15.725
they don't realize
that and for example,

599
00:23:15.725 --> 00:23:18.710
if they are indexing a face
for a specific person,

600
00:23:18.710 --> 00:23:21.320
they might end up seeing
index this person's face

601
00:23:21.320 --> 00:23:22.790
and there might be
a few other people,

602
00:23:22.790 --> 00:23:25.775
then all of those images or
all of those additional faces

603
00:23:25.775 --> 00:23:27.530
will get index by

604
00:23:27.530 --> 00:23:29.480
the name of the person
that you are sending.

605
00:23:29.480 --> 00:23:31.550
So when you are indexing a face,

606
00:23:31.550 --> 00:23:34.145
you should send usually

607
00:23:34.145 --> 00:23:37.010
just one image unless
there is a use case where

608
00:23:37.010 --> 00:23:39.020
you might want to tag

609
00:23:39.020 --> 00:23:42.535
multiple people by the
same ID or something.

610
00:23:42.535 --> 00:23:45.625
Similarly, when you use
search face by image,

611
00:23:45.625 --> 00:23:49.010
it detects the largest
face in the image and then

612
00:23:49.010 --> 00:23:50.989
searches the specify collection

613
00:23:50.989 --> 00:23:52.490
for all the matching faces.

614
00:23:52.490 --> 00:23:56.285
So if you are looking to
detect a specific face,

615
00:23:56.285 --> 00:23:58.560
just make sure that
search face by image

616
00:23:58.560 --> 00:24:00.720
is only going to do that
for the largest face.

617
00:24:00.720 --> 00:24:02.565
If you need to do it
for all the faces,

618
00:24:02.565 --> 00:24:05.300
then you can easily
use detect faces

619
00:24:05.300 --> 00:24:06.800
API first which will

620
00:24:06.800 --> 00:24:08.850
return you all the
faces in the image,

621
00:24:08.850 --> 00:24:10.490
and then for each phase you can

622
00:24:10.490 --> 00:24:12.890
call source faces by image.

623
00:24:12.890 --> 00:24:15.830
Another important
best practice is to

624
00:24:15.830 --> 00:24:19.834
have very high accuracy
and avoid false positives,

625
00:24:19.834 --> 00:24:22.910
make sure faces that you are

626
00:24:22.910 --> 00:24:26.375
images you're indexing
our high-quality images.

627
00:24:26.375 --> 00:24:29.330
If you have blurry
faces or if you

628
00:24:29.330 --> 00:24:30.860
have a face that you are indexing

629
00:24:30.860 --> 00:24:32.550
doesn't have a lot
of facial features,

630
00:24:32.550 --> 00:24:35.390
for example, maybe it's only
a small part of the face.

631
00:24:35.390 --> 00:24:36.920
Then that's where you might run

632
00:24:36.920 --> 00:24:39.335
into seeing some false positive.

633
00:24:39.335 --> 00:24:42.445
So having images
which are not blurry,

634
00:24:42.445 --> 00:24:46.280
which are good quality as
well as faces where you

635
00:24:46.280 --> 00:24:50.510
have most of the facial
features visible in that image,

636
00:24:50.510 --> 00:24:52.805
will increase the likelihood

637
00:24:52.805 --> 00:24:55.460
of detecting the correct faces.

638
00:24:55.460 --> 00:24:57.410
I wrap up the course
by talking about

639
00:24:57.410 --> 00:24:59.255
media analysis solution which

640
00:24:59.255 --> 00:25:01.475
we have available on AWS website.

641
00:25:01.475 --> 00:25:02.780
There's a link at the bottom of

642
00:25:02.780 --> 00:25:05.480
the slide and there's a
CloudFormation template which

643
00:25:05.480 --> 00:25:07.564
creates a bunch of
resources including

644
00:25:07.564 --> 00:25:09.890
a Lambda function,
from step functions,

645
00:25:09.890 --> 00:25:11.840
and S3 buckets and gives you

646
00:25:11.840 --> 00:25:14.390
a nice UI where you can
upload some images and

647
00:25:14.390 --> 00:25:16.310
videos and quickly get to see

648
00:25:16.310 --> 00:25:17.540
all the metadata that

649
00:25:17.540 --> 00:25:19.685
different recognition
API has returned.

650
00:25:19.685 --> 00:25:21.020
In addition to that,

651
00:25:21.020 --> 00:25:23.330
it creates an
Elasticsearch cluster

652
00:25:23.330 --> 00:25:26.630
which can then give you
an option to see how you

653
00:25:26.630 --> 00:25:29.960
can build a media library
and then quickly be able to

654
00:25:29.960 --> 00:25:32.795
search by using the
different metadata

655
00:25:32.795 --> 00:25:34.280
that you get from Rekognition.

656
00:25:34.280 --> 00:25:37.669
So you'll see instead of
manually tagging each file,

657
00:25:37.669 --> 00:25:39.560
you just upload the file and then

658
00:25:39.560 --> 00:25:41.930
Rekognition does all the
magic of detecting what

659
00:25:41.930 --> 00:25:45.050
is in those images and videos
and then automatically

660
00:25:45.050 --> 00:25:47.390
populating you're
Elasticsearch cluster

661
00:25:47.390 --> 00:25:49.240
to be able to find those easily.

662
00:25:49.240 --> 00:25:51.110
With that, here's
the list of a couple

663
00:25:51.110 --> 00:25:53.000
of resources for
you to get started.

664
00:25:53.000 --> 00:25:56.360
You can easily use
different Rekognition APIs,

665
00:25:56.360 --> 00:25:59.465
either using a variety of
programming languages,

666
00:25:59.465 --> 00:26:02.315
whether it's Python,
Java, Node.js,

667
00:26:02.315 --> 00:26:06.005
Csharp, Go, and so
on or you can use

668
00:26:06.005 --> 00:26:10.580
our mobile SDK for platforms
like Android and iOS,

669
00:26:10.580 --> 00:26:13.490
and easily get started by
calling Rekognition and build

670
00:26:13.490 --> 00:26:16.745
really powerful smart
computer vision applications.

671
00:26:16.745 --> 00:26:18.470
Thank you for watching
the course on

672
00:26:18.470 --> 00:26:20.090
Amazon Rekognition Deep Dive,

673
00:26:20.090 --> 00:26:21.740
my name is [inaudible] and I

674
00:26:21.740 --> 00:26:24.750
hope you check out some
of our other courses.